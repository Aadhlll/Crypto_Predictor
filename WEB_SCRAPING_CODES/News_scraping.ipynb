{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trial 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import time\n",
    "\n",
    "# Specify the URL\n",
    "url = \"https://www.cryptocompare.com/news/list/latest/\"\n",
    "\n",
    "# Initialize the webdriver\n",
    "driver = webdriver.Chrome()  # You need to have chromedriver installed and in your PATH\n",
    "\n",
    "# Open the URL\n",
    "driver.get(url)\n",
    "\n",
    "# Set the initial scroll position\n",
    "scroll_pos = 0\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        # Scroll down to the bottom of the page\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        \n",
    "        # Wait for a short time to let the page load\n",
    "        time.sleep(2)\n",
    "        \n",
    "        # Calculate the new scroll position\n",
    "        new_scroll_pos = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        \n",
    "        # Check if the scroll position hasn't changed (reached the bottom of the page)\n",
    "        if new_scroll_pos == scroll_pos:\n",
    "            break\n",
    "        \n",
    "        scroll_pos = new_scroll_pos\n",
    "except KeyboardInterrupt:\n",
    "    # If the user interrupts the program (e.g., with Ctrl+C), stop scrolling\n",
    "    pass\n",
    "finally:\n",
    "    # Close the webdriver\n",
    "    driver.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trial 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jim Cramer Issues Urgent Market Warning as Bitcoin Plunges\n",
      "Top 7 Bitcoin Whale Withdraws $627 Million in BTC From Major Exchange\n",
      "Key Reason Behind Dogecoin's Massive Price Surge\n",
      "Bitcoin (BTC) Scores Historic Monthly Close\n",
      "Bitcoin (BTC) Hashrate New High: 'Three Times More Money,' CryptoQuant CEO Says\n",
      "Bitcoin (BTC) Receives Massive Warning as SHA-256 Collision Raises Questions\n",
      "Bitcoin (BTC) Price Could Top $150,000, Yusko Predicts\n",
      "Shiba Inu (SHIB) Could See Major Price Surge: Trader\n",
      "Bitcoin About to Have Its Most Impactful Halving, Bitwise CEO Says\n",
      "Massive USDC Inflows Spur Bullish Sentiment\n",
      "Bitcoin (BTC) Price Might Be on Track to Hit $75,000, but There's Key Resistance\n",
      "Bitcoin Reacts to SEC v. Coinbase Ruling With Price Drop\n",
      "BlackRock's $9.5 Trillion May Flow Into Digital Assets, Bitcoin Expert Willy Woo Believes\n",
      "Hong Kong Primed for Bitcoin ETFs, Expert Calls It Game-Changer\n",
      "Silk Road Founder Ross Ulbricht Turns 40 with 11 Years Spent Behind Bars\n",
      "Solv Introduces SolvBTC, First Yield-Bearing Token for Bitcoiners\n",
      "Bitcoin to Reach $100,000 Sooner Than Expected, Predicts Analyst\n",
      "BlackRock Praises Bitcoin as Portfolio Diversifier as BTC Price Tops $70K\n",
      "Samson Mow Makes Bold Bitcoin ETF Prediction for This Week As BTC Tops $70,000\n",
      "Bitcoin Halving Hype: 'Rich Dad Poor Dad' Author Kiyosaki Predicts $100,000 BTC by September\n",
      "Are Bitcoin ETFs Over? Top Expert Doesn’t Think So\n",
      "Top Bitcoin Dev Mulls Comeback\n",
      "Shocking $3.8 Million Bitcoin (BTC) Price Target Announced by Cathie Wood\n",
      "BlackRock Claims There's \"Little\" Demand for Ethereum\n",
      "Analyst Names Key Reason Behind Disastrous Grayscale's Outflows\n",
      "Bitcoin (BTC) Price Collapsing Again After Short-Lived Recovery\n",
      "Bitcoin (BTC) Price Driven by \"Bigger Forces\" Than ETFs: Top Analyst\n",
      "'I Don't See Bitcoin Correction,' Peter Brandt Says, Looking at BTC Chart\n",
      "'Rich Dad Poor Dad' Author Says Time to Buy Bitcoin, Not Stocks and Bonds\n",
      "Major Bitcoin ETF Warning Made by Samson Mow, Hold Tight\n",
      "Bitcoin Tops $65K as Jerome Powell Signals Rate Cuts\n",
      "Is Bull Run Over? Bitcoin Price Trapped Inside Bearish Channel\n",
      "XRP, SHIB, Bitcoin Emerge as Most Angry Cryptocurrencies Amid $455 Million Crypto Bloodbath\n",
      "Peter Schiff Names Main Problem with Bitcoin ETFs\n",
      "Bitcoin (BTC) Collapses to $60K as ETFs Bleed $500 Million\n",
      "Bitcoin (BTC) Suddenly Reclaims $65K, Shorts Getting Clobbered\n",
      "Bitcoin's Bull Run Continues as Peter Brandt Signals Healthy Correction\n",
      "Major Bitcoin (BTC) Warning Sign Everyone Missed\n",
      "Major Bullish Bitcoin Statement Made by Max Keiser – 'Hard Floor and No Top'\n",
      "Schiff Strikes Back at Michael Saylor Over MicroStrategy's $623 Million Bitcoin Buy\n",
      "$500 Million Destroyed Amid Crypto Bloodbath; End of Bull Market?\n",
      "5 Signs of Crypto Market Crash to Watch Before It Happens\n",
      "BlackRock's Bitcoin ETF Enters Top 5 Ranking for Year\n",
      "Main Bitcoin Price Catalyst Just 30 Days Away\n",
      "Bitcoin (BTC) Sees Massive Short-Term Holder Activity - What Does It Mean?\n",
      "Three Jaw-Dropping Bitcoin Price Scenarios Suggested by Willy Woo\n",
      "Bitcoin Sees Significant Profit-Taking by Short-Term Holders\n",
      "Lost Bitcoin Worth £1.5B Leads to Court Case\n",
      "'BTC, BNB, ATH, to Moon': Binance CEO Delivers Unexpected Insight\n",
      "Bitcoin Price Dip: $314 Million BTC Transferred From Coinbase to Unknown Wallet\n",
      "Investor Names Key Reason Behind Bitcoin (BTC) Crash\n",
      "Bitcoin (BTC) Fees Down by 33%, Here's Why\n",
      "Peter Brandt Provides Crucial Bitcoin Price Update\n",
      "Bank of America Spotlights Unprecedented Inflows into US Stocks and Cryptocurrencies\n",
      "Bitcoin (BTC) Surpasses Gold in Investor Portfolios: JP Morgan\n",
      "Crypto Market Bloodbath: $740 Million Liquidated as Bitcoin Plummets\n",
      "Mysterious Bitcoin Move to Binance Makes BTC Price Plummet Again: Details\n",
      "'Rich Dad Poor Dad' Author Kiyosaki Recognizes Bitcoin's Dominance Over Gold\n",
      "Senators Urge SEC to Block More Crypto ETFs\n",
      "Solana (SOL) in Green as Crypto Prices Collapse\n",
      "Altcoin Crash Imminent, '$1 Million for Bitcoin,' Advocate Samson Mow Warns\n",
      "Peter Schiff Says Bitcoin U-Turn Claims Are Lies\n",
      "Bitcoin Outshines Taylor Swift in Google Searches\n",
      "$100 Billion Boom in Bitcoin ETFs and Crypto Investments Recorded\n",
      "Crypto Investors Gained $37.6 Billion Last Year: Report\n",
      "$4.8 Million for Bitcoin? Top Expert Offers New BTC Price Perspective\n",
      "Veteran Trader Peter Brandt Reveals Rare Bitcoin Price Chart\n",
      "Scaramucci on Grayscale's Mini Bitcoin Trust: Train Has Left the Station\n",
      "Block's Crypto Wallet Shuns Bitcoin for Boring Old Credit Cards\n",
      "Bitcoin ETFs Poised for Success: Expert Forecasts Full Survival\n",
      "Bitcoin ETFs Seeing Record Inflows\n",
      "Bitcoin (BTC) Preparing for Next Parabolic Run, Say Analysts\n",
      "Ripple CTO Names Main Problem of Self-Proclaimed Satoshi Craig Wright\n",
      "Peter Schiff Tries to Debunk Bitcoin Halving Hype\n",
      "Grayscale's GBTC Nears Outflow Record\n",
      "Bernstein Sees Bitcoin Reaching $150,000\n",
      "$50 Billion Grayscale Launches BTC, New Bitcoin ETF: Here's Why\n",
      "Bitcoin Spot ETFs Near $60 Billion in Net Assets Amid Heavy Trading\n",
      "Snowden Roasts JP Morgan CEO Over Bitcoin Price and Purchases\n",
      "‘$1 Million Bitcoin’ Advocate Samson Mow Says Price Run Not Even Started Yet\n",
      "'No Failed Rally in Bitcoin (BTC)': Says Legendary Trader John Bollinger\n",
      "JPMorgan Compares Bitcoin to Smoking Cigarettes\n",
      "XRP Becomes Best-Performing Cryptocurrency in Top 100 After Sudden Price Surge\n",
      "Top Lawyer Explains Ethereum ETF Approval Bearishness\n",
      "'Bitcoin Popularity Secret' Revealed by Michael Saylor on CNBC\n",
      "Peter Schiff Warns Bill Ackman About Bitcoin in Counterargument to Michael Saylor\n",
      "Bullish Bitcoin Price Prediction Hinted at by Samson Mow, Hold Tight\n",
      "MicroStrategy Trumps BlackRock With New 12,000 BTC Buy at $72,000 Bitcoin Price\n",
      "'Rich Dad Poor Dad' Author Says: 'Buy Bitcoin (BTC) Before Biggest Bubble in History Bursts'\n",
      "Bitcoin (BTC) Soars Above $71K, Logs New Record Peak\n",
      "This Bullish Bitcoin Chart Receives Peter Brandt's Support\n",
      "BlackRock Bypasses MicroStrategy in Bitcoin Numbers\n",
      "Legendary Hedge Fund Manager Bill Ackman Eyes Bitcoin, Believes in Its Sky-High Potential\n",
      "Bitcoin's Daily Candle Closes at All-Time High\n",
      "$100,000 Bitcoin (BTC) Before Halving? Top Analyst Indicates Three Scenarios\n",
      "Dogecoin to $1? Top Trader Believes It's No Longer Just Meme\n",
      "Gold May Beat Bitcoin in 2024, Mike McGlone Suggests\n",
      "Jerome Powell Makes Crucial Statement for Crypto Market: Details\n",
      "Key Reasons Why Bitcoin (BTC) Just Hit $70,000\n",
      "Ethereum (ETH) Approaching $4K First Time Since Late 2021\n",
      "Crypto Options Alert: Bitcoin and Ethereum Set for Significant Expiry Event\n",
      "SEC Boss Issues Major Bitcoin Warning\n",
      "$300,000 Bitcoin in 2024: 'Rich Dad Poor Dad' Author Kiyosaki Drops Epic Price Prediction\n",
      "Novogratz Predicts Bitcoin Will Break $100,000 in 2024\n",
      "Peter Schiff Accused of Promoting Bitcoin\n",
      "Bitcoin (BTC) Rally Far From Over, Say Glassnode Cofounders\n",
      "Satoshi-Era Bitcoin Whale Suddenly Wakes up and Does Unthinkable\n",
      "Legendary Trader John Bollinger Calls Bitcoin Price Drop 'Bit Much'\n",
      "Jim Cramer Claims Bitcoin (BTC) Price Has Topped\n",
      "Shytoshi Kusama Responds to Bullish Price Prediction\n",
      "Solana (SOL) Predicted to Make Major Gains Against Bitcoin (BTC)\n",
      "100% of Bitcoin Wallets Currently in Profit: Details\n",
      "Key Reasons Why Bitcoin (BTC) Just Reached New All-Time High\n",
      "Binance Warns of Large Bitcoin Transfers Coming\n",
      "Several Bitcoin ETFs Bleed Funds as BTC Price Flirts With All-Time High\n",
      "‘$1 Million Bitcoin’ Advocate Samson Mow Comments on BTC Surge As It Nears $70,000\n",
      "Dogecoin (DOGE) Overtakes Cardano (ADA). Is Shiba Inu (SHIB) Next?\n",
      "Bitcoin (BTC) Zeroing In on Alphabet After Surpassing Meta\n",
      "Peter Schiff Has Major Warning for Bitcoin ETF Buyers\n",
      "Bitcoin (BTC) Hits New Peak in Euro; Which Fiat Currencies Are Left?\n",
      "SHIB Lead Shytoshi Kusama Doesn't Care If Bitcoin Hits $70,000, Here's Why\n",
      "Gigantic $48.54 Billion Enter Crypto Market Amid Bullish Frenzy\n",
      "Crypto Market Tops $2.5 Trillion as Dogecoin (DOGE), Polkadot (DOT) and Filecoin (FIL) Surge\n",
      "Key Reasons Why Bitcoin (BTC) Just Reclaimed $64,000\n",
      "Bitcoin Might Not Kill Dollar, but This Will: Top Economist\n",
      "Dogecoin (DOGE) Creator Reveals Bitcoin Price and Altcoins Correlation\n",
      "Scaramucci Touts Bitcoin as 21st Century's Berkshire Hathaway\n",
      "Top Analyst Downplays Ether ETFs\n",
      "Shiba Inu (SHIB) Becomes Fourth Most Traded Cryptocurrency\n",
      "Bitcoin (BTC) Local High: 97% of Addresses in Profit\n",
      "Gold's Big Day Ignored? Peter Schiff Critiques CNBC's Bitcoin Fixation\n",
      "Bitcoin (BTC) 25% Drop in Cards, Historical Data Shows\n",
      "BlackRock's Bitcoin ETF Joins $10 Billion Club\n",
      "Legendary Trader Peter Brandt Unveils Bullish Bitcoin Price Outlook\n",
      "Key Reason Why Bitcoin Nearing $70,000, Per Bloomberg’s Mike McGlone\n",
      "Bitcoin (BTC) Price Shows Resilience With Strong Accumulation Indicators\n",
      "$621 Million Bitcoin Transfer Leaves Major Exchange for Unknown Hands\n",
      "Bitcoin Whale Sits on $900 Million Profit After Grabbing Big BTC Chunk in 2022\n",
      "Mike Novogratz Sees Bitcoin (BTC) Dropping to $55K\n",
      "Solana (SOL) Breaking Out in \"Big Way\"\n",
      "BlackRock's Bitcoin ETF to Leave Gold in Dust with $10 Billion Milestone\n",
      "Crypto Can Hopefully Bail Out Financial System: Savvy Trader Peter Brandt\n",
      "Dogecoin (DOGE) Creator Reacts to Wild Bitcoin Price Performance\n",
      "Bitcoin on Track for Record $22,000 Monthly Candle\n",
      "'Rich Dad Poor Dad' Author Comments on Massive Bitcoin Surge\n",
      "Countdown to Bitcoin All-Time High: Samson Mow Drops Hint\n",
      "Zero Balance Alert on Coinbase: What Happened?\n",
      "Crucial Bitcoin Message Sent by Michael Saylor to Community As BTC Tops $61,000\n",
      "Next Nvidia? Look at Bitcoin, Says Scaramucci\n",
      "Key Reason Why Bitcoin (BTC) Just Soared Above $60,000\n",
      "Shocking Bitcoin Strategy from BlackRock Calls for 28% Allocation\n",
      "'I'm Not Satoshi,' Adam Back Says\n",
      "Bitcoin (BTC) Getting Overheated as Key Bearish Signal Appears\n",
      "Bitcoin ETFs Witness Explosive $520 Million Inflows, While BlackRock Breaks Records\n",
      "Current BTC Surge Could Bring on $100,000 'God Candle': Max Keiser\n",
      "'$1 Million Bitcoin' Advocate Samson Mow Issues Warning to Altcoin Investors\n",
      "BTC to $200,000? Peter Brandt Drops Epic Bitcoin Price Prediction\n",
      "Key Reasons Why Bitcoin (BTC) Price Just Soared Past $57,000\n",
      "Bitcoin (BTC) Price Soars to Highest Level Since 2021\n",
      "MicroStrategy Announces Major Bitcoin (BTC) Purchase\n",
      "Bitcoin Leads With $570 Million Inflows as Digital Asset Management Peaks at $68.3 Billion\n",
      "Coinbase Records Major BTC Withdrawal as Bitcoin Price Stands Strong\n",
      "Bitcoin Bull Mike Novogratz Thinks ETFs Will Drive More Retail Demand\n",
      "Bitcoin ETFs: Soros-Inspired Boom Predicted by Investor\n",
      "'Rich Dad Poor Dad' Author Reveals His Possible Actions Should Bitcoin Crash\n",
      "Monster Bitcoin (BTC) Purchase Mystifies Community\n",
      "Nvidia Surpasses Entire Crypto Market as AI Hype Picks Up Steam\n",
      "Legendary Trader John Bollinger Issues Bullish Wake-Up Call as Bitcoin Price Turns Red\n",
      "Legendary Trader Peter Brandt Takes Dig at Bitcoin Maxis\n",
      "Reddit Buys Bitcoin (BTC) and Ethereum (ETH)\n",
      "Satoshi's New Emails Unearthed: Key Details\n",
      "Fidelity Exec Unveils Groundbreaking Bitcoin Valuation Model\n",
      "Euro Collapsed Against BTC: Balaji Srinivasan on ECB Bitcoin Criticism\n",
      "Gold out, Bitcoin in: Investor Predicts Shift in Hedging Strategy\n",
      "When New BTC ATH? Michael Saylor Triggers Crypto Community's Enthusiasm on This\n",
      "Bitcoin to Hit $500,000 on Upcoming Stocks and Gold Crash: Max Keiser\n",
      "'It Is Proof He Isn't Satoshi': Ripple CTO on Craig Wright's Court Update\n",
      "Up to 200% on Crypto AI Tokens: Top Performing AI Assets on Market Today\n",
      "Advanced Shiba Inu Trading Feature Rolled Out by Top Exchange\n",
      "Bitcoin Surges to Record Levels in Super Holders' Wallets\n",
      "Bitcoin Bull Michael Saylor Says He Will Be Buying BTC \"Forever\"\n",
      "Massive Crypto Seizure Takes Place During LockBit Ransomware Takedown\n",
      "Bitcoin to Win As Stocks Eye Major Crash: Max Keiser\n",
      "Bitcoin Options Market Heats Up\n",
      "Bitcoin (BTC) Crushing Ethereum (ETH) Despite Altcoin Rally\n",
      "Michael Saylor Doubles Down on His Bitcoin Bet, Dismissing Gold\n",
      "Bitcoin Breaks All-Time High in Japan Amid Regulatory Revolution\n",
      "Bitcoin Price Eyes $58,000 in Pre-Halving Surge, Says Top Analyst\n",
      "Bitcoin vs. AI: Crucial Warning Made by VanEck and Tether's Top Exec\n",
      "Nearly $1 Billion in Bitcoin Disappears After Transfer From Major US Exchange\n",
      "Will Bitcoin Hit $100,000? 'Rich Dad Poor Dad' Author Kiyosaki Makes Shocking Prediction\n",
      "Vanguard Leaving Bitcoin ETF Inflows in the Dust\n",
      "This Chart Spots Massive Bitcoin ETF Success\n",
      "Ripple Exec Denies XRP Price Manipulation\n",
      "Bitcoin (BTC) Cycle Top Near? Check Out This Metric\n",
      "'Rich Dad Poor Dad' Author Issues Crucial Bitcoin-Vs-Fed Statement\n",
      "List of Bullish Bitcoin Factors 'Normies' Unaware of Shown by Samson Mow\n",
      "Bitcoin ETFs See Major Influx of Fresh Funds\n",
      "Bitcoin Evangelist Saylor Takes Dig at WSJ\n",
      "Epic Altcoin Surge on Horizon, Top Trader Henrik Zeberg Predicts\n",
      "Bitcoin Whales Snap up $5 Billion in BTC - Sign of Another Rally?\n",
      "400 Million DOGE Sent to Robinhood as Dogecoin Reclaims Spot in Top 10\n",
      "Mike Novogratz Bullish on Bitcoin as Halving Looms\n",
      "Bitcoin's Surge to $52K Faces Threats From Two Major Sell-Off Events\n",
      "Ethereum (ETH) Jumps to Highest Level Since March 2022\n",
      "Bitcoin Bull Michael Saylor Breaks Silence on BTC Price\n",
      "Dogecoin Founder Reveals His Bitcoin Holdings\n",
      "Bitcoin (BTC) Hits New All-Time High in Japan\n",
      "Binance CEO Comments on Bitcoin’s Historic Market Cap Surge\n",
      "Bitcoin Leading Ransomware Market, Gensler Says\n",
      "Bitcoin Breaks $1 Trillion Market Cap as BTC Price Goes Parabolic\n",
      "Bitcoin ETF: Anthony Scaramucci Praises BlackRock for Huge New Milestone\n",
      "Bitcoin ETFs Surge With $631 Million Inflows in 24 Hours: Details Inside\n",
      "Key Reasons Why Bitcoin Price Is Pumping Today\n",
      "'$1 Million for BTC' Samson Mow Bullish on MicroStrategy After Learning This\n",
      "Bitcoin (BTC) Price to $600,000: Tuur Demeester Shares Uber-Bullish Forecast\n",
      "Bitcoin ETFs Leaving Gold in the Dust\n",
      "'Rich Dad Poor Dad' Author Kiyosaki Warns: Prepare for 70% Collapse of S&P500\n",
      "Elon Musk Ignites Bullish Sentiment in Community as Bitcoin Tops $50,000\n",
      "John Bollinger Demystifies Bitcoin (BTC) Rally\n",
      "Bitcoin (BTC) Just Entered \"New Era,\" Top Analyst Says\n",
      "Record-Breaking Bitcoin (BTC) Price Rally Predicted by Bernstein\n",
      "Bitcoin (BTC) to Hit $50K After Bullish Weekly Divergence, Says Top Analyst\n",
      "Dogecoin (DOGE) No Longer Top 10 Coin\n",
      "“Satoshi” Spotted at Super Bowl\n",
      "Shocking Bitcoin Move Nets Investor $13.5M in Just Days\n",
      "'$1 Million for BTC' Samson Mow Stuns With Chinese New Year Bitcoin Prediction\n",
      "Crypto Bears Demolished: $87 Million Shorts Orders Disappeared From Market\n",
      "'Rich Dad Poor Dad' Author Says 'Bet on Bitcoin,' Naming This Astonishing Reason\n",
      "Bitcoin ETFs Pump $500 Million Amid BTC Price Rally\n",
      "Piers Morgan: Bitcoin Traders Are \"Mugs\"\n",
      "$50,000 Remains Key Level for Bitcoin: Trader\n",
      "Bitcoin Is Wasteful, Top Berkeley Scientist Says\n",
      "$355 Bitcoin Shuffle Unfolds on Coinbase as BTC Price Eyes 27% Upside\n",
      "Bitcoin and Elon Musk Have This Stunning Thing In Common: VanEck's Top Exec\n",
      "'Rich Dad Poor Dad' Author Kiyosaki Sounds Alarm Amid Bank Failures, Advocates Bitcoin as Parachute\n",
      "Bitcoin Zeroing in On $45,000 Price Tag\n",
      "90% of All Bitcoin in Highest Profit Since 2021 ATH: PlanB Analyst\n",
      "Bitcoin Critic Peter Schiff Not Happy With New SEC Rules\n",
      "Bitcoin Wallet Activity Dips Despite ETF Approvals\n",
      "Top Fed Official Slams Bitcoin\n",
      "Charles Edwards Unveils Golden Bitcoin Buying Opportunity Amid BTC Halving\n",
      "Crucial Bitcoin Statement Issued by Samson Mow: 'Game Is Up'\n",
      "Ethereum to Outperform Bitcoin in Consolidation Phase, Says Top Analyst\n",
      "\"Rich Dad Poor Dad\" Author Predicts Stock Market Crash, Ripple Forecasts Biggest 2024 Breakthrough for DeFi, SHIB Rep Provides Important SHIB Burn Clarification: Crypto News Digest by U.Today\n",
      "Bitcoin Aiming for All-Time High in 2024, Predicts Top Analyst\n",
      "$1.6 Billion Crypto Sale Sought by Genesis Capital\n",
      "Scaramucci Slams Negative Coverage of Bitcoin ETF Launch\n",
      "Bitcoin Vet Charlie Shrem Foresees 'Last' Bull Run\n",
      "'Rich Dad Poor Dad' Author Bitcoiner Predicts Stock Market Crash, Hold Tight\n",
      "Greed Takes Over Ethereum\n",
      "$1 Million Per Bitcoin By 2028 Not Guaranteed: Tuur Demeester\n",
      "Bitcoin Spot ETFs Witness $38.45 Million Inflow, Marking Fifth Day of Gains\n",
      "Bitcoin (BTC) Price Just Recorded This Bullish Pattern\n",
      "Bitcoin ETF Race: BlackRock Now Ahead of Grayscale in Trading Volume\n",
      "Elon Musk Becomes Bitcoiner - Crypto Community Celebrates 3-Year Anniversary\n",
      "Bitcoin (BTC) Staking L2 Social Network Kicks off in Testnet\n",
      "Bitcoin (BTC) Accumulation Trend Score Hits 3-Year High: Details\n",
      "Bitcoin to $2.3 Million? ARK Invest Doesn't Exclude This\n",
      "Bitcoin (BTC) Price Steady as Fed Keeps Rates Unchanged\n",
      "Bitcoin Price Likely to Go Parabolic: Samson Mow\n",
      "New Bitcoin (BTC) Era Announced by VanEck's Gabor Gurbacs\n",
      "'Rich Dad Poor Dad' Author Kiyosaki Finally Explains Why He Owns Bitcoin (BTC)\n",
      "Billionaire Tim Draper Predicts Total Bitcoinization\n",
      "Key Reason Why Bitcoin (BTC) Volatility Has Collapsed\n",
      "€2 Billion in Bitcoin Seized by German Authorities\n",
      "Bitcoin (BTC) Whales Increase Holdings Despite Price Volatility: Details\n",
      "Shiba Inu (SHIB) on Track to Erase Zero After Price Spike\n",
      "Bitcoin (BTC) Suddenly Reclaims $43,000. Key Reason Why\n",
      "JPMorgan: Bitcoin ETF Hype Cooling\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "# Specify the URL\n",
    "url = \"https://u.today/bitcoin-news\"\n",
    "\n",
    "# Initialize the webdriver\n",
    "driver = webdriver.Chrome()  # You need to have chromedriver installed and in your PATH\n",
    "\n",
    "try:\n",
    "    # Open the URL\n",
    "    driver.get(url)\n",
    "\n",
    "    # Set the initial scroll position\n",
    "    scroll_pos = 0\n",
    "\n",
    "    # Scroll the page 5 times\n",
    "    for _ in range(5):\n",
    "        # Scroll down to the bottom of the page\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "        # Wait for a short time to let the page load\n",
    "        time.sleep(2)\n",
    "\n",
    "    # Once scrolling is done, get the page source\n",
    "    page_source = driver.page_source\n",
    "\n",
    "    # Parse the HTML content\n",
    "    soup = BeautifulSoup(page_source, \"html.parser\")\n",
    "\n",
    "    # Find and print the titles of news items\n",
    "    titles = soup.select(\"div.news__item-title\")\n",
    "    for title in titles:\n",
    "        print(title.text.strip())\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    # If the user interrupts the program (e.g., with Ctrl+C), stop scrolling and scraping\n",
    "    pass\n",
    "finally:\n",
    "    # Close the webdriver\n",
    "    driver.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trial 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Date                                               News\n",
      "0    2024/04/01 15:08  Jim Cramer Issues Urgent Market Warning as Bit...\n",
      "1    2024/04/01 09:29  Top 7 Bitcoin Whale Withdraws $627 Million in ...\n",
      "2    2024/04/01 07:36   Key Reason Behind Dogecoin's Massive Price Surge\n",
      "3    2024/04/01 05:24        Bitcoin (BTC) Scores Historic Monthly Close\n",
      "4    2024/03/31 14:10  Bitcoin (BTC) Hashrate New High: 'Three Times ...\n",
      "..                ...                                                ...\n",
      "205  2024/02/15 08:21  Bitcoin Bull Michael Saylor Breaks Silence on ...\n",
      "206  2024/02/15 08:10      Dogecoin Founder Reveals His Bitcoin Holdings\n",
      "207  2024/02/15 06:05      Bitcoin (BTC) Hits New All-Time High in Japan\n",
      "208  2024/02/14 16:45  Binance CEO Comments on Bitcoin’s Historic Mar...\n",
      "209  2024/02/14 16:06    Bitcoin Leading Ransomware Market, Gensler Says\n",
      "\n",
      "[210 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Specify the URL\n",
    "url = \"https://u.today/bitcoin-news\"\n",
    "\n",
    "# Initialize the webdriver\n",
    "driver = webdriver.Chrome()  # You need to have chromedriver installed and in your PATH\n",
    "\n",
    "try:\n",
    "    # Open the URL\n",
    "    driver.get(url)\n",
    "\n",
    "    # Set the initial scroll position\n",
    "    scroll_pos = 0\n",
    "\n",
    "    # Scroll the page 5 times\n",
    "    for _ in range(5):\n",
    "        # Scroll down to the bottom of the page\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "        # Wait for a short time to let the page load\n",
    "        time.sleep(2)\n",
    "\n",
    "    # Once scrolling is done, get the page source\n",
    "    page_source = driver.page_source\n",
    "\n",
    "    # Parse the HTML content\n",
    "    soup = BeautifulSoup(page_source, \"html.parser\")\n",
    "\n",
    "    # Find and store the dates and titles of news items\n",
    "    dates = [date.text.strip() for date in soup.find_all('div', class_='humble')]\n",
    "    titles = [title.text.strip() for title in soup.find_all('div', class_='news__item-title')]\n",
    "\n",
    "    # Create a DataFrame to store the data\n",
    "    df = pd.DataFrame({'Date': dates, 'News': titles})\n",
    "\n",
    "    # Print the DataFrame\n",
    "    print(df)\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    # If the user interrupts the program (e.g., with Ctrl+C), stop scrolling and scraping\n",
    "    pass\n",
    "finally:\n",
    "    # Close the webdriver\n",
    "    driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('crypto_news.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping Policy Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping data from page 1...\n",
      "Scraping data from page 2...\n",
      "Scraping data from page 3...\n",
      "Scraping data from page 4...\n",
      "Scraping data from page 5...\n",
      "Scraping data from page 6...\n",
      "Scraping data from page 7...\n",
      "Scraping data from page 8...\n",
      "Scraping data from page 9...\n",
      "Scraping data from page 10...\n",
      "Scraping data from page 11...\n",
      "Scraping data from page 12...\n",
      "Scraping data from page 13...\n",
      "Scraping data from page 14...\n",
      "Scraping data from page 15...\n",
      "Scraping data from page 16...\n",
      "Scraping data from page 17...\n",
      "Scraping data from page 18...\n",
      "Scraping data from page 19...\n",
      "Scraping data from page 20...\n",
      "Scraping data from page 21...\n",
      "Scraping data from page 22...\n",
      "Scraping data from page 23...\n",
      "Scraping data from page 24...\n",
      "Scraping data from page 25...\n",
      "Scraping data from page 26...\n",
      "Scraping data from page 27...\n",
      "Scraping data from page 28...\n",
      "Scraping data from page 29...\n",
      "Scraping data from page 30...\n",
      "Scraping data from page 31...\n",
      "Scraping data from page 32...\n",
      "Scraping data from page 33...\n",
      "Scraping data from page 34...\n",
      "Scraping data from page 35...\n",
      "Scraping data from page 36...\n",
      "Scraping data from page 37...\n",
      "Scraping data from page 38...\n",
      "Scraping data from page 39...\n",
      "Scraping data from page 40...\n",
      "Scraping data from page 41...\n",
      "Scraping data from page 42...\n",
      "Scraping data from page 43...\n",
      "Scraping data from page 44...\n",
      "Scraping data from page 45...\n",
      "Scraping data from page 46...\n",
      "Scraping data from page 47...\n",
      "Scraping data from page 48...\n",
      "Scraping data from page 49...\n",
      "Scraping data from page 50...\n",
      "Scraping data from page 51...\n",
      "Scraping data from page 52...\n",
      "Scraping data from page 53...\n",
      "Scraping data from page 54...\n",
      "Scraping data from page 55...\n",
      "Scraping data from page 56...\n",
      "Scraping data from page 57...\n",
      "Scraping data from page 58...\n",
      "Scraping data from page 59...\n",
      "Scraping data from page 60...\n",
      "Scraping data from page 61...\n",
      "Scraping data from page 62...\n",
      "Scraping data from page 63...\n",
      "Scraping data from page 64...\n",
      "Scraping data from page 65...\n",
      "Scraping data from page 66...\n",
      "Scraping data from page 67...\n",
      "Scraping data from page 68...\n",
      "Scraping data from page 69...\n",
      "Scraping data from page 70...\n",
      "Scraping data from page 71...\n",
      "Scraping data from page 72...\n",
      "Scraping data from page 73...\n",
      "Scraping data from page 74...\n",
      "Scraping data from page 75...\n",
      "Scraping data from page 76...\n",
      "Scraping data from page 77...\n",
      "Scraping data from page 78...\n",
      "Scraping data from page 79...\n",
      "Scraping data from page 80...\n",
      "Scraping data from page 81...\n",
      "Scraping data from page 82...\n",
      "Scraping data from page 83...\n",
      "Scraping data from page 84...\n",
      "Scraping data from page 85...\n",
      "Scraping data from page 86...\n",
      "Scraping data from page 87...\n",
      "Scraping data from page 88...\n",
      "Scraping data from page 89...\n",
      "Scraping data from page 90...\n",
      "Scraping data from page 91...\n",
      "Scraping data from page 92...\n",
      "Scraping data from page 93...\n",
      "Scraping data from page 94...\n",
      "Scraping data from page 95...\n",
      "Scraping data from page 96...\n",
      "Scraping data from page 97...\n",
      "Scraping data from page 98...\n",
      "Scraping data from page 99...\n",
      "Scraping data from page 100...\n",
      "Scraping data from page 101...\n",
      "Scraping data from page 102...\n",
      "Scraping data from page 103...\n",
      "Scraping data from page 104...\n",
      "Scraping data from page 105...\n",
      "Scraping data from page 106...\n",
      "Scraping data from page 107...\n",
      "Scraping data from page 108...\n",
      "Scraping data from page 109...\n",
      "Scraping data from page 110...\n",
      "Scraping data from page 111...\n",
      "Scraping data from page 112...\n",
      "Scraping data from page 113...\n",
      "Scraping data from page 114...\n",
      "Scraping data from page 115...\n",
      "Scraping data from page 116...\n",
      "Scraping data from page 117...\n",
      "Scraping data from page 118...\n",
      "Scraping data from page 119...\n",
      "Scraping data from page 120...\n",
      "Scraping data from page 121...\n",
      "Scraping data from page 122...\n",
      "Scraping data from page 123...\n",
      "Scraping data from page 124...\n",
      "Scraping data from page 125...\n",
      "Scraping data from page 126...\n",
      "Scraping data from page 127...\n",
      "Scraping data from page 128...\n",
      "Scraping data from page 129...\n",
      "Scraping data from page 130...\n",
      "Scraping data from page 131...\n",
      "Scraping data from page 132...\n",
      "Scraping data from page 133...\n",
      "Scraping data from page 134...\n",
      "Scraping data from page 135...\n",
      "Scraping data from page 136...\n",
      "Scraping data from page 137...\n",
      "Scraping data from page 138...\n",
      "Scraping data from page 139...\n",
      "Scraping data from page 140...\n",
      "Scraping data from page 141...\n",
      "Scraping data from page 142...\n",
      "Scraping data from page 143...\n",
      "Scraping data from page 144...\n",
      "Scraping data from page 145...\n",
      "Scraping data from page 146...\n",
      "Scraping data from page 147...\n",
      "Scraping data from page 148...\n",
      "Scraping data from page 149...\n",
      "Scraping data from page 150...\n",
      "Scraping data from page 151...\n",
      "Scraping data from page 152...\n",
      "Scraping data from page 153...\n",
      "Scraping data from page 154...\n",
      "Scraping data from page 155...\n",
      "Scraping data from page 156...\n",
      "Scraping data from page 157...\n",
      "Scraping data from page 158...\n",
      "Scraping data from page 159...\n",
      "Scraping data from page 160...\n",
      "Scraping data from page 161...\n",
      "Scraping data from page 162...\n",
      "Scraping data from page 163...\n",
      "Scraping data from page 164...\n",
      "Scraping data from page 165...\n",
      "Scraping data from page 166...\n",
      "Scraping data from page 167...\n",
      "Scraping data from page 168...\n",
      "Scraping data from page 169...\n",
      "Scraping data from page 170...\n",
      "Scraping data from page 171...\n",
      "Scraping data from page 172...\n",
      "Scraping data from page 173...\n",
      "Scraping data from page 174...\n",
      "Scraping data from page 175...\n",
      "Scraping data from page 176...\n",
      "Scraping data from page 177...\n",
      "Scraping data from page 178...\n",
      "Scraping data from page 179...\n",
      "Scraping data from page 180...\n",
      "Scraping data from page 181...\n",
      "Scraping data from page 182...\n",
      "Scraping data from page 183...\n",
      "Scraping data from page 184...\n",
      "Scraping data from page 185...\n",
      "Scraping data from page 186...\n",
      "Scraping data from page 187...\n",
      "Scraping data from page 188...\n",
      "Scraping data from page 189...\n",
      "Scraping data from page 190...\n",
      "Scraping data from page 191...\n",
      "Scraping data from page 192...\n",
      "Scraping data from page 193...\n",
      "Scraping data from page 194...\n",
      "Scraping data from page 195...\n",
      "Scraping data from page 196...\n",
      "Scraping data from page 197...\n",
      "Scraping data from page 198...\n",
      "Scraping data from page 199...\n",
      "Scraping data from page 200...\n",
      "Scraping data from page 201...\n",
      "Scraping data from page 202...\n",
      "Scraping data from page 203...\n",
      "Scraping data from page 204...\n",
      "Scraping data from page 205...\n",
      "Scraping data from page 206...\n",
      "Scraping data from page 207...\n",
      "Scraping data from page 208...\n",
      "Scraping data from page 209...\n",
      "Scraping data from page 210...\n",
      "Scraping data from page 211...\n",
      "Scraping data from page 212...\n",
      "Scraping data from page 213...\n",
      "Scraping data from page 214...\n",
      "Scraping data from page 215...\n",
      "Scraping data from page 216...\n",
      "Scraping data from page 217...\n",
      "Scraping data from page 218...\n",
      "Scraping data from page 219...\n",
      "Scraping data from page 220...\n",
      "Scraping data from page 221...\n",
      "Scraping data from page 222...\n",
      "Scraping data from page 223...\n",
      "Scraping data from page 224...\n",
      "Scraping data from page 225...\n",
      "Scraping data from page 226...\n",
      "Scraping data from page 227...\n",
      "Scraping data from page 228...\n",
      "Scraping data from page 229...\n",
      "Scraping data from page 230...\n",
      "Scraping data from page 231...\n",
      "Scraping data from page 232...\n",
      "Scraping data from page 233...\n",
      "Scraping data from page 234...\n",
      "Scraping data from page 235...\n",
      "Scraping data from page 236...\n",
      "Scraping data from page 237...\n",
      "Scraping data from page 238...\n",
      "Scraping data from page 239...\n",
      "Scraping data from page 240...\n",
      "Scraping data from page 241...\n",
      "Scraping data from page 242...\n",
      "Scraping data from page 243...\n",
      "Scraping data from page 244...\n",
      "Scraping data from page 245...\n",
      "Scraping data from page 246...\n",
      "Scraping data from page 247...\n",
      "Scraping data from page 248...\n",
      "Scraping data from page 249...\n",
      "Scraping data from page 250...\n",
      "Scraping data from page 251...\n",
      "Scraping data from page 252...\n",
      "Scraping data from page 253...\n",
      "Scraping data from page 254...\n",
      "Scraping data from page 255...\n",
      "Scraping data from page 256...\n",
      "Scraping data from page 257...\n",
      "Scraping data from page 258...\n",
      "Scraping data from page 259...\n",
      "Scraping data from page 260...\n",
      "Scraping data from page 261...\n",
      "Scraping data from page 262...\n",
      "Scraping data from page 263...\n",
      "Scraping data from page 264...\n",
      "Scraping data from page 265...\n",
      "Scraping data from page 266...\n",
      "Scraping data from page 267...\n",
      "Scraping data from page 268...\n",
      "Scraping data from page 269...\n",
      "Scraping data from page 270...\n",
      "Scraping data from page 271...\n",
      "Data saved to scraped_data.csv\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Define the base URL and the range of pages to scrape\n",
    "base_url = \"https://www.theblock.co/category/policy/\"\n",
    "start_page = 1\n",
    "end_page = 271\n",
    "\n",
    "# Initialize an empty list to store dictionaries\n",
    "data_list = []\n",
    "\n",
    "# Function to scrape data from a single page\n",
    "def scrape_page(page_number):\n",
    "    url = f\"{base_url}{page_number}\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        # Find all headline and date elements\n",
    "        headlines = soup.select('div.collection__feed div.headline span')\n",
    "        dates = soup.select('div.collection__feed div.meta div.pubDate')\n",
    "        # Iterate over each headline and date and store in the list\n",
    "        for headline, date in zip(headlines, dates):\n",
    "            headline_text = headline.get_text(strip=True)\n",
    "            date_text = date.get_text(strip=True)\n",
    "            data_list.append({'headline': headline_text, 'date': date_text})\n",
    "    else:\n",
    "        print(f\"Failed to fetch page {page_number}\")\n",
    "\n",
    "# Loop through pages and scrape data\n",
    "for page_number in range(start_page, end_page + 1):\n",
    "    print(f\"Scraping data from page {page_number}...\")\n",
    "    scrape_page(page_number)\n",
    "    time.sleep(1)  # Add a delay to be polite to the server\n",
    "\n",
    "# Create DataFrame from the list of dictionaries\n",
    "df = pd.DataFrame(data_list)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv(\"scraped_data.csv\", index=False)\n",
    "\n",
    "print(\"Data saved to scraped_data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            headline                date\n",
      "0  Hong Kong spot bitcoin ETFs could go live as s... 2024-04-16 02:54:00\n",
      "1  Lawmakers demand information on CFTC chair's r... 2024-04-15 16:46:00\n",
      "2  Nebraska man charged for mining $1 million in ... 2024-04-15 14:56:00\n",
      "3  Potential movement on stablecoin legislation f... 2024-04-15 13:09:00\n",
      "4  UK to legislate on cryptoasset regulatory fram... 2024-04-15 12:18:00\n"
     ]
    }
   ],
   "source": [
    "# Assuming df is your existing DataFrame with the \"date\" column containing dates with \"EDT\"\n",
    "df['date'] = df['date'].str.replace(' EDT', '')  # Remove 'EDT'\n",
    "df['date'] = df['date'].str.replace(' EST', '')  # Remove 'EDT'\n",
    "df['date'] = df['date'].str.strip()  # Remove leading and trailing whitespace\n",
    "df['date'] = pd.to_datetime(df['date'], errors='coerce')  # Set errors='coerce' to handle parsing errors\n",
    "\n",
    "# Print the first few rows of the DataFrame to verify the changes\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"theblock_policy_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping News Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trial 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping data from page 0...\n",
      "Scraping data from page 1...\n",
      "Scraping data from page 2...\n",
      "Scraping data from page 3...\n",
      "Scraping data from page 4...\n",
      "Data saved to news_data.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Define the base URL and the range of pages to scrape\n",
    "base_url = \"https://www.theblock.co/latest?start=\"\n",
    "start_page = 0\n",
    "end_page = 4\n",
    "\n",
    "# Initialize an empty list to store dictionaries\n",
    "data_list = []\n",
    "\n",
    "# Function to scrape data from a single page\n",
    "def scrape_page(page_number):\n",
    "    url = f\"{base_url}{page_number}0\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        # Find all headline and date elements\n",
    "        headlines = soup.select('div.collection__feed div.headline span')\n",
    "        dates = soup.select('div.collection__feed div.meta div.pubDate')\n",
    "        # Iterate over each headline and date and store in the list\n",
    "        for headline, date in zip(headlines, dates):\n",
    "            headline_text = headline.get_text(strip=True)\n",
    "            date_text = date.get_text(strip=True)\n",
    "            data_list.append({'headline': headline_text, 'date': date_text})\n",
    "    else:\n",
    "        print(f\"Failed to fetch page {page_number}\")\n",
    "\n",
    "# Loop through pages and scrape data\n",
    "for page_number in range(start_page, end_page + 1):\n",
    "    print(f\"Scraping data from page {page_number}...\")\n",
    "    scrape_page(page_number)\n",
    "    time.sleep(1)  # Add a delay to be polite to the server\n",
    "\n",
    "# Create DataFrame from the list of dictionaries\n",
    "df = pd.DataFrame(data_list)\n",
    "# Clean and convert the \"date\" column to datetime format\n",
    "\n",
    "#df['date'] = df['date'].str.replace(' EDT', '')  # Remove 'EDT'\n",
    "#df['date'] = df['date'].str.replace(' EST', '')  # Remove 'EDT'\n",
    "#df['date'] = pd.to_datetime(df['date'], errors='coerce')  # Convert to datetime format\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv(\"news_data.csv\", index=False)\n",
    "\n",
    "print(\"Data saved to news_data.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking with Request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to access the website. Status code: 429\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://www.theblock.co/latest?start=0\"\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    print(\"The website is accessible and scrapable.\")\n",
    "else:\n",
    "    print(f\"Failed to access the website. Status code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First try with Proxy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping data from page 0...\n",
      "Scraping data from page 1...\n",
      "Scraping data from page 2...\n",
      "Scraping data from page 3...\n",
      "Scraping data from page 4...\n",
      "Scraping data from page 5...\n",
      "Scraping data from page 6...\n",
      "Scraping data from page 7...\n",
      "Scraping data from page 8...\n",
      "Scraping data from page 9...\n",
      "Scraping data from page 10...\n",
      "Scraping data from page 11...\n",
      "Scraping data from page 12...\n",
      "Scraping data from page 13...\n",
      "Scraping data from page 14...\n",
      "Scraping data from page 15...\n",
      "Scraping data from page 16...\n",
      "Scraping data from page 17...\n",
      "Scraping data from page 18...\n",
      "Scraping data from page 19...\n",
      "Scraping data from page 20...\n",
      "Scraping data from page 21...\n",
      "Scraping data from page 22...\n",
      "Scraping data from page 23...\n",
      "Scraping data from page 24...\n",
      "Data saved to news_data.csv\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Define the base URL and the range of pages to scrape\n",
    "base_url = \"https://www.theblock.co/latest?start=\"\n",
    "start_page = 0\n",
    "end_page = 24  # Adjusted to scrape 25 pages at a time\n",
    "\n",
    "# List of proxies to rotate through\n",
    "proxies = [\n",
    "    {'http': 'http://102.130.125.86:80'},\n",
    "    {'http': 'http://85.198.13.205:80'},\n",
    "    {'http': 'http://185.110.189.166:80'},\n",
    "    {'http': 'http://103.168.254.62:8080'}\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Add more proxies as needed\n",
    "]\n",
    "\n",
    "# Initialize an empty list to store dictionaries\n",
    "data_list = []\n",
    "\n",
    "# Function to scrape data from a single page with a specified proxy\n",
    "def scrape_page(page_number, proxy):\n",
    "    url = f\"{base_url}{page_number}0\"\n",
    "    response = requests.get(url, proxies=proxy)  # Use specified proxy\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        # Find all headline and date elements\n",
    "        headlines = soup.select('div.collection__feed div.headline span')\n",
    "        dates = soup.select('div.collection__feed div.meta div.pubDate')\n",
    "        # Iterate over each headline and date and store in the list\n",
    "        for headline, date in zip(headlines, dates):\n",
    "            headline_text = headline.get_text(strip=True)\n",
    "            date_text = date.get_text(strip=True)\n",
    "            data_list.append({'headline': headline_text, 'date': date_text})\n",
    "    else:\n",
    "        print(f\"Failed to fetch page {page_number}\")\n",
    "\n",
    "# Loop through pages and scrape data\n",
    "proxy_index = 0\n",
    "for page_number in range(start_page, end_page + 1):\n",
    "    print(f\"Scraping data from page {page_number}...\")\n",
    "    # Select proxy for this batch of requests\n",
    "    proxy = proxies[proxy_index % len(proxies)]\n",
    "    try:\n",
    "        scrape_page(page_number, proxy)\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred while scraping page {page_number}: {e}\")\n",
    "        break  # Stop scraping if an error occurs\n",
    "    time.sleep(1)  # Add a delay to be polite to the server\n",
    "    proxy_index += 1\n",
    "\n",
    "# Create DataFrame from the list of dictionaries\n",
    "df = pd.DataFrame(data_list)\n",
    "# Clean and convert the \"date\" column to datetime format\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv(\"news_data.csv\", index=False)\n",
    "\n",
    "print(\"Data saved to news_data.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batching Try 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping data from pages 1-25...\n",
      "Scraping data from pages 26-50...\n",
      "Scraping data from pages 51-75...\n",
      "Scraping data from pages 76-100...\n",
      "Data saved to news_data.csv\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Define the base URL and the total number of pages to scrape\n",
    "base_url = \"https://www.theblock.co/latest?start=\"\n",
    "total_pages = 100\n",
    "pages_per_batch = 25\n",
    "\n",
    "# List of proxies to rotate through\n",
    "proxies = [\n",
    "    {'http': 'http://102.130.125.86:80'},\n",
    "    {'http': 'http://85.198.13.205:80'},\n",
    "    {'http': 'http://185.110.189.166:80'},\n",
    "    {'http': 'http://103.168.254.62:8080'}\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Add more proxies as needed\n",
    "]\n",
    "\n",
    "# Initialize an empty list to store dictionaries\n",
    "data_list = []\n",
    "\n",
    "# Function to scrape data from a single page with a specified proxy\n",
    "def scrape_page(page_number, proxy):\n",
    "    url = f\"{base_url}{page_number}0\"\n",
    "    response = requests.get(url, proxies=proxy)  # Use specified proxy\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        # Find all headline and date elements\n",
    "        headlines = soup.select('div.collection__feed div.headline span')\n",
    "        dates = soup.select('div.collection__feed div.meta div.pubDate')\n",
    "        # Iterate over each headline and date and store in the list\n",
    "        for headline, date in zip(headlines, dates):\n",
    "            headline_text = headline.get_text(strip=True)\n",
    "            date_text = date.get_text(strip=True)\n",
    "            data_list.append({'headline': headline_text, 'date': date_text})\n",
    "    else:\n",
    "        print(f\"Failed to fetch page {page_number}\")\n",
    "\n",
    "\n",
    "# Loop through pages and scrape data\n",
    "for batch_number in range(total_pages // pages_per_batch):\n",
    "    start_page = batch_number * pages_per_batch + 1\n",
    "    end_page = start_page + pages_per_batch - 1\n",
    "    print(f\"Scraping data from pages {start_page}-{end_page}...\")\n",
    "    # Select proxy for this batch of requests\n",
    "    proxy = proxies[batch_number % len(proxies)]\n",
    "    for page_number in range(start_page, end_page + 1):\n",
    "        try:\n",
    "            scrape_page(page_number, proxy)\n",
    "        except Exception as e:\n",
    "            print(f\"Error occurred while scraping page {page_number}: {e}\")\n",
    "            break  # Stop scraping if an error occurs\n",
    "        time.sleep(1)  # Add a delay to be polite to the server\n",
    "\n",
    "# Create DataFrame from the list of dictionaries\n",
    "df = pd.DataFrame(data_list)\n",
    "# Clean and convert the \"date\" column to datetime format\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv(\"news_data.csv\", index=False)\n",
    "\n",
    "print(\"Data saved to news_data.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Scraper with Proxy and Batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping data from pages 101-350...\n",
      "Now scraping 101\n",
      "Now scraping 102\n",
      "Now scraping 103\n",
      "Now scraping 104\n",
      "Now scraping 105\n",
      "Now scraping 106\n",
      "Now scraping 107\n",
      "Now scraping 108\n",
      "Now scraping 109\n",
      "Now scraping 110\n",
      "Now scraping 111\n",
      "Now scraping 112\n",
      "Now scraping 113\n",
      "Now scraping 114\n",
      "Now scraping 115\n",
      "Now scraping 116\n",
      "Now scraping 117\n",
      "Now scraping 118\n",
      "Now scraping 119\n",
      "Now scraping 120\n",
      "Now scraping 121\n",
      "Now scraping 122\n",
      "Now scraping 123\n",
      "Now scraping 124\n",
      "Now scraping 125\n",
      "Now scraping 126\n",
      "Now scraping 127\n",
      "Now scraping 128\n",
      "Now scraping 129\n",
      "Now scraping 130\n",
      "Now scraping 131\n",
      "Now scraping 132\n",
      "Now scraping 133\n",
      "Now scraping 134\n",
      "Now scraping 135\n",
      "Now scraping 136\n",
      "Now scraping 137\n",
      "Now scraping 138\n",
      "Now scraping 139\n",
      "Now scraping 140\n",
      "Now scraping 141\n",
      "Now scraping 142\n",
      "Now scraping 143\n",
      "Now scraping 144\n",
      "Now scraping 145\n",
      "Now scraping 146\n",
      "Now scraping 147\n",
      "Now scraping 148\n",
      "Now scraping 149\n",
      "Now scraping 150\n",
      "Now scraping 151\n",
      "Now scraping 152\n",
      "Now scraping 153\n",
      "Now scraping 154\n",
      "Now scraping 155\n",
      "Now scraping 156\n",
      "Now scraping 157\n",
      "Now scraping 158\n",
      "Now scraping 159\n",
      "Now scraping 160\n",
      "Now scraping 161\n",
      "Now scraping 162\n",
      "Now scraping 163\n",
      "Now scraping 164\n",
      "Now scraping 165\n",
      "Now scraping 166\n",
      "Now scraping 167\n",
      "Now scraping 168\n",
      "Now scraping 169\n",
      "Now scraping 170\n",
      "Now scraping 171\n",
      "Now scraping 172\n",
      "Now scraping 173\n",
      "Now scraping 174\n",
      "Now scraping 175\n",
      "Now scraping 176\n",
      "Now scraping 177\n",
      "Now scraping 178\n",
      "Now scraping 179\n",
      "Now scraping 180\n",
      "Now scraping 181\n",
      "Now scraping 182\n",
      "Now scraping 183\n",
      "Now scraping 184\n",
      "Now scraping 185\n",
      "Now scraping 186\n",
      "Now scraping 187\n",
      "Now scraping 188\n",
      "Now scraping 189\n",
      "Now scraping 190\n",
      "Now scraping 191\n",
      "Now scraping 192\n",
      "Now scraping 193\n",
      "Now scraping 194\n",
      "Now scraping 195\n",
      "Now scraping 196\n",
      "Now scraping 197\n",
      "Now scraping 198\n",
      "Now scraping 199\n",
      "Now scraping 200\n",
      "Now scraping 201\n",
      "Now scraping 202\n",
      "Now scraping 203\n",
      "Now scraping 204\n",
      "Now scraping 205\n",
      "Now scraping 206\n",
      "Now scraping 207\n",
      "Now scraping 208\n",
      "Now scraping 209\n",
      "Now scraping 210\n",
      "Now scraping 211\n",
      "Now scraping 212\n",
      "Now scraping 213\n",
      "Now scraping 214\n",
      "Now scraping 215\n",
      "Now scraping 216\n",
      "Now scraping 217\n",
      "Now scraping 218\n",
      "Now scraping 219\n",
      "Now scraping 220\n",
      "Now scraping 221\n",
      "Now scraping 222\n",
      "Now scraping 223\n",
      "Now scraping 224\n",
      "Now scraping 225\n",
      "Now scraping 226\n",
      "Now scraping 227\n",
      "Now scraping 228\n",
      "Now scraping 229\n",
      "Now scraping 230\n",
      "Now scraping 231\n",
      "Now scraping 232\n",
      "Now scraping 233\n",
      "Now scraping 234\n",
      "Now scraping 235\n",
      "Now scraping 236\n",
      "Now scraping 237\n",
      "Now scraping 238\n",
      "Now scraping 239\n",
      "Now scraping 240\n",
      "Now scraping 241\n",
      "Now scraping 242\n",
      "Now scraping 243\n",
      "Now scraping 244\n",
      "Now scraping 245\n",
      "Now scraping 246\n",
      "Now scraping 247\n",
      "Now scraping 248\n",
      "Now scraping 249\n",
      "Now scraping 250\n",
      "Now scraping 251\n",
      "Now scraping 252\n",
      "Now scraping 253\n",
      "Now scraping 254\n",
      "Now scraping 255\n",
      "Now scraping 256\n",
      "Now scraping 257\n",
      "Now scraping 258\n",
      "Now scraping 259\n",
      "Now scraping 260\n",
      "Now scraping 261\n",
      "Now scraping 262\n",
      "Now scraping 263\n",
      "Now scraping 264\n",
      "Now scraping 265\n",
      "Now scraping 266\n",
      "Now scraping 267\n",
      "Now scraping 268\n",
      "Now scraping 269\n",
      "Now scraping 270\n",
      "Now scraping 271\n",
      "Now scraping 272\n",
      "Now scraping 273\n",
      "Now scraping 274\n",
      "Now scraping 275\n",
      "Now scraping 276\n",
      "Now scraping 277\n",
      "Now scraping 278\n",
      "Now scraping 279\n",
      "Now scraping 280\n",
      "Now scraping 281\n",
      "Now scraping 282\n",
      "Now scraping 283\n",
      "Now scraping 284\n",
      "Now scraping 285\n",
      "Now scraping 286\n",
      "Now scraping 287\n",
      "Now scraping 288\n",
      "Now scraping 289\n",
      "Now scraping 290\n",
      "Now scraping 291\n",
      "Now scraping 292\n",
      "Now scraping 293\n",
      "Now scraping 294\n",
      "Now scraping 295\n",
      "Now scraping 296\n",
      "Now scraping 297\n",
      "Now scraping 298\n",
      "Now scraping 299\n",
      "Now scraping 300\n",
      "Now scraping 301\n",
      "Now scraping 302\n",
      "Now scraping 303\n",
      "Now scraping 304\n",
      "Now scraping 305\n",
      "Now scraping 306\n",
      "Now scraping 307\n",
      "Now scraping 308\n",
      "Now scraping 309\n",
      "Now scraping 310\n",
      "Now scraping 311\n",
      "Now scraping 312\n",
      "Now scraping 313\n",
      "Now scraping 314\n",
      "Now scraping 315\n",
      "Now scraping 316\n",
      "Now scraping 317\n",
      "Now scraping 318\n",
      "Now scraping 319\n",
      "Now scraping 320\n",
      "Now scraping 321\n",
      "Now scraping 322\n",
      "Now scraping 323\n",
      "Now scraping 324\n",
      "Now scraping 325\n",
      "Now scraping 326\n",
      "Now scraping 327\n",
      "Now scraping 328\n",
      "Now scraping 329\n",
      "Now scraping 330\n",
      "Now scraping 331\n",
      "Now scraping 332\n",
      "Now scraping 333\n",
      "Now scraping 334\n",
      "Now scraping 335\n",
      "Now scraping 336\n",
      "Now scraping 337\n",
      "Now scraping 338\n",
      "Now scraping 339\n",
      "Now scraping 340\n",
      "Now scraping 341\n",
      "Now scraping 342\n",
      "Now scraping 343\n",
      "Now scraping 344\n",
      "Now scraping 345\n",
      "Now scraping 346\n",
      "Now scraping 347\n",
      "Now scraping 348\n",
      "Now scraping 349\n",
      "Now scraping 350\n",
      "Scraping data from pages 351-600...\n",
      "Now scraping 351\n",
      "Now scraping 352\n",
      "Now scraping 353\n",
      "Now scraping 354\n",
      "Now scraping 355\n",
      "Now scraping 356\n",
      "Now scraping 357\n",
      "Now scraping 358\n",
      "Now scraping 359\n",
      "Now scraping 360\n",
      "Now scraping 361\n",
      "Now scraping 362\n",
      "Now scraping 363\n",
      "Now scraping 364\n",
      "Now scraping 365\n",
      "Now scraping 366\n",
      "Now scraping 367\n",
      "Now scraping 368\n",
      "Now scraping 369\n",
      "Now scraping 370\n",
      "Now scraping 371\n",
      "Now scraping 372\n",
      "Now scraping 373\n",
      "Now scraping 374\n",
      "Now scraping 375\n",
      "Now scraping 376\n",
      "Now scraping 377\n",
      "Now scraping 378\n",
      "Now scraping 379\n",
      "Failed to fetch page 380\n",
      "Now scraping 380\n",
      "Failed to fetch page 381\n",
      "Now scraping 381\n",
      "Failed to fetch page 382\n",
      "Now scraping 382\n",
      "Failed to fetch page 383\n",
      "Now scraping 383\n",
      "Failed to fetch page 384\n",
      "Now scraping 384\n",
      "Failed to fetch page 385\n",
      "Now scraping 385\n",
      "Failed to fetch page 386\n",
      "Now scraping 386\n",
      "Failed to fetch page 387\n",
      "Now scraping 387\n",
      "Failed to fetch page 388\n",
      "Now scraping 388\n",
      "Failed to fetch page 389\n",
      "Now scraping 389\n",
      "Failed to fetch page 390\n",
      "Now scraping 390\n",
      "Failed to fetch page 391\n",
      "Now scraping 391\n",
      "Failed to fetch page 392\n",
      "Now scraping 392\n",
      "Failed to fetch page 393\n",
      "Now scraping 393\n",
      "Failed to fetch page 394\n",
      "Now scraping 394\n",
      "Failed to fetch page 395\n",
      "Now scraping 395\n",
      "Failed to fetch page 396\n",
      "Now scraping 396\n",
      "Failed to fetch page 397\n",
      "Now scraping 397\n",
      "Failed to fetch page 398\n",
      "Now scraping 398\n",
      "Failed to fetch page 399\n",
      "Now scraping 399\n",
      "Failed to fetch page 400\n",
      "Now scraping 400\n",
      "Failed to fetch page 401\n",
      "Now scraping 401\n",
      "Failed to fetch page 402\n",
      "Now scraping 402\n",
      "Failed to fetch page 403\n",
      "Now scraping 403\n",
      "Failed to fetch page 404\n",
      "Now scraping 404\n",
      "Failed to fetch page 405\n",
      "Now scraping 405\n",
      "Failed to fetch page 406\n",
      "Now scraping 406\n",
      "Failed to fetch page 407\n",
      "Now scraping 407\n",
      "Failed to fetch page 408\n",
      "Now scraping 408\n",
      "Failed to fetch page 409\n",
      "Now scraping 409\n",
      "Failed to fetch page 410\n",
      "Now scraping 410\n",
      "Failed to fetch page 411\n",
      "Now scraping 411\n",
      "Failed to fetch page 412\n",
      "Now scraping 412\n",
      "Failed to fetch page 413\n",
      "Now scraping 413\n",
      "Failed to fetch page 414\n",
      "Now scraping 414\n",
      "Failed to fetch page 415\n",
      "Now scraping 415\n",
      "Failed to fetch page 416\n",
      "Now scraping 416\n",
      "Failed to fetch page 417\n",
      "Now scraping 417\n",
      "Failed to fetch page 418\n",
      "Now scraping 418\n",
      "Failed to fetch page 419\n",
      "Now scraping 419\n",
      "Failed to fetch page 420\n",
      "Now scraping 420\n",
      "Failed to fetch page 421\n",
      "Now scraping 421\n",
      "Failed to fetch page 422\n",
      "Now scraping 422\n",
      "Failed to fetch page 423\n",
      "Now scraping 423\n",
      "Failed to fetch page 424\n",
      "Now scraping 424\n",
      "Failed to fetch page 425\n",
      "Now scraping 425\n",
      "Failed to fetch page 426\n",
      "Now scraping 426\n",
      "Failed to fetch page 427\n",
      "Now scraping 427\n",
      "Failed to fetch page 428\n",
      "Now scraping 428\n",
      "Failed to fetch page 429\n",
      "Now scraping 429\n",
      "Failed to fetch page 430\n",
      "Now scraping 430\n",
      "Failed to fetch page 431\n",
      "Now scraping 431\n",
      "Failed to fetch page 432\n",
      "Now scraping 432\n",
      "Failed to fetch page 433\n",
      "Now scraping 433\n",
      "Failed to fetch page 434\n",
      "Now scraping 434\n",
      "Failed to fetch page 435\n",
      "Now scraping 435\n",
      "Failed to fetch page 436\n",
      "Now scraping 436\n",
      "Failed to fetch page 437\n",
      "Now scraping 437\n",
      "Failed to fetch page 438\n",
      "Now scraping 438\n",
      "Failed to fetch page 439\n",
      "Now scraping 439\n",
      "Failed to fetch page 440\n",
      "Now scraping 440\n",
      "Failed to fetch page 441\n",
      "Now scraping 441\n",
      "Failed to fetch page 442\n",
      "Now scraping 442\n",
      "Failed to fetch page 443\n",
      "Now scraping 443\n",
      "Failed to fetch page 444\n",
      "Now scraping 444\n",
      "Failed to fetch page 445\n",
      "Now scraping 445\n",
      "Failed to fetch page 446\n",
      "Now scraping 446\n",
      "Failed to fetch page 447\n",
      "Now scraping 447\n",
      "Failed to fetch page 448\n",
      "Now scraping 448\n",
      "Failed to fetch page 449\n",
      "Now scraping 449\n",
      "Failed to fetch page 450\n",
      "Now scraping 450\n",
      "Failed to fetch page 451\n",
      "Now scraping 451\n",
      "Failed to fetch page 452\n",
      "Now scraping 452\n",
      "Failed to fetch page 453\n",
      "Now scraping 453\n",
      "Failed to fetch page 454\n",
      "Now scraping 454\n",
      "Failed to fetch page 455\n",
      "Now scraping 455\n",
      "Failed to fetch page 456\n",
      "Now scraping 456\n",
      "Failed to fetch page 457\n",
      "Now scraping 457\n",
      "Failed to fetch page 458\n",
      "Now scraping 458\n",
      "Failed to fetch page 459\n",
      "Now scraping 459\n",
      "Failed to fetch page 460\n",
      "Now scraping 460\n",
      "Failed to fetch page 461\n",
      "Now scraping 461\n",
      "Failed to fetch page 462\n",
      "Now scraping 462\n",
      "Failed to fetch page 463\n",
      "Now scraping 463\n",
      "Failed to fetch page 464\n",
      "Now scraping 464\n",
      "Failed to fetch page 465\n",
      "Now scraping 465\n",
      "Failed to fetch page 466\n",
      "Now scraping 466\n",
      "Failed to fetch page 467\n",
      "Now scraping 467\n",
      "Failed to fetch page 468\n",
      "Now scraping 468\n",
      "Failed to fetch page 469\n",
      "Now scraping 469\n",
      "Failed to fetch page 470\n",
      "Now scraping 470\n",
      "Failed to fetch page 471\n",
      "Now scraping 471\n",
      "Failed to fetch page 472\n",
      "Now scraping 472\n",
      "Failed to fetch page 473\n",
      "Now scraping 473\n",
      "Failed to fetch page 474\n",
      "Now scraping 474\n",
      "Failed to fetch page 475\n",
      "Now scraping 475\n",
      "Failed to fetch page 476\n",
      "Now scraping 476\n",
      "Failed to fetch page 477\n",
      "Now scraping 477\n",
      "Failed to fetch page 478\n",
      "Now scraping 478\n",
      "Failed to fetch page 479\n",
      "Now scraping 479\n",
      "Failed to fetch page 480\n",
      "Now scraping 480\n",
      "Failed to fetch page 481\n",
      "Now scraping 481\n",
      "Failed to fetch page 482\n",
      "Now scraping 482\n",
      "Failed to fetch page 483\n",
      "Now scraping 483\n",
      "Failed to fetch page 484\n",
      "Now scraping 484\n",
      "Failed to fetch page 485\n",
      "Now scraping 485\n",
      "Failed to fetch page 486\n",
      "Now scraping 486\n",
      "Failed to fetch page 487\n",
      "Now scraping 487\n",
      "Failed to fetch page 488\n",
      "Now scraping 488\n",
      "Failed to fetch page 489\n",
      "Now scraping 489\n",
      "Failed to fetch page 490\n",
      "Now scraping 490\n",
      "Failed to fetch page 491\n",
      "Now scraping 491\n",
      "Failed to fetch page 492\n",
      "Now scraping 492\n",
      "Failed to fetch page 493\n",
      "Now scraping 493\n",
      "Failed to fetch page 494\n",
      "Now scraping 494\n",
      "Failed to fetch page 495\n",
      "Now scraping 495\n",
      "Failed to fetch page 496\n",
      "Now scraping 496\n",
      "Failed to fetch page 497\n",
      "Now scraping 497\n",
      "Failed to fetch page 498\n",
      "Now scraping 498\n",
      "Failed to fetch page 499\n",
      "Now scraping 499\n",
      "Failed to fetch page 500\n",
      "Now scraping 500\n",
      "Failed to fetch page 501\n",
      "Now scraping 501\n",
      "Failed to fetch page 502\n",
      "Now scraping 502\n",
      "Failed to fetch page 503\n",
      "Now scraping 503\n",
      "Failed to fetch page 504\n",
      "Now scraping 504\n",
      "Failed to fetch page 505\n",
      "Now scraping 505\n",
      "Failed to fetch page 506\n",
      "Now scraping 506\n",
      "Failed to fetch page 507\n",
      "Now scraping 507\n",
      "Failed to fetch page 508\n",
      "Now scraping 508\n",
      "Failed to fetch page 509\n",
      "Now scraping 509\n",
      "Failed to fetch page 510\n",
      "Now scraping 510\n",
      "Failed to fetch page 511\n",
      "Now scraping 511\n",
      "Failed to fetch page 512\n",
      "Now scraping 512\n",
      "Failed to fetch page 513\n",
      "Now scraping 513\n",
      "Failed to fetch page 514\n",
      "Now scraping 514\n",
      "Failed to fetch page 515\n",
      "Now scraping 515\n",
      "Failed to fetch page 516\n",
      "Now scraping 516\n",
      "Failed to fetch page 517\n",
      "Now scraping 517\n",
      "Failed to fetch page 518\n",
      "Now scraping 518\n",
      "Failed to fetch page 519\n",
      "Now scraping 519\n",
      "Failed to fetch page 520\n",
      "Now scraping 520\n",
      "Failed to fetch page 521\n",
      "Now scraping 521\n",
      "Failed to fetch page 522\n",
      "Now scraping 522\n",
      "Failed to fetch page 523\n",
      "Now scraping 523\n",
      "Failed to fetch page 524\n",
      "Now scraping 524\n",
      "Failed to fetch page 525\n",
      "Now scraping 525\n",
      "Failed to fetch page 526\n",
      "Now scraping 526\n",
      "Failed to fetch page 527\n",
      "Now scraping 527\n",
      "Failed to fetch page 528\n",
      "Now scraping 528\n",
      "Failed to fetch page 529\n",
      "Now scraping 529\n",
      "Failed to fetch page 530\n",
      "Now scraping 530\n",
      "Failed to fetch page 531\n",
      "Now scraping 531\n",
      "Failed to fetch page 532\n",
      "Now scraping 532\n",
      "Failed to fetch page 533\n",
      "Now scraping 533\n",
      "Failed to fetch page 534\n",
      "Now scraping 534\n",
      "Failed to fetch page 535\n",
      "Now scraping 535\n",
      "Failed to fetch page 536\n",
      "Now scraping 536\n",
      "Failed to fetch page 537\n",
      "Now scraping 537\n",
      "Failed to fetch page 538\n",
      "Now scraping 538\n",
      "Failed to fetch page 539\n",
      "Now scraping 539\n",
      "Failed to fetch page 540\n",
      "Now scraping 540\n",
      "Failed to fetch page 541\n",
      "Now scraping 541\n",
      "Failed to fetch page 542\n",
      "Now scraping 542\n",
      "Failed to fetch page 543\n",
      "Now scraping 543\n",
      "Failed to fetch page 544\n",
      "Now scraping 544\n",
      "Failed to fetch page 545\n",
      "Now scraping 545\n",
      "Failed to fetch page 546\n",
      "Now scraping 546\n",
      "Failed to fetch page 547\n",
      "Now scraping 547\n",
      "Failed to fetch page 548\n",
      "Now scraping 548\n",
      "Failed to fetch page 549\n",
      "Now scraping 549\n",
      "Failed to fetch page 550\n",
      "Now scraping 550\n",
      "Failed to fetch page 551\n",
      "Now scraping 551\n",
      "Failed to fetch page 552\n",
      "Now scraping 552\n",
      "Failed to fetch page 553\n",
      "Now scraping 553\n",
      "Failed to fetch page 554\n",
      "Now scraping 554\n",
      "Failed to fetch page 555\n",
      "Now scraping 555\n",
      "Failed to fetch page 556\n",
      "Now scraping 556\n",
      "Failed to fetch page 557\n",
      "Now scraping 557\n",
      "Failed to fetch page 558\n",
      "Now scraping 558\n",
      "Failed to fetch page 559\n",
      "Now scraping 559\n",
      "Failed to fetch page 560\n",
      "Now scraping 560\n",
      "Failed to fetch page 561\n",
      "Now scraping 561\n",
      "Failed to fetch page 562\n",
      "Now scraping 562\n",
      "Failed to fetch page 563\n",
      "Now scraping 563\n",
      "Failed to fetch page 564\n",
      "Now scraping 564\n",
      "Failed to fetch page 565\n",
      "Now scraping 565\n",
      "Failed to fetch page 566\n",
      "Now scraping 566\n",
      "Failed to fetch page 567\n",
      "Now scraping 567\n",
      "Failed to fetch page 568\n",
      "Now scraping 568\n",
      "Now scraping 569\n",
      "Failed to fetch page 570\n",
      "Now scraping 570\n",
      "Failed to fetch page 571\n",
      "Now scraping 571\n",
      "Failed to fetch page 572\n",
      "Now scraping 572\n",
      "Failed to fetch page 573\n",
      "Now scraping 573\n",
      "Failed to fetch page 574\n",
      "Now scraping 574\n",
      "Failed to fetch page 575\n",
      "Now scraping 575\n",
      "Failed to fetch page 576\n",
      "Now scraping 576\n",
      "Failed to fetch page 577\n",
      "Now scraping 577\n",
      "Now scraping 578\n",
      "Failed to fetch page 579\n",
      "Now scraping 579\n",
      "Failed to fetch page 580\n",
      "Now scraping 580\n",
      "Failed to fetch page 581\n",
      "Now scraping 581\n",
      "Failed to fetch page 582\n",
      "Now scraping 582\n",
      "Failed to fetch page 583\n",
      "Now scraping 583\n",
      "Failed to fetch page 584\n",
      "Now scraping 584\n",
      "Now scraping 585\n",
      "Failed to fetch page 586\n",
      "Now scraping 586\n",
      "Failed to fetch page 587\n",
      "Now scraping 587\n",
      "Failed to fetch page 588\n",
      "Now scraping 588\n",
      "Failed to fetch page 589\n",
      "Now scraping 589\n",
      "Failed to fetch page 590\n",
      "Now scraping 590\n",
      "Failed to fetch page 591\n",
      "Now scraping 591\n",
      "Failed to fetch page 592\n",
      "Now scraping 592\n",
      "Failed to fetch page 593\n",
      "Now scraping 593\n",
      "Failed to fetch page 594\n",
      "Now scraping 594\n",
      "Failed to fetch page 595\n",
      "Now scraping 595\n",
      "Failed to fetch page 596\n",
      "Now scraping 596\n",
      "Failed to fetch page 597\n",
      "Now scraping 597\n",
      "Failed to fetch page 598\n",
      "Now scraping 598\n",
      "Failed to fetch page 599\n",
      "Now scraping 599\n",
      "Failed to fetch page 600\n",
      "Now scraping 600\n",
      "Scraping data from pages 601-850...\n",
      "Failed to fetch page 601\n",
      "Now scraping 601\n",
      "Failed to fetch page 602\n",
      "Now scraping 602\n",
      "Now scraping 603\n",
      "Failed to fetch page 604\n",
      "Now scraping 604\n",
      "Failed to fetch page 605\n",
      "Now scraping 605\n",
      "Failed to fetch page 606\n",
      "Now scraping 606\n",
      "Failed to fetch page 607\n",
      "Now scraping 607\n",
      "Now scraping 608\n",
      "Failed to fetch page 609\n",
      "Now scraping 609\n",
      "Failed to fetch page 610\n",
      "Now scraping 610\n",
      "Failed to fetch page 611\n",
      "Now scraping 611\n",
      "Failed to fetch page 612\n",
      "Now scraping 612\n",
      "Failed to fetch page 613\n",
      "Now scraping 613\n",
      "Failed to fetch page 614\n",
      "Now scraping 614\n",
      "Failed to fetch page 615\n",
      "Now scraping 615\n",
      "Now scraping 616\n",
      "Failed to fetch page 617\n",
      "Now scraping 617\n",
      "Failed to fetch page 618\n",
      "Now scraping 618\n",
      "Failed to fetch page 619\n",
      "Now scraping 619\n",
      "Failed to fetch page 620\n",
      "Now scraping 620\n",
      "Failed to fetch page 621\n",
      "Now scraping 621\n",
      "Failed to fetch page 622\n",
      "Now scraping 622\n",
      "Failed to fetch page 623\n",
      "Now scraping 623\n",
      "Now scraping 624\n",
      "Failed to fetch page 625\n",
      "Now scraping 625\n",
      "Failed to fetch page 626\n",
      "Now scraping 626\n",
      "Failed to fetch page 627\n",
      "Now scraping 627\n",
      "Failed to fetch page 628\n",
      "Now scraping 628\n",
      "Now scraping 629\n",
      "Failed to fetch page 630\n",
      "Now scraping 630\n",
      "Now scraping 631\n",
      "Failed to fetch page 632\n",
      "Now scraping 632\n",
      "Failed to fetch page 633\n",
      "Now scraping 633\n",
      "Failed to fetch page 634\n",
      "Now scraping 634\n",
      "Now scraping 635\n",
      "Failed to fetch page 636\n",
      "Now scraping 636\n",
      "Failed to fetch page 637\n",
      "Now scraping 637\n",
      "Failed to fetch page 638\n",
      "Now scraping 638\n",
      "Failed to fetch page 639\n",
      "Now scraping 639\n",
      "Failed to fetch page 640\n",
      "Now scraping 640\n",
      "Failed to fetch page 641\n",
      "Now scraping 641\n",
      "Failed to fetch page 642\n",
      "Now scraping 642\n",
      "Failed to fetch page 643\n",
      "Now scraping 643\n",
      "Failed to fetch page 644\n",
      "Now scraping 644\n",
      "Now scraping 645\n",
      "Failed to fetch page 646\n",
      "Now scraping 646\n",
      "Now scraping 647\n",
      "Failed to fetch page 648\n",
      "Now scraping 648\n",
      "Failed to fetch page 649\n",
      "Now scraping 649\n",
      "Failed to fetch page 650\n",
      "Now scraping 650\n",
      "Failed to fetch page 651\n",
      "Now scraping 651\n",
      "Failed to fetch page 652\n",
      "Now scraping 652\n",
      "Failed to fetch page 653\n",
      "Now scraping 653\n",
      "Failed to fetch page 654\n",
      "Now scraping 654\n",
      "Failed to fetch page 655\n",
      "Now scraping 655\n",
      "Failed to fetch page 656\n",
      "Now scraping 656\n",
      "Failed to fetch page 657\n",
      "Now scraping 657\n",
      "Now scraping 658\n",
      "Failed to fetch page 659\n",
      "Now scraping 659\n",
      "Failed to fetch page 660\n",
      "Now scraping 660\n",
      "Failed to fetch page 661\n",
      "Now scraping 661\n",
      "Failed to fetch page 662\n",
      "Now scraping 662\n",
      "Failed to fetch page 663\n",
      "Now scraping 663\n",
      "Failed to fetch page 664\n",
      "Now scraping 664\n",
      "Failed to fetch page 665\n",
      "Now scraping 665\n",
      "Failed to fetch page 666\n",
      "Now scraping 666\n",
      "Failed to fetch page 667\n",
      "Now scraping 667\n",
      "Failed to fetch page 668\n",
      "Now scraping 668\n",
      "Failed to fetch page 669\n",
      "Now scraping 669\n",
      "Failed to fetch page 670\n",
      "Now scraping 670\n",
      "Failed to fetch page 671\n",
      "Now scraping 671\n",
      "Failed to fetch page 672\n",
      "Now scraping 672\n",
      "Now scraping 673\n",
      "Failed to fetch page 674\n",
      "Now scraping 674\n",
      "Failed to fetch page 675\n",
      "Now scraping 675\n",
      "Failed to fetch page 676\n",
      "Now scraping 676\n",
      "Failed to fetch page 677\n",
      "Now scraping 677\n",
      "Failed to fetch page 678\n",
      "Now scraping 678\n",
      "Now scraping 679\n",
      "Failed to fetch page 680\n",
      "Now scraping 680\n",
      "Failed to fetch page 681\n",
      "Now scraping 681\n",
      "Failed to fetch page 682\n",
      "Now scraping 682\n",
      "Failed to fetch page 683\n",
      "Now scraping 683\n",
      "Failed to fetch page 684\n",
      "Now scraping 684\n",
      "Failed to fetch page 685\n",
      "Now scraping 685\n",
      "Failed to fetch page 686\n",
      "Now scraping 686\n",
      "Failed to fetch page 687\n",
      "Now scraping 687\n",
      "Failed to fetch page 688\n",
      "Now scraping 688\n",
      "Failed to fetch page 689\n",
      "Now scraping 689\n",
      "Now scraping 690\n",
      "Failed to fetch page 691\n",
      "Now scraping 691\n",
      "Failed to fetch page 692\n",
      "Now scraping 692\n",
      "Failed to fetch page 693\n",
      "Now scraping 693\n",
      "Failed to fetch page 694\n",
      "Now scraping 694\n",
      "Failed to fetch page 695\n",
      "Now scraping 695\n",
      "Failed to fetch page 696\n",
      "Now scraping 696\n",
      "Failed to fetch page 697\n",
      "Now scraping 697\n",
      "Failed to fetch page 698\n",
      "Now scraping 698\n",
      "Failed to fetch page 699\n",
      "Now scraping 699\n",
      "Failed to fetch page 700\n",
      "Now scraping 700\n",
      "Failed to fetch page 701\n",
      "Now scraping 701\n",
      "Failed to fetch page 702\n",
      "Now scraping 702\n",
      "Failed to fetch page 703\n",
      "Now scraping 703\n",
      "Failed to fetch page 704\n",
      "Now scraping 704\n",
      "Failed to fetch page 705\n",
      "Now scraping 705\n",
      "Failed to fetch page 706\n",
      "Now scraping 706\n",
      "Failed to fetch page 707\n",
      "Now scraping 707\n",
      "Failed to fetch page 708\n",
      "Now scraping 708\n",
      "Failed to fetch page 709\n",
      "Now scraping 709\n",
      "Failed to fetch page 710\n",
      "Now scraping 710\n",
      "Failed to fetch page 711\n",
      "Now scraping 711\n",
      "Failed to fetch page 712\n",
      "Now scraping 712\n",
      "Failed to fetch page 713\n",
      "Now scraping 713\n",
      "Failed to fetch page 714\n",
      "Now scraping 714\n",
      "Failed to fetch page 715\n",
      "Now scraping 715\n",
      "Now scraping 716\n",
      "Failed to fetch page 717\n",
      "Now scraping 717\n",
      "Failed to fetch page 718\n",
      "Now scraping 718\n",
      "Failed to fetch page 719\n",
      "Now scraping 719\n",
      "Failed to fetch page 720\n",
      "Now scraping 720\n",
      "Failed to fetch page 721\n",
      "Now scraping 721\n",
      "Failed to fetch page 722\n",
      "Now scraping 722\n",
      "Failed to fetch page 723\n",
      "Now scraping 723\n",
      "Failed to fetch page 724\n",
      "Now scraping 724\n",
      "Now scraping 725\n",
      "Failed to fetch page 726\n",
      "Now scraping 726\n",
      "Failed to fetch page 727\n",
      "Now scraping 727\n",
      "Failed to fetch page 728\n",
      "Now scraping 728\n",
      "Now scraping 729\n",
      "Failed to fetch page 730\n",
      "Now scraping 730\n",
      "Failed to fetch page 731\n",
      "Now scraping 731\n",
      "Failed to fetch page 732\n",
      "Now scraping 732\n",
      "Failed to fetch page 733\n",
      "Now scraping 733\n",
      "Failed to fetch page 734\n",
      "Now scraping 734\n",
      "Failed to fetch page 735\n",
      "Now scraping 735\n",
      "Failed to fetch page 736\n",
      "Now scraping 736\n",
      "Now scraping 737\n",
      "Failed to fetch page 738\n",
      "Now scraping 738\n",
      "Failed to fetch page 739\n",
      "Now scraping 739\n",
      "Now scraping 740\n",
      "Failed to fetch page 741\n",
      "Now scraping 741\n",
      "Failed to fetch page 742\n",
      "Now scraping 742\n",
      "Failed to fetch page 743\n",
      "Now scraping 743\n",
      "Failed to fetch page 744\n",
      "Now scraping 744\n",
      "Failed to fetch page 745\n",
      "Now scraping 745\n",
      "Failed to fetch page 746\n",
      "Now scraping 746\n",
      "Failed to fetch page 747\n",
      "Now scraping 747\n",
      "Failed to fetch page 748\n",
      "Now scraping 748\n",
      "Failed to fetch page 749\n",
      "Now scraping 749\n",
      "Failed to fetch page 750\n",
      "Now scraping 750\n",
      "Failed to fetch page 751\n",
      "Now scraping 751\n",
      "Failed to fetch page 752\n",
      "Now scraping 752\n",
      "Now scraping 753\n",
      "Failed to fetch page 754\n",
      "Now scraping 754\n",
      "Failed to fetch page 755\n",
      "Now scraping 755\n",
      "Failed to fetch page 756\n",
      "Now scraping 756\n",
      "Failed to fetch page 757\n",
      "Now scraping 757\n",
      "Failed to fetch page 758\n",
      "Now scraping 758\n",
      "Failed to fetch page 759\n",
      "Now scraping 759\n",
      "Failed to fetch page 760\n",
      "Now scraping 760\n",
      "Failed to fetch page 761\n",
      "Now scraping 761\n",
      "Failed to fetch page 762\n",
      "Now scraping 762\n",
      "Failed to fetch page 763\n",
      "Now scraping 763\n",
      "Failed to fetch page 764\n",
      "Now scraping 764\n",
      "Failed to fetch page 765\n",
      "Now scraping 765\n",
      "Failed to fetch page 766\n",
      "Now scraping 766\n",
      "Now scraping 767\n",
      "Failed to fetch page 768\n",
      "Now scraping 768\n",
      "Failed to fetch page 769\n",
      "Now scraping 769\n",
      "Failed to fetch page 770\n",
      "Now scraping 770\n",
      "Now scraping 771\n",
      "Now scraping 772\n",
      "Failed to fetch page 773\n",
      "Now scraping 773\n",
      "Now scraping 774\n",
      "Failed to fetch page 775\n",
      "Now scraping 775\n",
      "Failed to fetch page 776\n",
      "Now scraping 776\n",
      "Failed to fetch page 777\n",
      "Now scraping 777\n",
      "Failed to fetch page 778\n",
      "Now scraping 778\n",
      "Failed to fetch page 779\n",
      "Now scraping 779\n",
      "Failed to fetch page 780\n",
      "Now scraping 780\n",
      "Failed to fetch page 781\n",
      "Now scraping 781\n",
      "Failed to fetch page 782\n",
      "Now scraping 782\n",
      "Failed to fetch page 783\n",
      "Now scraping 783\n",
      "Failed to fetch page 784\n",
      "Now scraping 784\n",
      "Failed to fetch page 785\n",
      "Now scraping 785\n",
      "Failed to fetch page 786\n",
      "Now scraping 786\n",
      "Failed to fetch page 787\n",
      "Now scraping 787\n",
      "Failed to fetch page 788\n",
      "Now scraping 788\n",
      "Failed to fetch page 789\n",
      "Now scraping 789\n",
      "Failed to fetch page 790\n",
      "Now scraping 790\n",
      "Failed to fetch page 791\n",
      "Now scraping 791\n",
      "Now scraping 792\n",
      "Failed to fetch page 793\n",
      "Now scraping 793\n",
      "Failed to fetch page 794\n",
      "Now scraping 794\n",
      "Failed to fetch page 795\n",
      "Now scraping 795\n",
      "Failed to fetch page 796\n",
      "Now scraping 796\n",
      "Failed to fetch page 797\n",
      "Now scraping 797\n",
      "Failed to fetch page 798\n",
      "Now scraping 798\n",
      "Failed to fetch page 799\n",
      "Now scraping 799\n",
      "Failed to fetch page 800\n",
      "Now scraping 800\n",
      "Failed to fetch page 801\n",
      "Now scraping 801\n",
      "Now scraping 802\n",
      "Failed to fetch page 803\n",
      "Now scraping 803\n",
      "Failed to fetch page 804\n",
      "Now scraping 804\n",
      "Failed to fetch page 805\n",
      "Now scraping 805\n",
      "Failed to fetch page 806\n",
      "Now scraping 806\n",
      "Failed to fetch page 807\n",
      "Now scraping 807\n",
      "Failed to fetch page 808\n",
      "Now scraping 808\n",
      "Failed to fetch page 809\n",
      "Now scraping 809\n",
      "Failed to fetch page 810\n",
      "Now scraping 810\n",
      "Failed to fetch page 811\n",
      "Now scraping 811\n",
      "Now scraping 812\n",
      "Failed to fetch page 813\n",
      "Now scraping 813\n",
      "Failed to fetch page 814\n",
      "Now scraping 814\n",
      "Failed to fetch page 815\n",
      "Now scraping 815\n",
      "Failed to fetch page 816\n",
      "Now scraping 816\n",
      "Failed to fetch page 817\n",
      "Now scraping 817\n",
      "Failed to fetch page 818\n",
      "Now scraping 818\n",
      "Failed to fetch page 819\n",
      "Now scraping 819\n",
      "Failed to fetch page 820\n",
      "Now scraping 820\n",
      "Failed to fetch page 821\n",
      "Now scraping 821\n",
      "Failed to fetch page 822\n",
      "Now scraping 822\n",
      "Failed to fetch page 823\n",
      "Now scraping 823\n",
      "Failed to fetch page 824\n",
      "Now scraping 824\n",
      "Failed to fetch page 825\n",
      "Now scraping 825\n",
      "Failed to fetch page 826\n",
      "Now scraping 826\n",
      "Failed to fetch page 827\n",
      "Now scraping 827\n",
      "Failed to fetch page 828\n",
      "Now scraping 828\n",
      "Failed to fetch page 829\n",
      "Now scraping 829\n",
      "Failed to fetch page 830\n",
      "Now scraping 830\n",
      "Failed to fetch page 831\n",
      "Now scraping 831\n",
      "Failed to fetch page 832\n",
      "Now scraping 832\n",
      "Failed to fetch page 833\n",
      "Now scraping 833\n",
      "Failed to fetch page 834\n",
      "Now scraping 834\n",
      "Failed to fetch page 835\n",
      "Now scraping 835\n",
      "Failed to fetch page 836\n",
      "Now scraping 836\n",
      "Failed to fetch page 837\n",
      "Now scraping 837\n",
      "Failed to fetch page 838\n",
      "Now scraping 838\n",
      "Failed to fetch page 839\n",
      "Now scraping 839\n",
      "Failed to fetch page 840\n",
      "Now scraping 840\n",
      "Failed to fetch page 841\n",
      "Now scraping 841\n",
      "Failed to fetch page 842\n",
      "Now scraping 842\n",
      "Failed to fetch page 843\n",
      "Now scraping 843\n",
      "Failed to fetch page 844\n",
      "Now scraping 844\n",
      "Failed to fetch page 845\n",
      "Now scraping 845\n",
      "Failed to fetch page 846\n",
      "Now scraping 846\n",
      "Failed to fetch page 847\n",
      "Now scraping 847\n",
      "Failed to fetch page 848\n",
      "Now scraping 848\n",
      "Failed to fetch page 849\n",
      "Now scraping 849\n",
      "Failed to fetch page 850\n",
      "Now scraping 850\n",
      "Scraping data from pages 851-1100...\n",
      "Failed to fetch page 851\n",
      "Now scraping 851\n",
      "Failed to fetch page 852\n",
      "Now scraping 852\n",
      "Failed to fetch page 853\n",
      "Now scraping 853\n",
      "Failed to fetch page 854\n",
      "Now scraping 854\n",
      "Failed to fetch page 855\n",
      "Now scraping 855\n",
      "Failed to fetch page 856\n",
      "Now scraping 856\n",
      "Failed to fetch page 857\n",
      "Now scraping 857\n",
      "Failed to fetch page 858\n",
      "Now scraping 858\n",
      "Failed to fetch page 859\n",
      "Now scraping 859\n",
      "Failed to fetch page 860\n",
      "Now scraping 860\n",
      "Failed to fetch page 861\n",
      "Now scraping 861\n",
      "Now scraping 862\n",
      "Failed to fetch page 863\n",
      "Now scraping 863\n",
      "Failed to fetch page 864\n",
      "Now scraping 864\n",
      "Failed to fetch page 865\n",
      "Now scraping 865\n",
      "Now scraping 866\n",
      "Failed to fetch page 867\n",
      "Now scraping 867\n",
      "Failed to fetch page 868\n",
      "Now scraping 868\n",
      "Failed to fetch page 869\n",
      "Now scraping 869\n",
      "Failed to fetch page 870\n",
      "Now scraping 870\n",
      "Failed to fetch page 871\n",
      "Now scraping 871\n",
      "Failed to fetch page 872\n",
      "Now scraping 872\n",
      "Failed to fetch page 873\n",
      "Now scraping 873\n",
      "Failed to fetch page 874\n",
      "Now scraping 874\n",
      "Failed to fetch page 875\n",
      "Now scraping 875\n",
      "Failed to fetch page 876\n",
      "Now scraping 876\n",
      "Failed to fetch page 877\n",
      "Now scraping 877\n",
      "Failed to fetch page 878\n",
      "Now scraping 878\n",
      "Now scraping 879\n",
      "Failed to fetch page 880\n",
      "Now scraping 880\n",
      "Now scraping 881\n",
      "Failed to fetch page 882\n",
      "Now scraping 882\n",
      "Failed to fetch page 883\n",
      "Now scraping 883\n",
      "Now scraping 884\n",
      "Failed to fetch page 885\n",
      "Now scraping 885\n",
      "Failed to fetch page 886\n",
      "Now scraping 886\n",
      "Failed to fetch page 887\n",
      "Now scraping 887\n",
      "Failed to fetch page 888\n",
      "Now scraping 888\n",
      "Failed to fetch page 889\n",
      "Now scraping 889\n",
      "Now scraping 890\n",
      "Failed to fetch page 891\n",
      "Now scraping 891\n",
      "Failed to fetch page 892\n",
      "Now scraping 892\n",
      "Failed to fetch page 893\n",
      "Now scraping 893\n",
      "Now scraping 894\n",
      "Failed to fetch page 895\n",
      "Now scraping 895\n",
      "Now scraping 896\n",
      "Failed to fetch page 897\n",
      "Now scraping 897\n",
      "Failed to fetch page 898\n",
      "Now scraping 898\n",
      "Failed to fetch page 899\n",
      "Now scraping 899\n",
      "Failed to fetch page 900\n",
      "Now scraping 900\n",
      "Failed to fetch page 901\n",
      "Now scraping 901\n",
      "Failed to fetch page 902\n",
      "Now scraping 902\n",
      "Failed to fetch page 903\n",
      "Now scraping 903\n",
      "Failed to fetch page 904\n",
      "Now scraping 904\n",
      "Now scraping 905\n",
      "Failed to fetch page 906\n",
      "Now scraping 906\n",
      "Failed to fetch page 907\n",
      "Now scraping 907\n",
      "Now scraping 908\n",
      "Failed to fetch page 909\n",
      "Now scraping 909\n",
      "Failed to fetch page 910\n",
      "Now scraping 910\n",
      "Failed to fetch page 911\n",
      "Now scraping 911\n",
      "Failed to fetch page 912\n",
      "Now scraping 912\n",
      "Failed to fetch page 913\n",
      "Now scraping 913\n",
      "Failed to fetch page 914\n",
      "Now scraping 914\n",
      "Failed to fetch page 915\n",
      "Now scraping 915\n",
      "Failed to fetch page 916\n",
      "Now scraping 916\n",
      "Failed to fetch page 917\n",
      "Now scraping 917\n",
      "Now scraping 918\n",
      "Failed to fetch page 919\n",
      "Now scraping 919\n",
      "Failed to fetch page 920\n",
      "Now scraping 920\n",
      "Failed to fetch page 921\n",
      "Now scraping 921\n",
      "Now scraping 922\n",
      "Failed to fetch page 923\n",
      "Now scraping 923\n",
      "Failed to fetch page 924\n",
      "Now scraping 924\n",
      "Failed to fetch page 925\n",
      "Now scraping 925\n",
      "Failed to fetch page 926\n",
      "Now scraping 926\n",
      "Failed to fetch page 927\n",
      "Now scraping 927\n",
      "Failed to fetch page 928\n",
      "Now scraping 928\n",
      "Failed to fetch page 929\n",
      "Now scraping 929\n",
      "Now scraping 930\n",
      "Failed to fetch page 931\n",
      "Now scraping 931\n",
      "Failed to fetch page 932\n",
      "Now scraping 932\n",
      "Failed to fetch page 933\n",
      "Now scraping 933\n",
      "Failed to fetch page 934\n",
      "Now scraping 934\n",
      "Failed to fetch page 935\n",
      "Now scraping 935\n",
      "Failed to fetch page 936\n",
      "Now scraping 936\n",
      "Failed to fetch page 937\n",
      "Now scraping 937\n",
      "Failed to fetch page 938\n",
      "Now scraping 938\n",
      "Failed to fetch page 939\n",
      "Now scraping 939\n",
      "Failed to fetch page 940\n",
      "Now scraping 940\n",
      "Now scraping 941\n",
      "Failed to fetch page 942\n",
      "Now scraping 942\n",
      "Failed to fetch page 943\n",
      "Now scraping 943\n",
      "Failed to fetch page 944\n",
      "Now scraping 944\n",
      "Failed to fetch page 945\n",
      "Now scraping 945\n",
      "Now scraping 946\n",
      "Failed to fetch page 947\n",
      "Now scraping 947\n",
      "Failed to fetch page 948\n",
      "Now scraping 948\n",
      "Failed to fetch page 949\n",
      "Now scraping 949\n",
      "Failed to fetch page 950\n",
      "Now scraping 950\n",
      "Failed to fetch page 951\n",
      "Now scraping 951\n",
      "Failed to fetch page 952\n",
      "Now scraping 952\n",
      "Failed to fetch page 953\n",
      "Now scraping 953\n",
      "Failed to fetch page 954\n",
      "Now scraping 954\n",
      "Failed to fetch page 955\n",
      "Now scraping 955\n",
      "Failed to fetch page 956\n",
      "Now scraping 956\n",
      "Failed to fetch page 957\n",
      "Now scraping 957\n",
      "Failed to fetch page 958\n",
      "Now scraping 958\n",
      "Failed to fetch page 959\n",
      "Now scraping 959\n",
      "Failed to fetch page 960\n",
      "Now scraping 960\n",
      "Failed to fetch page 961\n",
      "Now scraping 961\n",
      "Failed to fetch page 962\n",
      "Now scraping 962\n",
      "Failed to fetch page 963\n",
      "Now scraping 963\n",
      "Now scraping 964\n",
      "Failed to fetch page 965\n",
      "Now scraping 965\n",
      "Failed to fetch page 966\n",
      "Now scraping 966\n",
      "Failed to fetch page 967\n",
      "Now scraping 967\n",
      "Failed to fetch page 968\n",
      "Now scraping 968\n",
      "Now scraping 969\n",
      "Failed to fetch page 970\n",
      "Now scraping 970\n",
      "Failed to fetch page 971\n",
      "Now scraping 971\n",
      "Failed to fetch page 972\n",
      "Now scraping 972\n",
      "Failed to fetch page 973\n",
      "Now scraping 973\n",
      "Failed to fetch page 974\n",
      "Now scraping 974\n",
      "Failed to fetch page 975\n",
      "Now scraping 975\n",
      "Now scraping 976\n",
      "Failed to fetch page 977\n",
      "Now scraping 977\n",
      "Failed to fetch page 978\n",
      "Now scraping 978\n",
      "Failed to fetch page 979\n",
      "Now scraping 979\n",
      "Failed to fetch page 980\n",
      "Now scraping 980\n",
      "Failed to fetch page 981\n",
      "Now scraping 981\n",
      "Failed to fetch page 982\n",
      "Now scraping 982\n",
      "Failed to fetch page 983\n",
      "Now scraping 983\n",
      "Failed to fetch page 984\n",
      "Now scraping 984\n",
      "Failed to fetch page 985\n",
      "Now scraping 985\n",
      "Failed to fetch page 986\n",
      "Now scraping 986\n",
      "Failed to fetch page 987\n",
      "Now scraping 987\n",
      "Now scraping 988\n",
      "Failed to fetch page 989\n",
      "Now scraping 989\n",
      "Failed to fetch page 990\n",
      "Now scraping 990\n",
      "Failed to fetch page 991\n",
      "Now scraping 991\n",
      "Failed to fetch page 992\n",
      "Now scraping 992\n",
      "Now scraping 993\n",
      "Failed to fetch page 994\n",
      "Now scraping 994\n",
      "Failed to fetch page 995\n",
      "Now scraping 995\n",
      "Failed to fetch page 996\n",
      "Now scraping 996\n",
      "Failed to fetch page 997\n",
      "Now scraping 997\n",
      "Failed to fetch page 998\n",
      "Now scraping 998\n",
      "Failed to fetch page 999\n",
      "Now scraping 999\n",
      "Failed to fetch page 1000\n",
      "Now scraping 1000\n",
      "Failed to fetch page 1001\n",
      "Now scraping 1001\n",
      "Failed to fetch page 1002\n",
      "Now scraping 1002\n",
      "Failed to fetch page 1003\n",
      "Now scraping 1003\n",
      "Failed to fetch page 1004\n",
      "Now scraping 1004\n",
      "Failed to fetch page 1005\n",
      "Now scraping 1005\n",
      "Failed to fetch page 1006\n",
      "Now scraping 1006\n",
      "Failed to fetch page 1007\n",
      "Now scraping 1007\n",
      "Failed to fetch page 1008\n",
      "Now scraping 1008\n",
      "Failed to fetch page 1009\n",
      "Now scraping 1009\n",
      "Failed to fetch page 1010\n",
      "Now scraping 1010\n",
      "Failed to fetch page 1011\n",
      "Now scraping 1011\n",
      "Failed to fetch page 1012\n",
      "Now scraping 1012\n",
      "Now scraping 1013\n",
      "Failed to fetch page 1014\n",
      "Now scraping 1014\n",
      "Failed to fetch page 1015\n",
      "Now scraping 1015\n",
      "Failed to fetch page 1016\n",
      "Now scraping 1016\n",
      "Failed to fetch page 1017\n",
      "Now scraping 1017\n",
      "Failed to fetch page 1018\n",
      "Now scraping 1018\n",
      "Failed to fetch page 1019\n",
      "Now scraping 1019\n",
      "Failed to fetch page 1020\n",
      "Now scraping 1020\n",
      "Failed to fetch page 1021\n",
      "Now scraping 1021\n",
      "Now scraping 1022\n",
      "Failed to fetch page 1023\n",
      "Now scraping 1023\n",
      "Now scraping 1024\n",
      "Failed to fetch page 1025\n",
      "Now scraping 1025\n",
      "Failed to fetch page 1026\n",
      "Now scraping 1026\n",
      "Now scraping 1027\n",
      "Failed to fetch page 1028\n",
      "Now scraping 1028\n",
      "Failed to fetch page 1029\n",
      "Now scraping 1029\n",
      "Failed to fetch page 1030\n",
      "Now scraping 1030\n",
      "Failed to fetch page 1031\n",
      "Now scraping 1031\n",
      "Failed to fetch page 1032\n",
      "Now scraping 1032\n",
      "Failed to fetch page 1033\n",
      "Now scraping 1033\n",
      "Failed to fetch page 1034\n",
      "Now scraping 1034\n",
      "Failed to fetch page 1035\n",
      "Now scraping 1035\n",
      "Failed to fetch page 1036\n",
      "Now scraping 1036\n",
      "Failed to fetch page 1037\n",
      "Now scraping 1037\n",
      "Failed to fetch page 1038\n",
      "Now scraping 1038\n",
      "Failed to fetch page 1039\n",
      "Now scraping 1039\n",
      "Failed to fetch page 1040\n",
      "Now scraping 1040\n",
      "Failed to fetch page 1041\n",
      "Now scraping 1041\n",
      "Failed to fetch page 1042\n",
      "Now scraping 1042\n",
      "Failed to fetch page 1043\n",
      "Now scraping 1043\n",
      "Now scraping 1044\n",
      "Failed to fetch page 1045\n",
      "Now scraping 1045\n",
      "Now scraping 1046\n",
      "Failed to fetch page 1047\n",
      "Now scraping 1047\n",
      "Failed to fetch page 1048\n",
      "Now scraping 1048\n",
      "Failed to fetch page 1049\n",
      "Now scraping 1049\n",
      "Failed to fetch page 1050\n",
      "Now scraping 1050\n",
      "Failed to fetch page 1051\n",
      "Now scraping 1051\n",
      "Now scraping 1052\n",
      "Failed to fetch page 1053\n",
      "Now scraping 1053\n",
      "Failed to fetch page 1054\n",
      "Now scraping 1054\n",
      "Now scraping 1055\n",
      "Failed to fetch page 1056\n",
      "Now scraping 1056\n",
      "Failed to fetch page 1057\n",
      "Now scraping 1057\n",
      "Failed to fetch page 1058\n",
      "Now scraping 1058\n",
      "Failed to fetch page 1059\n",
      "Now scraping 1059\n",
      "Failed to fetch page 1060\n",
      "Now scraping 1060\n",
      "Failed to fetch page 1061\n",
      "Now scraping 1061\n",
      "Failed to fetch page 1062\n",
      "Now scraping 1062\n",
      "Failed to fetch page 1063\n",
      "Now scraping 1063\n",
      "Now scraping 1064\n",
      "Failed to fetch page 1065\n",
      "Now scraping 1065\n",
      "Failed to fetch page 1066\n",
      "Now scraping 1066\n",
      "Failed to fetch page 1067\n",
      "Now scraping 1067\n",
      "Failed to fetch page 1068\n",
      "Now scraping 1068\n",
      "Failed to fetch page 1069\n",
      "Now scraping 1069\n",
      "Failed to fetch page 1070\n",
      "Now scraping 1070\n",
      "Failed to fetch page 1071\n",
      "Now scraping 1071\n",
      "Failed to fetch page 1072\n",
      "Now scraping 1072\n",
      "Failed to fetch page 1073\n",
      "Now scraping 1073\n",
      "Now scraping 1074\n",
      "Failed to fetch page 1075\n",
      "Now scraping 1075\n",
      "Failed to fetch page 1076\n",
      "Now scraping 1076\n",
      "Failed to fetch page 1077\n",
      "Now scraping 1077\n",
      "Failed to fetch page 1078\n",
      "Now scraping 1078\n",
      "Failed to fetch page 1079\n",
      "Now scraping 1079\n",
      "Failed to fetch page 1080\n",
      "Now scraping 1080\n",
      "Failed to fetch page 1081\n",
      "Now scraping 1081\n",
      "Now scraping 1082\n",
      "Failed to fetch page 1083\n",
      "Now scraping 1083\n",
      "Failed to fetch page 1084\n",
      "Now scraping 1084\n",
      "Now scraping 1085\n",
      "Failed to fetch page 1086\n",
      "Now scraping 1086\n",
      "Failed to fetch page 1087\n",
      "Now scraping 1087\n",
      "Failed to fetch page 1088\n",
      "Now scraping 1088\n",
      "Failed to fetch page 1089\n",
      "Now scraping 1089\n",
      "Failed to fetch page 1090\n",
      "Now scraping 1090\n",
      "Failed to fetch page 1091\n",
      "Now scraping 1091\n",
      "Failed to fetch page 1092\n",
      "Now scraping 1092\n",
      "Now scraping 1093\n",
      "Now scraping 1094\n",
      "Failed to fetch page 1095\n",
      "Now scraping 1095\n",
      "Failed to fetch page 1096\n",
      "Now scraping 1096\n",
      "Failed to fetch page 1097\n",
      "Now scraping 1097\n",
      "Failed to fetch page 1098\n",
      "Now scraping 1098\n",
      "Now scraping 1099\n",
      "Failed to fetch page 1100\n",
      "Now scraping 1100\n",
      "Data saved to news_data2.csv\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Define the base URL and the total number of pages to scrape\n",
    "base_url = \"https://www.theblock.co/latest?start=\"\n",
    "total_pages = 1300\n",
    "pages_per_batch = 250\n",
    "\n",
    "# List of proxies to rotate through\n",
    "proxies = [\n",
    "    {'http': 'http://102.130.125.86:80'},\n",
    "    {'http': 'http://85.198.13.205:80'},\n",
    "    {'http': 'http://185.110.189.166:80'},\n",
    "    {'http': 'http://103.168.254.62:8080'}\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Add more proxies as needed\n",
    "]\n",
    "\n",
    "# Initialize an empty list to store dictionaries\n",
    "data_list = []\n",
    "\n",
    "# Function to scrape data from a single page with a specified proxy\n",
    "def scrape_page(page_number, proxy):\n",
    "    url = f\"{base_url}{page_number}0\"\n",
    "    response = requests.get(url, proxies=proxy)  # Use specified proxy\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        # Find all headline and date elements\n",
    "        headlines = soup.select('div.collection__feed div.headline span')\n",
    "        dates = soup.select('div.collection__feed div.meta div.pubDate')\n",
    "        # Iterate over each headline and date and store in the list\n",
    "        for headline, date in zip(headlines, dates):\n",
    "            headline_text = headline.get_text(strip=True)\n",
    "            date_text = date.get_text(strip=True)\n",
    "            data_list.append({'headline': headline_text, 'date': date_text})\n",
    "    else:\n",
    "        print(f\"Failed to fetch page {page_number}\")\n",
    "\n",
    "\n",
    "# Loop through pages and scrape data\n",
    "for batch_number in range(1,total_pages // pages_per_batch):\n",
    "    start_page = -150+ batch_number * pages_per_batch +1\n",
    "    end_page = start_page + pages_per_batch-1\n",
    "    print(f\"Scraping data from pages {start_page}-{end_page}...\")\n",
    "    # Select proxy for this batch of requests\n",
    "    proxy = proxies[batch_number % len(proxies)]\n",
    "    for page_number in range(start_page, end_page + 1):\n",
    "        try:\n",
    "            scrape_page(page_number, proxy)\n",
    "            print(f\"Now scraping {page_number}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error occurred while scraping page {page_number}: {e}\")\n",
    "            break  # Stop scraping if an error occurs\n",
    "        time.sleep(15)  # Add a delay to be polite to the server\n",
    "\n",
    "# Create DataFrame from the list of dictionaries\n",
    "df = pd.DataFrame(data_list)\n",
    "# Clean and convert the \"date\" column to datetime format\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv(\"news_data2.csv\", index=False)\n",
    "\n",
    "print(\"Data saved to news_data2.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping data from pages 380-629...\n",
      "Scraping data from pages 630-879...\n",
      "Scraping data from pages 880-1129...\n",
      "Scraping data from pages 1130-1379...\n",
      "Scraping data from pages 1380-1629...\n"
     ]
    }
   ],
   "source": [
    "total_pages=100\n",
    "pages_per_batch=250\n",
    "# Loop through pages and scrape data\n",
    "for batch_number in range(total_pages // pages_per_batch):\n",
    "    \n",
    "    start_page1 = batch_number * pages_per_batch + 1 +379\n",
    "    end_page2 = start_page1 + pages_per_batch -1\n",
    "    print(f\"Scraping data from pages {start_page1}-{end_page2}...\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Define the base URL and the total number of pages to scrape\n",
    "base_url = \"https://www.theblock.co/latest?start=\"\n",
    "total_pages = 1300\n",
    "pages_per_batch = 250\n",
    "\n",
    "# List of proxies to rotate through\n",
    "proxies = [\n",
    "    'http://102.130.125.86:80',\n",
    "    'http://85.198.13.205:80',\n",
    "    'http://185.110.189.166:80',\n",
    "    'http://103.168.254.62:8080'\n",
    "    # Add more proxies as needed\n",
    "]\n",
    "\n",
    "# Initialize an empty list to store dictionaries\n",
    "data_list = []\n",
    "\n",
    "# Function to scrape data from a single page with a specified proxy\n",
    "def scrape_page(page_number, proxy):\n",
    "    # Configure Selenium WebDriver with proxy\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(f'--proxy-server={proxy}')\n",
    "    service = Service('path_to_chromedriver')\n",
    "    driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "    \n",
    "    # Load page\n",
    "    url = f\"{base_url}{page_number}0\"\n",
    "    driver.get(url)\n",
    "    time.sleep(2)  # Wait for page to load (adjust as needed)\n",
    "    \n",
    "    # Parse HTML with BeautifulSoup\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    headlines = soup.select('div.collection__feed div.headline span')\n",
    "    dates = soup.select('div.collection__feed div.meta div.pubDate')\n",
    "    \n",
    "    # Iterate over each headline and date and store in the list\n",
    "    for headline, date in zip(headlines, dates):\n",
    "        headline_text = headline.get_text(strip=True)\n",
    "        date_text = date.get_text(strip=True)\n",
    "        data_list.append({'headline': headline_text, 'date': date_text})\n",
    "    \n",
    "    # Close the WebDriver\n",
    "    driver.quit()\n",
    "\n",
    "# Loop through pages and scrape data\n",
    "for batch_number in range(1, total_pages // pages_per_batch):\n",
    "    start_page = -150 + batch_number * pages_per_batch + 1\n",
    "    end_page = start_page + pages_per_batch - 1\n",
    "    print(f\"Scraping data from pages {start_page}-{end_page}...\")\n",
    "    # Select proxy for this batch of requests\n",
    "    proxy = proxies[batch_number % len(proxies)]\n",
    "    for page_number in range(start_page, end_page + 1):\n",
    "        try:\n",
    "            scrape_page(page_number, proxy)\n",
    "            print(f\"Now scraping {page_number}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error occurred while scraping page {page_number}: {e}\")\n",
    "            break  # Stop scraping if an error occurs\n",
    "        time.sleep(1)  # Add a delay to be polite to the server\n",
    "\n",
    "# Create DataFrame from the list of dictionaries\n",
    "df = pd.DataFrame(data_list)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv(\"news_data2.csv\", index=False)\n",
    "\n",
    "print(\"Data saved to news_data2.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

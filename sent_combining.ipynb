{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Headline</th>\n",
       "      <th>Date</th>\n",
       "      <th>pred_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7451</td>\n",
       "      <td>Will Komodo Reach 12,000 Satoshis? [Premium An...</td>\n",
       "      <td>10-Dec-19</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7452</td>\n",
       "      <td>Will Ethereum’s Consolidation End With a Break...</td>\n",
       "      <td>10-Dec-19</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7453</td>\n",
       "      <td>China Orders Government Agencies to Replace Al...</td>\n",
       "      <td>09-Dec-19</td>\n",
       "      <td>-0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7454</td>\n",
       "      <td>Facebook Files Lawsuit Against Hong Kong Firm ...</td>\n",
       "      <td>09-Dec-19</td>\n",
       "      <td>-0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7455</td>\n",
       "      <td>Bitcoin Stability Highlighted by Crashing Oil ...</td>\n",
       "      <td>09-Dec-19</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19301</th>\n",
       "      <td>27052</td>\n",
       "      <td>Gold, Stocks, and Bitcoin: Weekly Overview — J...</td>\n",
       "      <td>01-Jul-21</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19302</th>\n",
       "      <td>27053</td>\n",
       "      <td>Calaxy Raises $7.5M in Funding to Develop Fan App</td>\n",
       "      <td>01-Jul-21</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19303</th>\n",
       "      <td>27054</td>\n",
       "      <td>BTCD Gets Rejected at 48% – Will Alts Rally?</td>\n",
       "      <td>01-Jul-21</td>\n",
       "      <td>-0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19304</th>\n",
       "      <td>27055</td>\n",
       "      <td>WazirX Hires TRM Labs to Help Detect Fraud</td>\n",
       "      <td>01-Jul-21</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19305</th>\n",
       "      <td>27056</td>\n",
       "      <td>BTC On-Chain Analysis: CDD Falls to New Yearly...</td>\n",
       "      <td>01-Jul-21</td>\n",
       "      <td>-0.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19306 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       index                                           Headline       Date  \\\n",
       "0       7451  Will Komodo Reach 12,000 Satoshis? [Premium An...  10-Dec-19   \n",
       "1       7452  Will Ethereum’s Consolidation End With a Break...  10-Dec-19   \n",
       "2       7453  China Orders Government Agencies to Replace Al...  09-Dec-19   \n",
       "3       7454  Facebook Files Lawsuit Against Hong Kong Firm ...  09-Dec-19   \n",
       "4       7455  Bitcoin Stability Highlighted by Crashing Oil ...  09-Dec-19   \n",
       "...      ...                                                ...        ...   \n",
       "19301  27052  Gold, Stocks, and Bitcoin: Weekly Overview — J...  01-Jul-21   \n",
       "19302  27053  Calaxy Raises $7.5M in Funding to Develop Fan App  01-Jul-21   \n",
       "19303  27054       BTCD Gets Rejected at 48% – Will Alts Rally?  01-Jul-21   \n",
       "19304  27055         WazirX Hires TRM Labs to Help Detect Fraud  01-Jul-21   \n",
       "19305  27056  BTC On-Chain Analysis: CDD Falls to New Yearly...  01-Jul-21   \n",
       "\n",
       "       pred_score  \n",
       "0             0.1  \n",
       "1             0.0  \n",
       "2            -0.2  \n",
       "3            -0.3  \n",
       "4             0.4  \n",
       "...           ...  \n",
       "19301         0.0  \n",
       "19302         0.5  \n",
       "19303        -0.3  \n",
       "19304         0.2  \n",
       "19305        -0.2  \n",
       "\n",
       "[19306 rows x 4 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv(r\"D:\\DATA SCIENCE\\Internship\\CrypoDataAnalysis\\SENT_ANAL\\merged_data_till.27050.csv\")\n",
    "df=df.drop(columns=[\"Headline_x\",\"Headline_y\",\"pred_label\"])\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Headline</th>\n",
       "      <th>Date</th>\n",
       "      <th>pred_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2549</th>\n",
       "      <td>10000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>27-Jun-19</td>\n",
       "      <td>-0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2550</th>\n",
       "      <td>10001</td>\n",
       "      <td>Kraken Raises $13.5 Million in Crowdfunding Fr...</td>\n",
       "      <td>27-Jun-19</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2551</th>\n",
       "      <td>10002</td>\n",
       "      <td>Satoshi’s Treasure Releases New Clue with $70,...</td>\n",
       "      <td>26-Jun-19</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2552</th>\n",
       "      <td>10003</td>\n",
       "      <td>US to Update Evidence Used for Auditing to Inc...</td>\n",
       "      <td>26-Jun-19</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2553</th>\n",
       "      <td>10004</td>\n",
       "      <td>Bitcoin Surge Should Not Be a Surprise, Claims...</td>\n",
       "      <td>26-Jun-19</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2605</th>\n",
       "      <td>10056</td>\n",
       "      <td>Line Continues to Deny Its Japanese Cryptocurr...</td>\n",
       "      <td>21-Jun-19</td>\n",
       "      <td>-0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2606</th>\n",
       "      <td>10057</td>\n",
       "      <td>Craig Wright Reportedly Failed to Submit Court...</td>\n",
       "      <td>21-Jun-19</td>\n",
       "      <td>-0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2607</th>\n",
       "      <td>10058</td>\n",
       "      <td>Bank of England on Libra: ‘We’re Keeping an Op...</td>\n",
       "      <td>21-Jun-19</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2608</th>\n",
       "      <td>10059</td>\n",
       "      <td>France Plans to Create a ‘G7 Cryptocurrency Ta...</td>\n",
       "      <td>21-Jun-19</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2609</th>\n",
       "      <td>10060</td>\n",
       "      <td>Spotify CEO: Facebook’s Libra Could Allow List...</td>\n",
       "      <td>21-Jun-19</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>61 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index                                           Headline       Date  \\\n",
       "2549  10000                                                NaN  27-Jun-19   \n",
       "2550  10001  Kraken Raises $13.5 Million in Crowdfunding Fr...  27-Jun-19   \n",
       "2551  10002  Satoshi’s Treasure Releases New Clue with $70,...  26-Jun-19   \n",
       "2552  10003  US to Update Evidence Used for Auditing to Inc...  26-Jun-19   \n",
       "2553  10004  Bitcoin Surge Should Not Be a Surprise, Claims...  26-Jun-19   \n",
       "...     ...                                                ...        ...   \n",
       "2605  10056  Line Continues to Deny Its Japanese Cryptocurr...  21-Jun-19   \n",
       "2606  10057  Craig Wright Reportedly Failed to Submit Court...  21-Jun-19   \n",
       "2607  10058  Bank of England on Libra: ‘We’re Keeping an Op...  21-Jun-19   \n",
       "2608  10059  France Plans to Create a ‘G7 Cryptocurrency Ta...  21-Jun-19   \n",
       "2609  10060  Spotify CEO: Facebook’s Libra Could Allow List...  21-Jun-19   \n",
       "\n",
       "      pred_score  \n",
       "2549        -0.8  \n",
       "2550         0.7  \n",
       "2551         0.1  \n",
       "2552         0.3  \n",
       "2553         0.2  \n",
       "...          ...  \n",
       "2605        -0.1  \n",
       "2606        -0.3  \n",
       "2607         0.1  \n",
       "2608         0.2  \n",
       "2609         0.3  \n",
       "\n",
       "[61 rows x 4 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_subset = df[(df['index'] >=10000) & (df['index'] <=10060)]\n",
    "\n",
    "# Display the subset of the DataFrame\n",
    "df_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       index Headline       Date  pred_score\n",
      "5150   12651      NaN  02-Oct-23         NaN\n",
      "5151   12652      NaN  02-Oct-23         NaN\n",
      "5152   12653      NaN  02-Oct-23         NaN\n",
      "5153   12654      NaN  02-Oct-23         NaN\n",
      "5154   12655      NaN  02-Oct-23         NaN\n",
      "...      ...      ...        ...         ...\n",
      "18395  26096      NaN  30-Aug-21         NaN\n",
      "18396  26097      NaN  30-Aug-21         NaN\n",
      "18397  26098      NaN  30-Aug-21         NaN\n",
      "18398  26099      NaN  29-Aug-21         NaN\n",
      "18399  26100      NaN  29-Aug-21         NaN\n",
      "\n",
      "[150 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# Filter rows that contain any null values\n",
    "df_with_nulls = df[df['pred_score'].isnull()]\n",
    "\n",
    "# Display the rows with null values\n",
    "print(df_with_nulls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       index Headline       Date  pred_score\n",
      "100     7551      NaN  04-Dec-19         0.1\n",
      "101     7552      NaN  04-Dec-19         0.1\n",
      "102     7553      NaN  04-Dec-19         0.1\n",
      "103     7554      NaN  04-Dec-19         0.3\n",
      "104     7555      NaN  03-Dec-19         0.0\n",
      "...      ...      ...        ...         ...\n",
      "19095  26846      NaN  14-Jul-21        -0.5\n",
      "19096  26847      NaN  14-Jul-21         0.0\n",
      "19097  26848      NaN  14-Jul-21         0.3\n",
      "19098  26849      NaN  14-Jul-21        -0.6\n",
      "19099  26850      NaN  14-Jul-21         0.2\n",
      "\n",
      "[1800 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# Filter rows that contain any null values\n",
    "df_with_nulls = df[df.isnull().any(axis=1)]\n",
    "\n",
    "# Display the rows with null values\n",
    "print(df_with_nulls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7551,\n",
       " 7552,\n",
       " 7553,\n",
       " 7554,\n",
       " 7555,\n",
       " 7556,\n",
       " 7557,\n",
       " 7558,\n",
       " 7559,\n",
       " 7560,\n",
       " 7561,\n",
       " 7562,\n",
       " 7563,\n",
       " 7564,\n",
       " 7565,\n",
       " 7566,\n",
       " 7567,\n",
       " 7568,\n",
       " 7569,\n",
       " 7570,\n",
       " 7571,\n",
       " 7572,\n",
       " 7573,\n",
       " 7574,\n",
       " 7575,\n",
       " 7576,\n",
       " 7577,\n",
       " 7578,\n",
       " 7579,\n",
       " 7580,\n",
       " 7581,\n",
       " 7582,\n",
       " 7583,\n",
       " 7584,\n",
       " 7585,\n",
       " 7586,\n",
       " 7587,\n",
       " 7588,\n",
       " 7589,\n",
       " 7590,\n",
       " 7591,\n",
       " 7592,\n",
       " 7593,\n",
       " 7594,\n",
       " 7595,\n",
       " 7596,\n",
       " 7597,\n",
       " 7598,\n",
       " 7599,\n",
       " 7600,\n",
       " 9351,\n",
       " 9352,\n",
       " 9353,\n",
       " 9354,\n",
       " 9355,\n",
       " 9356,\n",
       " 9357,\n",
       " 9358,\n",
       " 9359,\n",
       " 9360,\n",
       " 9361,\n",
       " 9362,\n",
       " 9363,\n",
       " 9364,\n",
       " 9365,\n",
       " 9366,\n",
       " 9367,\n",
       " 9368,\n",
       " 9369,\n",
       " 9370,\n",
       " 9371,\n",
       " 9372,\n",
       " 9373,\n",
       " 9374,\n",
       " 9375,\n",
       " 9376,\n",
       " 9377,\n",
       " 9378,\n",
       " 9379,\n",
       " 9380,\n",
       " 9381,\n",
       " 9382,\n",
       " 9383,\n",
       " 9384,\n",
       " 9385,\n",
       " 9386,\n",
       " 9387,\n",
       " 9388,\n",
       " 9389,\n",
       " 9390,\n",
       " 9391,\n",
       " 9392,\n",
       " 9393,\n",
       " 9394,\n",
       " 9395,\n",
       " 9396,\n",
       " 9397,\n",
       " 9398,\n",
       " 9399,\n",
       " 9400,\n",
       " 9951,\n",
       " 9952,\n",
       " 9953,\n",
       " 9954,\n",
       " 9955,\n",
       " 9956,\n",
       " 9957,\n",
       " 9958,\n",
       " 9959,\n",
       " 9960,\n",
       " 9961,\n",
       " 9962,\n",
       " 9963,\n",
       " 9964,\n",
       " 9965,\n",
       " 9966,\n",
       " 9967,\n",
       " 9968,\n",
       " 9969,\n",
       " 9970,\n",
       " 9971,\n",
       " 9972,\n",
       " 9973,\n",
       " 9974,\n",
       " 9975,\n",
       " 9976,\n",
       " 9977,\n",
       " 9978,\n",
       " 9979,\n",
       " 9980,\n",
       " 9981,\n",
       " 9982,\n",
       " 9983,\n",
       " 9984,\n",
       " 9985,\n",
       " 9986,\n",
       " 9987,\n",
       " 9988,\n",
       " 9989,\n",
       " 9990,\n",
       " 9991,\n",
       " 9992,\n",
       " 9993,\n",
       " 9994,\n",
       " 9995,\n",
       " 9996,\n",
       " 9997,\n",
       " 9998,\n",
       " 9999,\n",
       " 10000,\n",
       " 10251,\n",
       " 10252,\n",
       " 10253,\n",
       " 10254,\n",
       " 10255,\n",
       " 10256,\n",
       " 10257,\n",
       " 10258,\n",
       " 10259,\n",
       " 10260,\n",
       " 10261,\n",
       " 10262,\n",
       " 10263,\n",
       " 10264,\n",
       " 10265,\n",
       " 10266,\n",
       " 10267,\n",
       " 10268,\n",
       " 10269,\n",
       " 10270,\n",
       " 10271,\n",
       " 10272,\n",
       " 10273,\n",
       " 10274,\n",
       " 10275,\n",
       " 10276,\n",
       " 10277,\n",
       " 10278,\n",
       " 10279,\n",
       " 10280,\n",
       " 10281,\n",
       " 10282,\n",
       " 10283,\n",
       " 10284,\n",
       " 10285,\n",
       " 10286,\n",
       " 10287,\n",
       " 10288,\n",
       " 10289,\n",
       " 10290,\n",
       " 10291,\n",
       " 10292,\n",
       " 10293,\n",
       " 10294,\n",
       " 10295,\n",
       " 10296,\n",
       " 10297,\n",
       " 10298,\n",
       " 10299,\n",
       " 10300,\n",
       " 10601,\n",
       " 10602,\n",
       " 10603,\n",
       " 10604,\n",
       " 10605,\n",
       " 10606,\n",
       " 10607,\n",
       " 10608,\n",
       " 10609,\n",
       " 10610,\n",
       " 10611,\n",
       " 10612,\n",
       " 10613,\n",
       " 10614,\n",
       " 10615,\n",
       " 10616,\n",
       " 10617,\n",
       " 10618,\n",
       " 10619,\n",
       " 10620,\n",
       " 10621,\n",
       " 10622,\n",
       " 10623,\n",
       " 10624,\n",
       " 10625,\n",
       " 10626,\n",
       " 10627,\n",
       " 10628,\n",
       " 10629,\n",
       " 10630,\n",
       " 10631,\n",
       " 10632,\n",
       " 10633,\n",
       " 10634,\n",
       " 10635,\n",
       " 10636,\n",
       " 10637,\n",
       " 10638,\n",
       " 10639,\n",
       " 10640,\n",
       " 10641,\n",
       " 10642,\n",
       " 10643,\n",
       " 10644,\n",
       " 10645,\n",
       " 10646,\n",
       " 10647,\n",
       " 10648,\n",
       " 10649,\n",
       " 10650,\n",
       " 11151,\n",
       " 11152,\n",
       " 11153,\n",
       " 11154,\n",
       " 11155,\n",
       " 11156,\n",
       " 11157,\n",
       " 11158,\n",
       " 11159,\n",
       " 11160,\n",
       " 11161,\n",
       " 11162,\n",
       " 11163,\n",
       " 11164,\n",
       " 11165,\n",
       " 11166,\n",
       " 11167,\n",
       " 11168,\n",
       " 11169,\n",
       " 11170,\n",
       " 11171,\n",
       " 11172,\n",
       " 11173,\n",
       " 11174,\n",
       " 11175,\n",
       " 11176,\n",
       " 11177,\n",
       " 11178,\n",
       " 11179,\n",
       " 11180,\n",
       " 11181,\n",
       " 11182,\n",
       " 11183,\n",
       " 11184,\n",
       " 11185,\n",
       " 11186,\n",
       " 11187,\n",
       " 11188,\n",
       " 11189,\n",
       " 11190,\n",
       " 11191,\n",
       " 11192,\n",
       " 11193,\n",
       " 11194,\n",
       " 11195,\n",
       " 11196,\n",
       " 11197,\n",
       " 11198,\n",
       " 11199,\n",
       " 11200,\n",
       " 12001,\n",
       " 12002,\n",
       " 12003,\n",
       " 12004,\n",
       " 12005,\n",
       " 12006,\n",
       " 12007,\n",
       " 12008,\n",
       " 12009,\n",
       " 12010,\n",
       " 12011,\n",
       " 12012,\n",
       " 12013,\n",
       " 12014,\n",
       " 12015,\n",
       " 12016,\n",
       " 12017,\n",
       " 12018,\n",
       " 12019,\n",
       " 12020,\n",
       " 12021,\n",
       " 12022,\n",
       " 12023,\n",
       " 12024,\n",
       " 12025,\n",
       " 12026,\n",
       " 12027,\n",
       " 12028,\n",
       " 12029,\n",
       " 12030,\n",
       " 12031,\n",
       " 12032,\n",
       " 12033,\n",
       " 12034,\n",
       " 12035,\n",
       " 12036,\n",
       " 12037,\n",
       " 12038,\n",
       " 12039,\n",
       " 12040,\n",
       " 12041,\n",
       " 12042,\n",
       " 12043,\n",
       " 12044,\n",
       " 12045,\n",
       " 12046,\n",
       " 12047,\n",
       " 12048,\n",
       " 12049,\n",
       " 12050,\n",
       " 12151,\n",
       " 12152,\n",
       " 12153,\n",
       " 12154,\n",
       " 12155,\n",
       " 12156,\n",
       " 12157,\n",
       " 12158,\n",
       " 12159,\n",
       " 12160,\n",
       " 12161,\n",
       " 12162,\n",
       " 12163,\n",
       " 12164,\n",
       " 12165,\n",
       " 12166,\n",
       " 12167,\n",
       " 12168,\n",
       " 12169,\n",
       " 12170,\n",
       " 12171,\n",
       " 12172,\n",
       " 12173,\n",
       " 12174,\n",
       " 12175,\n",
       " 12176,\n",
       " 12177,\n",
       " 12178,\n",
       " 12179,\n",
       " 12180,\n",
       " 12181,\n",
       " 12182,\n",
       " 12183,\n",
       " 12184,\n",
       " 12185,\n",
       " 12186,\n",
       " 12187,\n",
       " 12188,\n",
       " 12189,\n",
       " 12190,\n",
       " 12191,\n",
       " 12192,\n",
       " 12193,\n",
       " 12194,\n",
       " 12195,\n",
       " 12196,\n",
       " 12197,\n",
       " 12198,\n",
       " 12199,\n",
       " 12200,\n",
       " 12451,\n",
       " 12452,\n",
       " 12453,\n",
       " 12454,\n",
       " 12455,\n",
       " 12456,\n",
       " 12457,\n",
       " 12458,\n",
       " 12459,\n",
       " 12460,\n",
       " 12461,\n",
       " 12462,\n",
       " 12463,\n",
       " 12464,\n",
       " 12465,\n",
       " 12466,\n",
       " 12467,\n",
       " 12468,\n",
       " 12469,\n",
       " 12470,\n",
       " 12471,\n",
       " 12472,\n",
       " 12473,\n",
       " 12474,\n",
       " 12475,\n",
       " 12476,\n",
       " 12477,\n",
       " 12478,\n",
       " 12479,\n",
       " 12480,\n",
       " 12481,\n",
       " 12482,\n",
       " 12483,\n",
       " 12484,\n",
       " 12485,\n",
       " 12486,\n",
       " 12487,\n",
       " 12488,\n",
       " 12489,\n",
       " 12490,\n",
       " 12491,\n",
       " 12492,\n",
       " 12493,\n",
       " 12494,\n",
       " 12495,\n",
       " 12496,\n",
       " 12497,\n",
       " 12498,\n",
       " 12499,\n",
       " 12500,\n",
       " 12601,\n",
       " 12602,\n",
       " 12603,\n",
       " 12604,\n",
       " 12605,\n",
       " 12606,\n",
       " 12607,\n",
       " 12608,\n",
       " 12609,\n",
       " 12610,\n",
       " 12611,\n",
       " 12612,\n",
       " 12613,\n",
       " 12614,\n",
       " 12615,\n",
       " 12616,\n",
       " 12617,\n",
       " 12618,\n",
       " 12619,\n",
       " 12620,\n",
       " 12621,\n",
       " 12622,\n",
       " 12623,\n",
       " 12624,\n",
       " 12625,\n",
       " 12626,\n",
       " 12627,\n",
       " 12628,\n",
       " 12629,\n",
       " 12630,\n",
       " 12631,\n",
       " 12632,\n",
       " 12633,\n",
       " 12634,\n",
       " 12635,\n",
       " 12636,\n",
       " 12637,\n",
       " 12638,\n",
       " 12639,\n",
       " 12640,\n",
       " 12641,\n",
       " 12642,\n",
       " 12643,\n",
       " 12644,\n",
       " 12645,\n",
       " 12646,\n",
       " 12647,\n",
       " 12648,\n",
       " 12649,\n",
       " 12650,\n",
       " 12651,\n",
       " 12652,\n",
       " 12653,\n",
       " 12654,\n",
       " 12655,\n",
       " 12656,\n",
       " 12657,\n",
       " 12658,\n",
       " 12659,\n",
       " 12660,\n",
       " 12661,\n",
       " 12662,\n",
       " 12663,\n",
       " 12664,\n",
       " 12665,\n",
       " 12666,\n",
       " 12667,\n",
       " 12668,\n",
       " 12669,\n",
       " 12670,\n",
       " 12671,\n",
       " 12672,\n",
       " 12673,\n",
       " 12674,\n",
       " 12675,\n",
       " 12676,\n",
       " 12677,\n",
       " 12678,\n",
       " 12679,\n",
       " 12680,\n",
       " 12681,\n",
       " 12682,\n",
       " 12683,\n",
       " 12684,\n",
       " 12685,\n",
       " 12686,\n",
       " 12687,\n",
       " 12688,\n",
       " 12689,\n",
       " 12690,\n",
       " 12691,\n",
       " 12692,\n",
       " 12693,\n",
       " 12694,\n",
       " 12695,\n",
       " 12696,\n",
       " 12697,\n",
       " 12698,\n",
       " 12699,\n",
       " 12700,\n",
       " 12801,\n",
       " 12802,\n",
       " 12803,\n",
       " 12804,\n",
       " 12805,\n",
       " 12806,\n",
       " 12807,\n",
       " 12808,\n",
       " 12809,\n",
       " 12810,\n",
       " 12811,\n",
       " 12812,\n",
       " 12813,\n",
       " 12814,\n",
       " 12815,\n",
       " 12816,\n",
       " 12817,\n",
       " 12818,\n",
       " 12819,\n",
       " 12820,\n",
       " 12821,\n",
       " 12822,\n",
       " 12823,\n",
       " 12824,\n",
       " 12825,\n",
       " 12826,\n",
       " 12827,\n",
       " 12828,\n",
       " 12829,\n",
       " 12830,\n",
       " 12831,\n",
       " 12832,\n",
       " 12833,\n",
       " 12834,\n",
       " 12835,\n",
       " 12836,\n",
       " 12837,\n",
       " 12838,\n",
       " 12839,\n",
       " 12840,\n",
       " 12841,\n",
       " 12842,\n",
       " 12843,\n",
       " 12844,\n",
       " 12845,\n",
       " 12846,\n",
       " 12847,\n",
       " 12848,\n",
       " 12849,\n",
       " 12850,\n",
       " 13151,\n",
       " 13152,\n",
       " 13153,\n",
       " 13154,\n",
       " 13155,\n",
       " 13156,\n",
       " 13157,\n",
       " 13158,\n",
       " 13159,\n",
       " 13160,\n",
       " 13161,\n",
       " 13162,\n",
       " 13163,\n",
       " 13164,\n",
       " 13165,\n",
       " 13166,\n",
       " 13167,\n",
       " 13168,\n",
       " 13169,\n",
       " 13170,\n",
       " 13171,\n",
       " 13172,\n",
       " 13173,\n",
       " 13174,\n",
       " 13175,\n",
       " 13176,\n",
       " 13177,\n",
       " 13178,\n",
       " 13179,\n",
       " 13180,\n",
       " 13181,\n",
       " 13182,\n",
       " 13183,\n",
       " 13184,\n",
       " 13185,\n",
       " 13186,\n",
       " 13187,\n",
       " 13188,\n",
       " 13189,\n",
       " 13190,\n",
       " 13191,\n",
       " 13192,\n",
       " 13193,\n",
       " 13194,\n",
       " 13195,\n",
       " 13196,\n",
       " 13197,\n",
       " 13198,\n",
       " 13199,\n",
       " 13200,\n",
       " 13251,\n",
       " 13252,\n",
       " 13253,\n",
       " 13254,\n",
       " 13255,\n",
       " 13256,\n",
       " 13257,\n",
       " 13258,\n",
       " 13259,\n",
       " 13260,\n",
       " 13261,\n",
       " 13262,\n",
       " 13263,\n",
       " 13264,\n",
       " 13265,\n",
       " 13266,\n",
       " 13267,\n",
       " 13268,\n",
       " 13269,\n",
       " 13270,\n",
       " 13271,\n",
       " 13272,\n",
       " 13273,\n",
       " 13274,\n",
       " 13275,\n",
       " 13276,\n",
       " 13277,\n",
       " 13278,\n",
       " 13279,\n",
       " 13280,\n",
       " 13281,\n",
       " 13282,\n",
       " 13283,\n",
       " 13284,\n",
       " 13285,\n",
       " 13286,\n",
       " 13287,\n",
       " 13288,\n",
       " 13289,\n",
       " 13290,\n",
       " 13291,\n",
       " 13292,\n",
       " 13293,\n",
       " 13294,\n",
       " 13295,\n",
       " 13296,\n",
       " 13297,\n",
       " 13298,\n",
       " 13299,\n",
       " 13300,\n",
       " 13501,\n",
       " 13502,\n",
       " 13503,\n",
       " 13504,\n",
       " 13505,\n",
       " 13506,\n",
       " 13507,\n",
       " 13508,\n",
       " 13509,\n",
       " 13510,\n",
       " 13511,\n",
       " 13512,\n",
       " 13513,\n",
       " 13514,\n",
       " 13515,\n",
       " 13516,\n",
       " 13517,\n",
       " 13518,\n",
       " 13519,\n",
       " 13520,\n",
       " 13521,\n",
       " 13522,\n",
       " 13523,\n",
       " 13524,\n",
       " 13525,\n",
       " 13526,\n",
       " 13527,\n",
       " 13528,\n",
       " 13529,\n",
       " 13530,\n",
       " 13531,\n",
       " 13532,\n",
       " 13533,\n",
       " 13534,\n",
       " 13535,\n",
       " 13536,\n",
       " 13537,\n",
       " 13538,\n",
       " 13539,\n",
       " 13540,\n",
       " 13541,\n",
       " 13542,\n",
       " 13543,\n",
       " 13544,\n",
       " 13545,\n",
       " 13546,\n",
       " 13547,\n",
       " 13548,\n",
       " 13549,\n",
       " 13550,\n",
       " 13651,\n",
       " 13652,\n",
       " 13653,\n",
       " 13654,\n",
       " 13655,\n",
       " 13656,\n",
       " 13657,\n",
       " 13658,\n",
       " 13659,\n",
       " 13660,\n",
       " 13661,\n",
       " 13662,\n",
       " 13663,\n",
       " 13664,\n",
       " 13665,\n",
       " 13666,\n",
       " 13667,\n",
       " 13668,\n",
       " 13669,\n",
       " 13670,\n",
       " 13671,\n",
       " 13672,\n",
       " 13673,\n",
       " 13674,\n",
       " 13675,\n",
       " 13676,\n",
       " 13677,\n",
       " 13678,\n",
       " 13679,\n",
       " 13680,\n",
       " 13681,\n",
       " 13682,\n",
       " 13683,\n",
       " 13684,\n",
       " 13685,\n",
       " 13686,\n",
       " 13687,\n",
       " 13688,\n",
       " 13689,\n",
       " 13690,\n",
       " 13691,\n",
       " 13692,\n",
       " 13693,\n",
       " 13694,\n",
       " 13695,\n",
       " 13696,\n",
       " 13697,\n",
       " 13698,\n",
       " 13699,\n",
       " 13700,\n",
       " 13701,\n",
       " 13702,\n",
       " 13703,\n",
       " 13704,\n",
       " 13705,\n",
       " 13706,\n",
       " 13707,\n",
       " 13708,\n",
       " 13709,\n",
       " 13710,\n",
       " 13711,\n",
       " 13712,\n",
       " 13713,\n",
       " 13714,\n",
       " 13715,\n",
       " 13716,\n",
       " 13717,\n",
       " 13718,\n",
       " 13719,\n",
       " 13720,\n",
       " 13721,\n",
       " 13722,\n",
       " 13723,\n",
       " 13724,\n",
       " 13725,\n",
       " 13726,\n",
       " 13727,\n",
       " 13728,\n",
       " 13729,\n",
       " 13730,\n",
       " 13731,\n",
       " 13732,\n",
       " 13733,\n",
       " 13734,\n",
       " 13735,\n",
       " 13736,\n",
       " 13737,\n",
       " 13738,\n",
       " 13739,\n",
       " 13740,\n",
       " 13741,\n",
       " 13742,\n",
       " 13743,\n",
       " 13744,\n",
       " 13745,\n",
       " 13746,\n",
       " 13747,\n",
       " 13748,\n",
       " 13749,\n",
       " 13750,\n",
       " 14601,\n",
       " 14602,\n",
       " 14603,\n",
       " 14604,\n",
       " 14605,\n",
       " 14606,\n",
       " 14607,\n",
       " 14608,\n",
       " 14609,\n",
       " 14610,\n",
       " 14611,\n",
       " 14612,\n",
       " 14613,\n",
       " 14614,\n",
       " 14615,\n",
       " 14616,\n",
       " 14617,\n",
       " 14618,\n",
       " 14619,\n",
       " 14620,\n",
       " 14621,\n",
       " 14622,\n",
       " 14623,\n",
       " 14624,\n",
       " 14625,\n",
       " 14626,\n",
       " 14627,\n",
       " 14628,\n",
       " 14629,\n",
       " 14630,\n",
       " 14631,\n",
       " 14632,\n",
       " 14633,\n",
       " 14634,\n",
       " 14635,\n",
       " 14636,\n",
       " 14637,\n",
       " 14638,\n",
       " 14639,\n",
       " 14640,\n",
       " 14641,\n",
       " 14642,\n",
       " 14643,\n",
       " 14644,\n",
       " 14645,\n",
       " 14646,\n",
       " 14647,\n",
       " 14648,\n",
       " 14649,\n",
       " 14650,\n",
       " 15651,\n",
       " 15652,\n",
       " 15653,\n",
       " 15654,\n",
       " 15655,\n",
       " 15656,\n",
       " 15657,\n",
       " 15658,\n",
       " 15659,\n",
       " 15660,\n",
       " 15661,\n",
       " 15662,\n",
       " 15663,\n",
       " 15664,\n",
       " 15665,\n",
       " 15666,\n",
       " 15667,\n",
       " 15668,\n",
       " 15669,\n",
       " 15670,\n",
       " 15671,\n",
       " 15672,\n",
       " 15673,\n",
       " 15674,\n",
       " 15675,\n",
       " 15676,\n",
       " 15677,\n",
       " 15678,\n",
       " 15679,\n",
       " 15680,\n",
       " 15681,\n",
       " 15682,\n",
       " 15683,\n",
       " 15684,\n",
       " 15685,\n",
       " 15686,\n",
       " 15687,\n",
       " 15688,\n",
       " 15689,\n",
       " 15690,\n",
       " 15691,\n",
       " 15692,\n",
       " 15693,\n",
       " 15694,\n",
       " 15695,\n",
       " 15696,\n",
       " 15697,\n",
       " 15698,\n",
       " 15699,\n",
       " 15700,\n",
       " 16051,\n",
       " 16052,\n",
       " 16053,\n",
       " 16054,\n",
       " 16055,\n",
       " 16056,\n",
       " 16057,\n",
       " 16058,\n",
       " 16059,\n",
       " 16060,\n",
       " 16061,\n",
       " 16062,\n",
       " 16063,\n",
       " 16064,\n",
       " 16065,\n",
       " 16066,\n",
       " 16067,\n",
       " 16068,\n",
       " 16069,\n",
       " 16070,\n",
       " 16071,\n",
       " 16072,\n",
       " 16073,\n",
       " 16074,\n",
       " 16075,\n",
       " 16076,\n",
       " 16077,\n",
       " 16078,\n",
       " 16079,\n",
       " 16080,\n",
       " 16081,\n",
       " 16082,\n",
       " 16083,\n",
       " 16084,\n",
       " 16085,\n",
       " 16086,\n",
       " 16087,\n",
       " 16088,\n",
       " 16089,\n",
       " 16090,\n",
       " 16091,\n",
       " 16092,\n",
       " 16093,\n",
       " 16094,\n",
       " 16095,\n",
       " 16096,\n",
       " 16097,\n",
       " 16098,\n",
       " 16099,\n",
       " 16100,\n",
       " ...]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list=df_with_nulls['index'].tolist()\n",
    "list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "list1=[7550,9350,9950,10250,10600,11150,12000,12150,12450,12600,12650,12800,13150,13250,13500,13650,13700,14600,15650,16050]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['index'].is_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.insert(0, 'index1', range(1, len(df) + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index1</th>\n",
       "      <th>index</th>\n",
       "      <th>Headline</th>\n",
       "      <th>Date</th>\n",
       "      <th>pred_score</th>\n",
       "      <th>Headline_x</th>\n",
       "      <th>Headline_y</th>\n",
       "      <th>pred_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>7451</td>\n",
       "      <td>Will Komodo Reach 12,000 Satoshis? [Premium An...</td>\n",
       "      <td>10-Dec-19</td>\n",
       "      <td>0.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>7452</td>\n",
       "      <td>Will Ethereum’s Consolidation End With a Break...</td>\n",
       "      <td>10-Dec-19</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>7453</td>\n",
       "      <td>China Orders Government Agencies to Replace Al...</td>\n",
       "      <td>09-Dec-19</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>7454</td>\n",
       "      <td>Facebook Files Lawsuit Against Hong Kong Firm ...</td>\n",
       "      <td>09-Dec-19</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>7455</td>\n",
       "      <td>Bitcoin Stability Highlighted by Crashing Oil ...</td>\n",
       "      <td>09-Dec-19</td>\n",
       "      <td>0.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19301</th>\n",
       "      <td>19302</td>\n",
       "      <td>27052</td>\n",
       "      <td>Gold, Stocks, and Bitcoin: Weekly Overview — J...</td>\n",
       "      <td>01-Jul-21</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19302</th>\n",
       "      <td>19303</td>\n",
       "      <td>27053</td>\n",
       "      <td>Calaxy Raises $7.5M in Funding to Develop Fan App</td>\n",
       "      <td>01-Jul-21</td>\n",
       "      <td>0.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19303</th>\n",
       "      <td>19304</td>\n",
       "      <td>27054</td>\n",
       "      <td>BTCD Gets Rejected at 48% – Will Alts Rally?</td>\n",
       "      <td>01-Jul-21</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19304</th>\n",
       "      <td>19305</td>\n",
       "      <td>27055</td>\n",
       "      <td>WazirX Hires TRM Labs to Help Detect Fraud</td>\n",
       "      <td>01-Jul-21</td>\n",
       "      <td>0.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19305</th>\n",
       "      <td>19306</td>\n",
       "      <td>27056</td>\n",
       "      <td>BTC On-Chain Analysis: CDD Falls to New Yearly...</td>\n",
       "      <td>01-Jul-21</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19306 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       index1  index                                           Headline  \\\n",
       "0           1   7451  Will Komodo Reach 12,000 Satoshis? [Premium An...   \n",
       "1           2   7452  Will Ethereum’s Consolidation End With a Break...   \n",
       "2           3   7453  China Orders Government Agencies to Replace Al...   \n",
       "3           4   7454  Facebook Files Lawsuit Against Hong Kong Firm ...   \n",
       "4           5   7455  Bitcoin Stability Highlighted by Crashing Oil ...   \n",
       "...       ...    ...                                                ...   \n",
       "19301   19302  27052  Gold, Stocks, and Bitcoin: Weekly Overview — J...   \n",
       "19302   19303  27053  Calaxy Raises $7.5M in Funding to Develop Fan App   \n",
       "19303   19304  27054       BTCD Gets Rejected at 48% – Will Alts Rally?   \n",
       "19304   19305  27055         WazirX Hires TRM Labs to Help Detect Fraud   \n",
       "19305   19306  27056  BTC On-Chain Analysis: CDD Falls to New Yearly...   \n",
       "\n",
       "            Date  pred_score Headline_x Headline_y  pred_label  \n",
       "0      10-Dec-19         0.1        NaN        NaN         NaN  \n",
       "1      10-Dec-19         0.0        NaN        NaN         NaN  \n",
       "2      09-Dec-19        -0.2        NaN        NaN         NaN  \n",
       "3      09-Dec-19        -0.3        NaN        NaN         NaN  \n",
       "4      09-Dec-19         0.4        NaN        NaN         NaN  \n",
       "...          ...         ...        ...        ...         ...  \n",
       "19301  01-Jul-21         0.0        NaN        NaN         NaN  \n",
       "19302  01-Jul-21         0.5        NaN        NaN         NaN  \n",
       "19303  01-Jul-21        -0.3        NaN        NaN         NaN  \n",
       "19304  01-Jul-21         0.2        NaN        NaN         NaN  \n",
       "19305  01-Jul-21        -0.2        NaN        NaN         NaN  \n",
       "\n",
       "[19306 rows x 8 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inconsistencies found:\n",
      "From index 11450 to index 11501, difference: 51\n",
      "From index 15550 to index 15601, difference: 51\n",
      "From index 19700 to index 19751, difference: 51\n",
      "From index 21850 to index 21901, difference: 51\n",
      "From index 24900 to index 24951, difference: 51\n",
      "From index 26200 to index 26251, difference: 51\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming df is your DataFrame\n",
    "# Replace 'index' with the actual name of your index column if it's different\n",
    "index_column_name = 'index'\n",
    "\n",
    "# Iterate through the index values and check for inconsistencies\n",
    "inconsistencies = []\n",
    "prev_index = None\n",
    "for index_val in df[index_column_name]:\n",
    "    if prev_index is not None:\n",
    "        diff = index_val - prev_index\n",
    "        if diff != 1:\n",
    "            inconsistencies.append((prev_index, index_val, diff))\n",
    "    prev_index = index_val\n",
    "\n",
    "# Print or handle the inconsistencies found\n",
    "if len(inconsistencies) > 0:\n",
    "    print(\"Inconsistencies found:\")\n",
    "    for inconsistency in inconsistencies:\n",
    "        print(f\"From index {inconsistency[0]} to index {inconsistency[1]}, difference: {inconsistency[2]}\")\n",
    "else:\n",
    "    print(\"No inconsistencies found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inconsistencies found:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index1</th>\n",
       "      <th>index</th>\n",
       "      <th>Headline</th>\n",
       "      <th>Date</th>\n",
       "      <th>pred_score</th>\n",
       "      <th>Headline_x</th>\n",
       "      <th>Headline_y</th>\n",
       "      <th>pred_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3999</th>\n",
       "      <td>4000</td>\n",
       "      <td>11450</td>\n",
       "      <td>Little Known LBRY Credits Is Making Waves (Pri...</td>\n",
       "      <td>05-Mar-19</td>\n",
       "      <td>0.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4000</th>\n",
       "      <td>4001</td>\n",
       "      <td>11501</td>\n",
       "      <td>Dogecoin Daily Price Analysis: Can DOGE Has Ga...</td>\n",
       "      <td>26-Feb-19</td>\n",
       "      <td>0.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      index1  index                                           Headline  \\\n",
       "3999    4000  11450  Little Known LBRY Credits Is Making Waves (Pri...   \n",
       "4000    4001  11501  Dogecoin Daily Price Analysis: Can DOGE Has Ga...   \n",
       "\n",
       "           Date  pred_score Headline_x Headline_y  pred_label  \n",
       "3999  05-Mar-19         0.5        NaN        NaN         NaN  \n",
       "4000  26-Feb-19         0.1        NaN        NaN         NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index1</th>\n",
       "      <th>index</th>\n",
       "      <th>Headline</th>\n",
       "      <th>Date</th>\n",
       "      <th>pred_score</th>\n",
       "      <th>Headline_x</th>\n",
       "      <th>Headline_y</th>\n",
       "      <th>pred_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8049</th>\n",
       "      <td>8050</td>\n",
       "      <td>15550</td>\n",
       "      <td>How Bitcoin Could Be America’s Financial Lifel...</td>\n",
       "      <td>13-Mar-24</td>\n",
       "      <td>0.7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8050</th>\n",
       "      <td>8051</td>\n",
       "      <td>15601</td>\n",
       "      <td>Tether Aims to Teach Georgians About Financial...</td>\n",
       "      <td>24-May-23</td>\n",
       "      <td>0.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      index1  index                                           Headline  \\\n",
       "8049    8050  15550  How Bitcoin Could Be America’s Financial Lifel...   \n",
       "8050    8051  15601  Tether Aims to Teach Georgians About Financial...   \n",
       "\n",
       "           Date  pred_score Headline_x Headline_y  pred_label  \n",
       "8049  13-Mar-24         0.7        NaN        NaN         NaN  \n",
       "8050  24-May-23         0.1        NaN        NaN         NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index1</th>\n",
       "      <th>index</th>\n",
       "      <th>Headline</th>\n",
       "      <th>Date</th>\n",
       "      <th>pred_score</th>\n",
       "      <th>Headline_x</th>\n",
       "      <th>Headline_y</th>\n",
       "      <th>pred_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12149</th>\n",
       "      <td>12150</td>\n",
       "      <td>19700</td>\n",
       "      <td>Solend Founder Calls Out Alameda and ‘Sam Bank...</td>\n",
       "      <td>25-Oct-22</td>\n",
       "      <td>-0.7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12150</th>\n",
       "      <td>12151</td>\n",
       "      <td>19751</td>\n",
       "      <td>American Rapper Lil Baby Admits Losing Million...</td>\n",
       "      <td>21-Oct-22</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       index1  index                                           Headline  \\\n",
       "12149   12150  19700  Solend Founder Calls Out Alameda and ‘Sam Bank...   \n",
       "12150   12151  19751  American Rapper Lil Baby Admits Losing Million...   \n",
       "\n",
       "            Date  pred_score Headline_x Headline_y  pred_label  \n",
       "12149  25-Oct-22        -0.7        NaN        NaN         NaN  \n",
       "12150  21-Oct-22        -0.5        NaN        NaN         NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index1</th>\n",
       "      <th>index</th>\n",
       "      <th>Headline</th>\n",
       "      <th>Date</th>\n",
       "      <th>pred_score</th>\n",
       "      <th>Headline_x</th>\n",
       "      <th>Headline_y</th>\n",
       "      <th>pred_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14249</th>\n",
       "      <td>14250</td>\n",
       "      <td>21850</td>\n",
       "      <td>Cristiano Ronaldo and Binance Sign an Exclusiv...</td>\n",
       "      <td>23-Jun-22</td>\n",
       "      <td>0.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14250</th>\n",
       "      <td>14251</td>\n",
       "      <td>21901</td>\n",
       "      <td>USDC Market Cap Soared More than $4 Billion in...</td>\n",
       "      <td>21-Jun-22</td>\n",
       "      <td>0.7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       index1  index                                           Headline  \\\n",
       "14249   14250  21850  Cristiano Ronaldo and Binance Sign an Exclusiv...   \n",
       "14250   14251  21901  USDC Market Cap Soared More than $4 Billion in...   \n",
       "\n",
       "            Date  pred_score Headline_x Headline_y  pred_label  \n",
       "14249  23-Jun-22         0.5        NaN        NaN         NaN  \n",
       "14250  21-Jun-22         0.7        NaN        NaN         NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index1</th>\n",
       "      <th>index</th>\n",
       "      <th>Headline</th>\n",
       "      <th>Date</th>\n",
       "      <th>pred_score</th>\n",
       "      <th>Headline_x</th>\n",
       "      <th>Headline_y</th>\n",
       "      <th>pred_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17249</th>\n",
       "      <td>17250</td>\n",
       "      <td>24900</td>\n",
       "      <td>Cardano Active Addresses Increased by Over 200...</td>\n",
       "      <td>30-Nov-21</td>\n",
       "      <td>0.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17250</th>\n",
       "      <td>17251</td>\n",
       "      <td>24951</td>\n",
       "      <td>NFTs Grow in Popularity and Price — Raising Co...</td>\n",
       "      <td>25-Nov-21</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       index1  index                                           Headline  \\\n",
       "17249   17250  24900  Cardano Active Addresses Increased by Over 200...   \n",
       "17250   17251  24951  NFTs Grow in Popularity and Price — Raising Co...   \n",
       "\n",
       "            Date  pred_score Headline_x Headline_y  pred_label  \n",
       "17249  30-Nov-21         0.3        NaN        NaN         NaN  \n",
       "17250  25-Nov-21         0.0        NaN        NaN         NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index1</th>\n",
       "      <th>index</th>\n",
       "      <th>Headline</th>\n",
       "      <th>Date</th>\n",
       "      <th>pred_score</th>\n",
       "      <th>Headline_x</th>\n",
       "      <th>Headline_y</th>\n",
       "      <th>pred_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18499</th>\n",
       "      <td>18500</td>\n",
       "      <td>26200</td>\n",
       "      <td>Over 85,000 Merchants in Switzerland Now Accep...</td>\n",
       "      <td>23-Aug-21</td>\n",
       "      <td>0.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18500</th>\n",
       "      <td>18501</td>\n",
       "      <td>26251</td>\n",
       "      <td>Japanese Crypto Exchange Liquid has Hot Wallet...</td>\n",
       "      <td>19-Aug-21</td>\n",
       "      <td>-0.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       index1  index                                           Headline  \\\n",
       "18499   18500  26200  Over 85,000 Merchants in Switzerland Now Accep...   \n",
       "18500   18501  26251  Japanese Crypto Exchange Liquid has Hot Wallet...   \n",
       "\n",
       "            Date  pred_score Headline_x Headline_y  pred_label  \n",
       "18499  23-Aug-21         0.5        NaN        NaN         NaN  \n",
       "18500  19-Aug-21        -0.8        NaN        NaN         NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming df is your DataFrame\n",
    "# Replace 'index' with the actual name of your index column if it's different\n",
    "index_column_name = 'index'\n",
    "\n",
    "# Iterate through the index values and check for inconsistencies\n",
    "inconsistencies = []\n",
    "prev_index = None\n",
    "for index_val in df[index_column_name]:\n",
    "    if prev_index is not None:\n",
    "        diff = index_val - prev_index\n",
    "        if diff != 1:\n",
    "            inconsistencies.append((prev_index, index_val, diff))\n",
    "    prev_index = index_val\n",
    "\n",
    "# Display the DataFrame where inconsistency is found\n",
    "if len(inconsistencies) > 0:\n",
    "    print(\"Inconsistencies found:\")\n",
    "    for inconsistency in inconsistencies:\n",
    "        from_index, to_index, _ = inconsistency\n",
    "        display(df[(df[index_column_name] >= from_index) & (df[index_column_name] <= to_index)])\n",
    "else:\n",
    "    print(\"No inconsistencies found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nahala\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sleeping\n",
      "Woke UP\n",
      "Batch 11450 completed.\n",
      "Sleeping\n",
      "Woke UP\n",
      "Batch 15550 completed.\n",
      "Sleeping\n",
      "Woke UP\n"
     ]
    },
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 69\u001b[0m\n\u001b[0;32m     66\u001b[0m json_data1 \u001b[38;5;241m=\u001b[39m response1\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mstrip(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjson\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m# Load the cleaned data and convert to DataFrame\u001b[39;00m\n\u001b[1;32m---> 69\u001b[0m jsontolist1 \u001b[38;5;241m=\u001b[39m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson_data1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     70\u001b[0m df_sample1 \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(jsontolist1)\n\u001b[0;32m     72\u001b[0m \u001b[38;5;66;03m# Merge the dataframes\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Nahala\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\json\\__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[1;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    341\u001b[0m     s \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mdecode(detect_encoding(s), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurrogatepass\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[1;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    348\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONDecoder\n",
      "File \u001b[1;32mc:\\Users\\Nahala\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\json\\decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[1;34m(self, s, _w)\u001b[0m\n\u001b[0;32m    332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, s, _w\u001b[38;5;241m=\u001b[39mWHITESPACE\u001b[38;5;241m.\u001b[39mmatch):\n\u001b[0;32m    333\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;124;03m    containing a JSON document).\u001b[39;00m\n\u001b[0;32m    335\u001b[0m \n\u001b[0;32m    336\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 337\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    338\u001b[0m     end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n\u001b[0;32m    339\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m end \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(s):\n",
      "File \u001b[1;32mc:\\Users\\Nahala\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\json\\decoder.py:355\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[1;34m(self, s, idx)\u001b[0m\n\u001b[0;32m    353\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscan_once(s, idx)\n\u001b[0;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m--> 355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    356\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj, end\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import google.generativeai as genai\n",
    "genai.configure(api_key=\"AIzaSyA9-COrUP5zX1frzRAxls1lhJJ8Cw7F1AQ\")\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import time\n",
    "# Create the model\n",
    "# See https://ai.google.dev/api/python/google/generativeai/GenerativeModel\n",
    "generation_config = {\n",
    "  \"temperature\": 0,\n",
    "  \"top_p\": 0.95,\n",
    "  \"top_k\": 64,\n",
    "  \"max_output_tokens\":100000 ,\n",
    "  \"response_mime_type\": \"text/plain\",\n",
    "}\n",
    "safety_settings = [\n",
    "  {\n",
    "    \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
    "    \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\",\n",
    "  },\n",
    "  {\n",
    "    \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
    "    \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\",\n",
    "  },\n",
    "  {\n",
    "    \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
    "    \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\",\n",
    "  },\n",
    "  {\n",
    "    \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
    "    \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\",\n",
    "  },\n",
    "]\n",
    "\n",
    "model = genai.GenerativeModel(\n",
    "  model_name=\"gemini-1.5-pro-latest\",\n",
    "  safety_settings=safety_settings,\n",
    "  generation_config=generation_config,\n",
    "  system_instruction=\"You are an advanced sentiment analysis model designed to evaluate the sentiment of news headlines related to cryptocurrency. Your task is to categorize each headline into one of three categories: positive, negative, or neutral. \\n A positive sentiment indicates that the headline is likely to have a beneficial impact on the perception of the cryptocurrency market, suggesting optimism or good news.\\nA negative sentiment indicates that the headline is likely to have an adverse impact on the perception of the cryptocurrency market, suggesting pessimism or bad news.\\nA neutral sentiment indicates that the headline does not express a clear positive or negative sentiment, or it has no significant impact on the perception of the cryptocurrency market.\\nFor each headline, provide a sentiment score ranging from -1 to 1, where:\\n- A score close to 1 indicates a strong positive sentiment,\\n- A score close to -1 indicates a strong negative sentiment,\\n- A score close to 0 indicates a neutral sentiment.\\n\\nHere are some examples for clarity:\\n1. Headline: \\\"Bitcoin prices soar to new all-time high.\\\"\\n   Sentiment: Positive\\n   Score: 0.9\\n2. Headline: \\\"Cryptocurrency market faces regulatory crackdowns.\\\"\\n   Sentiment: Negative\\n   Score: -0.8\\n3. Headline: \\\"Blockchain technology adoption continues to grow steadily.\\\"\\n   Sentiment: Neutral\\n   Score: 0.1\\nYour task is to update predicted score under 'pred_label' in the json provided. Don't make any changes to the Output format..\\nPlease analyze the following headlines and provide the sentiment and score for each:\\nNote: The analysis should be consistent across repeated evaluations of the same headlines.Note: Output should only have Index and corresponding pred_score \",)\n",
    "\n",
    "sam=[11450,15550,19700,21850,24900,24900,26200]\n",
    "Batch_size = 50\n",
    "full_merged_data1 = pd.DataFrame()  # Initialize the DataFrame before the loop\n",
    "\n",
    "for i in sam:\n",
    "    batch = df[i:i+Batch_size]\n",
    "    dct1 = batch[['index', 'Headline']].to_dict(orient='records')\n",
    "    json_string1 = json.dumps(dct1)\n",
    "\n",
    "    chat_session = model.start_chat(\n",
    "        history=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"parts\": [\n",
    "                    json_string1\n",
    "                ],\n",
    "            },\n",
    "        ]\n",
    "    )\n",
    "    response1 = chat_session.send_message(\"INSERT_INPUT_HERE\")\n",
    "    print(\"Sleeping\")\n",
    "    time.sleep(10)  # Wait for 30 seconds between requests\n",
    "    print(\"Woke UP\")\n",
    "    # Clean the data by stripping the backticks and any unwanted characters\n",
    "    json_data1 = response1.text.strip(\"`\").strip(\"json\")\n",
    "\n",
    "    # Load the cleaned data and convert to DataFrame\n",
    "    jsontolist1 = json.loads(json_data1)\n",
    "    df_sample1 = pd.DataFrame(jsontolist1)\n",
    "\n",
    "    # Merge the dataframes\n",
    "    merged_data1 = pd.merge(df, df_sample1, on='index')\n",
    "\n",
    "    # Update the full merged data DataFrame\n",
    "    full_merged_data1 = pd.concat([full_merged_data1, merged_data1], ignore_index=True)\n",
    "\n",
    "    # Save the full merged data to CSV\n",
    "    full_merged_data1.to_csv(f'D:/DATA SCIENCE/Internship/CrypoDataAnalysis/test/merged_data_till.{i}.csv', index=False)\n",
    "    print(f\"Batch {i} completed.\")\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nahala\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sleeping\n",
      "Woke UP\n",
      "Batch 11450 completed.\n",
      "Sleeping\n",
      "Woke UP\n",
      "Batch 15550 completed.\n",
      "Sleeping\n",
      "Woke UP\n",
      "JSONDecodeError: Expecting value: line 1 column 1 (char 0). Refreshing dct1 to a fresh batch.\n",
      "Sleeping\n",
      "Woke UP\n",
      "JSONDecodeError: Expecting value: line 1 column 1 (char 0). Refreshing dct1 to a fresh batch.\n",
      "Sleeping\n",
      "Woke UP\n",
      "JSONDecodeError: Expecting value: line 1 column 1 (char 0). Refreshing dct1 to a fresh batch.\n"
     ]
    },
    {
     "ename": "ResourceExhausted",
     "evalue": "429 Resource has been exhausted (e.g. check quota).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhausted\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 62\u001b[0m\n\u001b[0;32m     51\u001b[0m chat_session \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mstart_chat(\n\u001b[0;32m     52\u001b[0m     history\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m     53\u001b[0m         {\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     59\u001b[0m     ]\n\u001b[0;32m     60\u001b[0m )\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 62\u001b[0m     response1 \u001b[38;5;241m=\u001b[39m \u001b[43mchat_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_message\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mINSERT_INPUT_HERE\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSleeping\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     64\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m10\u001b[39m)  \u001b[38;5;66;03m# Wait for 30 seconds between requests\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Nahala\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\google\\generativeai\\generative_models.py:496\u001b[0m, in \u001b[0;36mChatSession.send_message\u001b[1;34m(self, content, generation_config, safety_settings, stream, tools, tool_config)\u001b[0m\n\u001b[0;32m    493\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m generation_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcandidate_count\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    494\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt chat with `candidate_count > 1`\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 496\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m    \u001b[49m\u001b[43msafety_settings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msafety_settings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtools\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtools_lib\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtool_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtool_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    505\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_response(response\u001b[38;5;241m=\u001b[39mresponse, stream\u001b[38;5;241m=\u001b[39mstream)\n\u001b[0;32m    507\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menable_automatic_function_calling \u001b[38;5;129;01mand\u001b[39;00m tools_lib \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Nahala\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\google\\generativeai\\generative_models.py:262\u001b[0m, in \u001b[0;36mGenerativeModel.generate_content\u001b[1;34m(self, contents, generation_config, safety_settings, stream, tools, tool_config, request_options)\u001b[0m\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m generation_types\u001b[38;5;241m.\u001b[39mGenerateContentResponse\u001b[38;5;241m.\u001b[39mfrom_iterator(iterator)\n\u001b[0;32m    261\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 262\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    263\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    264\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrequest_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    265\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    266\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m generation_types\u001b[38;5;241m.\u001b[39mGenerateContentResponse\u001b[38;5;241m.\u001b[39mfrom_response(response)\n\u001b[0;32m    267\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m google\u001b[38;5;241m.\u001b[39mapi_core\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mInvalidArgument \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\Nahala\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\google\\ai\\generativelanguage_v1beta\\services\\generative_service\\client.py:812\u001b[0m, in \u001b[0;36mGenerativeServiceClient.generate_content\u001b[1;34m(self, request, model, contents, retry, timeout, metadata)\u001b[0m\n\u001b[0;32m    809\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_universe_domain()\n\u001b[0;32m    811\u001b[0m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[1;32m--> 812\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrpc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    813\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    814\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    815\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    816\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    817\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    819\u001b[0m \u001b[38;5;66;03m# Done; return the response.\u001b[39;00m\n\u001b[0;32m    820\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mc:\\Users\\Nahala\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\google\\api_core\\gapic_v1\\method.py:131\u001b[0m, in \u001b[0;36m_GapicCallable.__call__\u001b[1;34m(self, timeout, retry, compression, *args, **kwargs)\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compression \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    129\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m compression\n\u001b[1;32m--> 131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Nahala\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:293\u001b[0m, in \u001b[0;36mRetry.__call__.<locals>.retry_wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    289\u001b[0m target \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    290\u001b[0m sleep_generator \u001b[38;5;241m=\u001b[39m exponential_sleep_generator(\n\u001b[0;32m    291\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maximum, multiplier\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_multiplier\n\u001b[0;32m    292\u001b[0m )\n\u001b[1;32m--> 293\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretry_target\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43msleep_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    298\u001b[0m \u001b[43m    \u001b[49m\u001b[43mon_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    299\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Nahala\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:153\u001b[0m, in \u001b[0;36mretry_target\u001b[1;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;66;03m# This function explicitly must deal with broad exceptions.\u001b[39;00m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;66;03m# defer to shared logic for handling errors\u001b[39;00m\n\u001b[1;32m--> 153\u001b[0m     \u001b[43m_retry_error_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    154\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    155\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdeadline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43msleep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43merror_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpredicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexception_factory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    162\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;66;03m# if exception not raised, sleep before next attempt\u001b[39;00m\n\u001b[0;32m    164\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(sleep)\n",
      "File \u001b[1;32mc:\\Users\\Nahala\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\google\\api_core\\retry\\retry_base.py:212\u001b[0m, in \u001b[0;36m_retry_error_helper\u001b[1;34m(exc, deadline, next_sleep, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m predicate_fn(exc):\n\u001b[0;32m    207\u001b[0m     final_exc, source_exc \u001b[38;5;241m=\u001b[39m exc_factory_fn(\n\u001b[0;32m    208\u001b[0m         error_list,\n\u001b[0;32m    209\u001b[0m         RetryFailureReason\u001b[38;5;241m.\u001b[39mNON_RETRYABLE_ERROR,\n\u001b[0;32m    210\u001b[0m         original_timeout,\n\u001b[0;32m    211\u001b[0m     )\n\u001b[1;32m--> 212\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m final_exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msource_exc\u001b[39;00m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m on_error_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    214\u001b[0m     on_error_fn(exc)\n",
      "File \u001b[1;32mc:\\Users\\Nahala\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\google\\api_core\\retry\\retry_unary.py:144\u001b[0m, in \u001b[0;36mretry_target\u001b[1;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sleep \u001b[38;5;129;01min\u001b[39;00m sleep_generator:\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 144\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    145\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39misawaitable(result):\n\u001b[0;32m    146\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(_ASYNC_RETRY_WARNING)\n",
      "File \u001b[1;32mc:\\Users\\Nahala\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\google\\api_core\\timeout.py:120\u001b[0m, in \u001b[0;36mTimeToDeadlineTimeout.__call__.<locals>.func_with_timeout\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    117\u001b[0m     \u001b[38;5;66;03m# Avoid setting negative timeout\u001b[39;00m\n\u001b[0;32m    118\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout \u001b[38;5;241m-\u001b[39m time_since_first_attempt)\n\u001b[1;32m--> 120\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Nahala\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\google\\api_core\\grpc_helpers.py:78\u001b[0m, in \u001b[0;36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m callable_(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m---> 78\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mfrom_grpc_error(exc) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n",
      "\u001b[1;31mResourceExhausted\u001b[0m: 429 Resource has been exhausted (e.g. check quota)."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import google.generativeai as genai\n",
    "genai.configure(api_key=\"AIzaSyA9-COrUP5zX1frzRAxls1lhJJ8Cw7F1AQ\")\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import time\n",
    "# Create the model\n",
    "# See https://ai.google.dev/api/python/google/generativeai/GenerativeModel\n",
    "generation_config = {\n",
    "  \"temperature\": 0,\n",
    "  \"top_p\": 0.95,\n",
    "  \"top_k\": 64,\n",
    "  \"max_output_tokens\":100000 ,\n",
    "  \"response_mime_type\": \"text/plain\",\n",
    "}\n",
    "safety_settings = [\n",
    "  {\n",
    "    \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
    "    \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\",\n",
    "  },\n",
    "  {\n",
    "    \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
    "    \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\",\n",
    "  },\n",
    "  {\n",
    "    \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
    "    \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\",\n",
    "  },\n",
    "  {\n",
    "    \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
    "    \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\",\n",
    "  },\n",
    "]\n",
    "\n",
    "model = genai.GenerativeModel(\n",
    "  model_name=\"gemini-1.5-pro-latest\",\n",
    "  safety_settings=safety_settings,\n",
    "  generation_config=generation_config,\n",
    "  system_instruction=\"You are an advanced sentiment analysis model designed to evaluate the sentiment of news headlines related to cryptocurrency. Your task is to categorize each headline into one of three categories: positive, negative, or neutral. \\n A positive sentiment indicates that the headline is likely to have a beneficial impact on the perception of the cryptocurrency market, suggesting optimism or good news.\\nA negative sentiment indicates that the headline is likely to have an adverse impact on the perception of the cryptocurrency market, suggesting pessimism or bad news.\\nA neutral sentiment indicates that the headline does not express a clear positive or negative sentiment, or it has no significant impact on the perception of the cryptocurrency market.\\nFor each headline, provide a sentiment score ranging from -1 to 1, where:\\n- A score close to 1 indicates a strong positive sentiment,\\n- A score close to -1 indicates a strong negative sentiment,\\n- A score close to 0 indicates a neutral sentiment.\\n\\nHere are some examples for clarity:\\n1. Headline: \\\"Bitcoin prices soar to new all-time high.\\\"\\n   Sentiment: Positive\\n   Score: 0.9\\n2. Headline: \\\"Cryptocurrency market faces regulatory crackdowns.\\\"\\n   Sentiment: Negative\\n   Score: -0.8\\n3. Headline: \\\"Blockchain technology adoption continues to grow steadily.\\\"\\n   Sentiment: Neutral\\n   Score: 0.1\\nYour task is to update predicted score under 'pred_label' in the json provided. Don't make any changes to the Output format..\\nPlease analyze the following headlines and provide the sentiment and score for each:\\nNote: The analysis should be consistent across repeated evaluations of the same headlines.Note: Output should only have Index and corresponding pred_score \",)\n",
    "\n",
    "sam=[11450,15550,19700,21850,24900,24900,26200]\n",
    "Batch_size = 50\n",
    "full_merged_data1 = pd.DataFrame()  # Initialize the DataFrame before the loop\n",
    "\n",
    "for i in sam:\n",
    "    batch = df[i:i+Batch_size]\n",
    "    dct1 = batch[['index', 'Headline']].to_dict(orient='records')\n",
    "    json_string1 = json.dumps(dct1)\n",
    "\n",
    "    chat_session = model.start_chat(\n",
    "        history=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"parts\": [\n",
    "                    json_string1\n",
    "                ],\n",
    "            },\n",
    "        ]\n",
    "    )\n",
    "    try:\n",
    "        response1 = chat_session.send_message(\"INSERT_INPUT_HERE\")\n",
    "        print(\"Sleeping\")\n",
    "        time.sleep(10)  # Wait for 30 seconds between requests\n",
    "        print(\"Woke UP\")\n",
    "        # Clean the data by stripping the backticks and any unwanted characters\n",
    "        json_data1 = response1.text.strip(\"`\").strip(\"json\")\n",
    "\n",
    "        # Load the cleaned data and convert to DataFrame\n",
    "        jsontolist1 = json.loads(json_data1)\n",
    "        df_sample1 = pd.DataFrame(jsontolist1)\n",
    "\n",
    "        # Merge the dataframes\n",
    "        merged_data1 = pd.merge(df, df_sample1, on='index')\n",
    "\n",
    "        # Update the full merged data DataFrame\n",
    "        full_merged_data1 = pd.concat([full_merged_data1, merged_data1], ignore_index=True)\n",
    "\n",
    "        # Save the full merged data to CSV\n",
    "        full_merged_data1.to_csv(f'D:/DATA SCIENCE/Internship/CrypoDataAnalysis/test/merged_data_till.{i}.csv', index=False)\n",
    "        print(f\"Batch {i} completed.\")\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"JSONDecodeError: {e}. Refreshing dct1 to a fresh batch.\")\n",
    "        # Refresh dct1 to a fresh batch\n",
    "        dct1 = []\n",
    "        i=i-1\n",
    "        #Should Have added i=i-50 also\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Headline</th>\n",
       "      <th>Date</th>\n",
       "      <th>pred_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7451</td>\n",
       "      <td>Will Komodo Reach 12,000 Satoshis? [Premium An...</td>\n",
       "      <td>10-Dec-19</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7452</td>\n",
       "      <td>Will Ethereum’s Consolidation End With a Break...</td>\n",
       "      <td>10-Dec-19</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7453</td>\n",
       "      <td>China Orders Government Agencies to Replace Al...</td>\n",
       "      <td>09-Dec-19</td>\n",
       "      <td>-0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7454</td>\n",
       "      <td>Facebook Files Lawsuit Against Hong Kong Firm ...</td>\n",
       "      <td>09-Dec-19</td>\n",
       "      <td>-0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7455</td>\n",
       "      <td>Bitcoin Stability Highlighted by Crashing Oil ...</td>\n",
       "      <td>09-Dec-19</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                           Headline       Date  \\\n",
       "0   7451  Will Komodo Reach 12,000 Satoshis? [Premium An...  10-Dec-19   \n",
       "1   7452  Will Ethereum’s Consolidation End With a Break...  10-Dec-19   \n",
       "2   7453  China Orders Government Agencies to Replace Al...  09-Dec-19   \n",
       "3   7454  Facebook Files Lawsuit Against Hong Kong Firm ...  09-Dec-19   \n",
       "4   7455  Bitcoin Stability Highlighted by Crashing Oil ...  09-Dec-19   \n",
       "\n",
       "   pred_score  \n",
       "0         0.1  \n",
       "1         0.0  \n",
       "2        -0.2  \n",
       "3        -0.3  \n",
       "4         0.4  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv(r\"D:\\DATA SCIENCE\\Internship\\CrypoDataAnalysis\\SENT_ANAL\\merged_data_till.27050.csv\")\n",
    "df=df.drop(columns=[\"Headline_x\",\"Headline_y\",\"pred_label\"])\n",
    "df1=pd.read_csv(r\"D:\\DATA SCIENCE\\Internship\\CrypoDataAnalysis\\test\\merged_data_till.26200.csv\")\n",
    "# Combine df1 and df using an outer join\n",
    "df1 = df1.drop_duplicates(subset='index')\n",
    "merged_df = pd.merge(df1, df, how='outer')\n",
    "\n",
    "# Display the first few rows of the merged DataFrame\n",
    "merged_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df['index'].is_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1['index'].is_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Headline</th>\n",
       "      <th>Date</th>\n",
       "      <th>pred_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [index, Headline, Date, pred_score]\n",
       "Index: []"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for duplicate indices\n",
    "duplicate_indices = merged_df[merged_df['index'].duplicated(keep=False)]\n",
    "\n",
    "# Display the rows with duplicate indices\n",
    "duplicate_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No inconsistencies found.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming df is your DataFrame\n",
    "# Replace 'index' with the actual name of your index column if it's different\n",
    "index_column_name = 'index'\n",
    "\n",
    "# Iterate through the index values and check for inconsistencies\n",
    "inconsistencies = []\n",
    "prev_index = None\n",
    "for index_val in merged_df[index_column_name]:\n",
    "    if prev_index is not None:\n",
    "        diff = index_val - prev_index\n",
    "        if diff != 1:\n",
    "            inconsistencies.append((prev_index, index_val, diff))\n",
    "    prev_index = index_val\n",
    "\n",
    "# Print or handle the inconsistencies found\n",
    "if len(inconsistencies) > 0:\n",
    "    print(\"Inconsistencies found:\")\n",
    "    for inconsistency in inconsistencies:\n",
    "        print(f\"From index {inconsistency[0]} to index {inconsistency[1]}, difference: {inconsistency[2]}\")\n",
    "else:\n",
    "    print(\"No inconsistencies found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.to_csv(f'D:/DATA SCIENCE/Internship/CrypoDataAnalysis/test/merged_data_till.from7451_to_27050.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Headline</th>\n",
       "      <th>Date</th>\n",
       "      <th>pred_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11451</td>\n",
       "      <td>Shill Might Not be Enough To Save Tron (Price ...</td>\n",
       "      <td>05-Mar-19</td>\n",
       "      <td>-0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11452</td>\n",
       "      <td>Halving-Happy Bitcoin Bulls Be Like: Buy Signa...</td>\n",
       "      <td>05-Mar-19</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11453</td>\n",
       "      <td>Will Another Coinbase Controversy Turn Ripple ...</td>\n",
       "      <td>05-Mar-19</td>\n",
       "      <td>-0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11454</td>\n",
       "      <td>Will DigiByte’s Growing Popularity Lead to Soa...</td>\n",
       "      <td>05-Mar-19</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11455</td>\n",
       "      <td>Binance Offering Big Bucks in Bounty Program a...</td>\n",
       "      <td>05-Mar-19</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>26246</td>\n",
       "      <td>Crypto.com Signs Innovation &amp; Tech Partnership...</td>\n",
       "      <td>19-Aug-21</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346</th>\n",
       "      <td>26247</td>\n",
       "      <td>On-Chain Analysis: Extreme Drop in BTC on Exch...</td>\n",
       "      <td>19-Aug-21</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>26248</td>\n",
       "      <td>Gold, Stocks, and Bitcoin: Weekly Overview — A...</td>\n",
       "      <td>19-Aug-21</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>26249</td>\n",
       "      <td>BIC’s Video News Show: Cardano’s Potential Beg...</td>\n",
       "      <td>19-Aug-21</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349</th>\n",
       "      <td>26250</td>\n",
       "      <td>DeFi Projects Are Not Exempt From Regulations,...</td>\n",
       "      <td>19-Aug-21</td>\n",
       "      <td>-0.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     index                                           Headline       Date  \\\n",
       "0    11451  Shill Might Not be Enough To Save Tron (Price ...  05-Mar-19   \n",
       "1    11452  Halving-Happy Bitcoin Bulls Be Like: Buy Signa...  05-Mar-19   \n",
       "2    11453  Will Another Coinbase Controversy Turn Ripple ...  05-Mar-19   \n",
       "3    11454  Will DigiByte’s Growing Popularity Lead to Soa...  05-Mar-19   \n",
       "4    11455  Binance Offering Big Bucks in Bounty Program a...  05-Mar-19   \n",
       "..     ...                                                ...        ...   \n",
       "345  26246  Crypto.com Signs Innovation & Tech Partnership...  19-Aug-21   \n",
       "346  26247  On-Chain Analysis: Extreme Drop in BTC on Exch...  19-Aug-21   \n",
       "347  26248  Gold, Stocks, and Bitcoin: Weekly Overview — A...  19-Aug-21   \n",
       "348  26249  BIC’s Video News Show: Cardano’s Potential Beg...  19-Aug-21   \n",
       "349  26250  DeFi Projects Are Not Exempt From Regulations,...  19-Aug-21   \n",
       "\n",
       "     pred_score  \n",
       "0          -0.3  \n",
       "1           0.6  \n",
       "2          -0.2  \n",
       "3           0.1  \n",
       "4           0.7  \n",
       "..          ...  \n",
       "345         0.6  \n",
       "346         0.4  \n",
       "347         0.1  \n",
       "348         0.5  \n",
       "349        -0.3  \n",
       "\n",
       "[300 rows x 4 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      index Headline       Date  pred_score\n",
      "2500   9951      NaN  29-Jun-19         0.1\n",
      "2501   9952      NaN  29-Jun-19         0.2\n",
      "2502   9953      NaN  29-Jun-19        -0.7\n",
      "2503   9954      NaN  29-Jun-19        -0.2\n",
      "2504   9955      NaN  29-Jun-19        -0.3\n",
      "2505   9956      NaN  29-Jun-19        -0.6\n",
      "2506   9957      NaN  29-Jun-19         0.3\n",
      "2507   9958      NaN  28-Jun-19         0.1\n",
      "2508   9959      NaN  28-Jun-19        -0.1\n",
      "2509   9960      NaN  28-Jun-19        -0.9\n",
      "2510   9961      NaN  28-Jun-19         0.8\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_subset = df[(df['index'] >= 9951) & (df['index'] <= 9961)]\n",
    "\n",
    "# Display the subset of the DataFrame\n",
    "print(df_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import google.generativeai as genai\n",
    "genai.configure(api_key=\"AIzaSyA9-COrUP5zX1frzRAxls1lhJJ8Cw7F1AQ\")\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import time\n",
    "# Create the model\n",
    "# See https://ai.google.dev/api/python/google/generativeai/GenerativeModel\n",
    "generation_config = {\n",
    "  \"temperature\": 0,\n",
    "  \"top_p\": 0.95,\n",
    "  \"top_k\": 64,\n",
    "  \"max_output_tokens\":100000 ,\n",
    "  \"response_mime_type\": \"text/plain\",\n",
    "}\n",
    "safety_settings = [\n",
    "  {\n",
    "    \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
    "    \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\",\n",
    "  },\n",
    "  {\n",
    "    \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
    "    \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\",\n",
    "  },\n",
    "  {\n",
    "    \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
    "    \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\",\n",
    "  },\n",
    "  {\n",
    "    \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
    "    \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\",\n",
    "  },\n",
    "]\n",
    "\n",
    "model = genai.GenerativeModel(\n",
    "  model_name=\"gemini-1.5-pro-latest\",\n",
    "  safety_settings=safety_settings,\n",
    "  generation_config=generation_config,\n",
    "  system_instruction=\"You are an advanced sentiment analysis model designed to evaluate the sentiment of news headlines related to cryptocurrency. Your task is to categorize each headline into one of three categories: positive, negative, or neutral. \\n A positive sentiment indicates that the headline is likely to have a beneficial impact on the perception of the cryptocurrency market, suggesting optimism or good news.\\nA negative sentiment indicates that the headline is likely to have an adverse impact on the perception of the cryptocurrency market, suggesting pessimism or bad news.\\nA neutral sentiment indicates that the headline does not express a clear positive or negative sentiment, or it has no significant impact on the perception of the cryptocurrency market.\\nFor each headline, provide a sentiment score ranging from -1 to 1, where:\\n- A score close to 1 indicates a strong positive sentiment,\\n- A score close to -1 indicates a strong negative sentiment,\\n- A score close to 0 indicates a neutral sentiment.\\n\\nHere are some examples for clarity:\\n1. Headline: \\\"Bitcoin prices soar to new all-time high.\\\"\\n   Sentiment: Positive\\n   Score: 0.9\\n2. Headline: \\\"Cryptocurrency market faces regulatory crackdowns.\\\"\\n   Sentiment: Negative\\n   Score: -0.8\\n3. Headline: \\\"Blockchain technology adoption continues to grow steadily.\\\"\\n   Sentiment: Neutral\\n   Score: 0.1\\nYour task is to update predicted score under 'pred_label' in the json provided. Don't make any changes to the Output format..\\nPlease analyze the following headlines and provide the sentiment and score for each:\\nNote: The analysis should be consistent across repeated evaluations of the same headlines.Note: Output should only have Index and corresponding pred_score \",)\n",
    "\n",
    "\n",
    "Batch_size = 50\n",
    "full_merged_data1 = pd.DataFrame()  # Initialize the DataFrame before the loop\n",
    "list1=[7550,9350,9950,10250,10600,11150,12000,12150,12450,12600,12650,12800,13150,13250,13500,13650,13700,14600,15650,16050]\n",
    "for i in list1:\n",
    "    batch = df[i:i+Batch_size]\n",
    "    dct1 = batch[['index', 'Headline']].to_dict(orient='records')\n",
    "    json_string1 = json.dumps(dct1)\n",
    "\n",
    "    chat_session = model.start_chat(\n",
    "        history=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"parts\": [\n",
    "                    json_string1\n",
    "                ],\n",
    "            },\n",
    "        ]\n",
    "    )\n",
    "    try:\n",
    "        response1 = chat_session.send_message(\"INSERT_INPUT_HERE\")\n",
    "        print(\"Sleeping\")\n",
    "        time.sleep(10)  # Wait for 30 seconds between requests\n",
    "        print(\"Woke UP\")\n",
    "        # Clean the data by stripping the backticks and any unwanted characters\n",
    "        json_data1 = response1.text.strip(\"`\").strip(\"json\")\n",
    "\n",
    "        # Load the cleaned data and convert to DataFrame\n",
    "        jsontolist1 = json.loads(json_data1)\n",
    "        df_sample1 = pd.DataFrame(jsontolist1)\n",
    "\n",
    "        # Merge the dataframes\n",
    "        merged_data1 = pd.merge(df, df_sample1, on='index')\n",
    "\n",
    "        # Update the full merged data DataFrame\n",
    "        full_merged_data1 = pd.concat([full_merged_data1, merged_data1], ignore_index=True)\n",
    "\n",
    "        # Save the full merged data to CSV\n",
    "        full_merged_data1.to_csv(f'D:/DATA SCIENCE/Internship/CrypoDataAnalysis/test/merged_data_till.{i}.csv', index=False)\n",
    "        print(f\"Batch {i} completed.\")\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"JSONDecodeError: {e}. Refreshing dct1 to a fresh batch.\")\n",
    "        # Refresh dct1 to a fresh batch\n",
    "        dct1 = []\n",
    "        i=i-1\n",
    "        #Should Have added i=i-50 also\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       index                                           Headline       Date\n",
      "0          1  UAE Pro League Teams Up With Chiliz to Bring W...  18-Apr-24\n",
      "1          2  Polkadot Price Forecast: Will DOT Reach the $5...  18-Apr-24\n",
      "2          3  Binance Opts for USDC Reserves While It Seeks ...  18-Apr-24\n",
      "3          4  Worldcoin Introduces World Chain Layer-2 Amids...  18-Apr-24\n",
      "4          5  BlackRock Was Tipped Off About High Inflation,...  18-Apr-24\n",
      "...      ...                                                ...        ...\n",
      "27051  27052  Gold, Stocks, and Bitcoin: Weekly Overview — J...  01-Jul-21\n",
      "27052  27053  Calaxy Raises $7.5M in Funding to Develop Fan App  01-Jul-21\n",
      "27053  27054       BTCD Gets Rejected at 48% – Will Alts Rally?  01-Jul-21\n",
      "27054  27055         WazirX Hires TRM Labs to Help Detect Fraud  01-Jul-21\n",
      "27055  27056  BTC On-Chain Analysis: CDD Falls to New Yearly...  01-Jul-21\n",
      "\n",
      "[27056 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data=pd.read_csv(r\"D:\\DATA SCIENCE\\Internship\\CrypoDataAnalysis\\combined.csv\")\n",
    "# Create an index column starting from 1\n",
    "data.insert(0, 'index', range(1, len(data) + 1))\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       index                                           Headline       Date\n",
      "7450    7451  Will Komodo Reach 12,000 Satoshis? [Premium An...  10-Dec-19\n",
      "7451    7452  Will Ethereum’s Consolidation End With a Break...  10-Dec-19\n",
      "7452    7453  China Orders Government Agencies to Replace Al...  09-Dec-19\n",
      "7453    7454  Facebook Files Lawsuit Against Hong Kong Firm ...  09-Dec-19\n",
      "7454    7455  Bitcoin Stability Highlighted by Crashing Oil ...  09-Dec-19\n",
      "...      ...                                                ...        ...\n",
      "27045  27046  Robinhood Made 34% of Q1 Crypto Revenue From D...  02-Jul-21\n",
      "27046  27047  Mark Cuban Backed NFT Platform Mintable Raises...  01-Jul-21\n",
      "27047  27048  Illinois Congressman Wants Law Allowing Revers...  01-Jul-21\n",
      "27048  27049  Kazakhstan Announces New Electricity Surcharge...  01-Jul-21\n",
      "27049  27050  BIC’s Video News Show: Top 5 Lowcap Gems For July  01-Jul-21\n",
      "\n",
      "[19600 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "df2 = data[(data['index'] >= 7451) & (data['index'] <= 27050)]\n",
    "\n",
    "# If 'index' is the actual index of the DataFrame\n",
    "# df2 = data.loc[7451:27050]\n",
    "\n",
    "print(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(r\"D:\\DATA SCIENCE\\Internship\\CrypoDataAnalysis\\test\\merged_data_till.from7451_to_27050.csv\")\n",
    "df1=df[['index','pred_score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>pred_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7451</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7452</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7453</td>\n",
       "      <td>-0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7454</td>\n",
       "      <td>-0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7455</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19601</th>\n",
       "      <td>27052</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19602</th>\n",
       "      <td>27053</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19603</th>\n",
       "      <td>27054</td>\n",
       "      <td>-0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19604</th>\n",
       "      <td>27055</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19605</th>\n",
       "      <td>27056</td>\n",
       "      <td>-0.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19606 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       index  pred_score\n",
       "0       7451         0.1\n",
       "1       7452         0.0\n",
       "2       7453        -0.2\n",
       "3       7454        -0.3\n",
       "4       7455         0.4\n",
       "...      ...         ...\n",
       "19601  27052         0.0\n",
       "19602  27053         0.5\n",
       "19603  27054        -0.3\n",
       "19604  27055         0.2\n",
       "19605  27056        -0.2\n",
       "\n",
       "[19606 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       index  pred_score                                           Headline  \\\n",
      "0       7451         0.1  Will Komodo Reach 12,000 Satoshis? [Premium An...   \n",
      "1       7452         0.0  Will Ethereum’s Consolidation End With a Break...   \n",
      "2       7453        -0.2  China Orders Government Agencies to Replace Al...   \n",
      "3       7454        -0.3  Facebook Files Lawsuit Against Hong Kong Firm ...   \n",
      "4       7455         0.4  Bitcoin Stability Highlighted by Crashing Oil ...   \n",
      "...      ...         ...                                                ...   \n",
      "19595  27046         0.1  Robinhood Made 34% of Q1 Crypto Revenue From D...   \n",
      "19596  27047         0.5  Mark Cuban Backed NFT Platform Mintable Raises...   \n",
      "19597  27048        -0.1  Illinois Congressman Wants Law Allowing Revers...   \n",
      "19598  27049        -0.3  Kazakhstan Announces New Electricity Surcharge...   \n",
      "19599  27050         0.0  BIC’s Video News Show: Top 5 Lowcap Gems For July   \n",
      "\n",
      "            Date  \n",
      "0      10-Dec-19  \n",
      "1      10-Dec-19  \n",
      "2      09-Dec-19  \n",
      "3      09-Dec-19  \n",
      "4      09-Dec-19  \n",
      "...          ...  \n",
      "19595  02-Jul-21  \n",
      "19596  01-Jul-21  \n",
      "19597  01-Jul-21  \n",
      "19598  01-Jul-21  \n",
      "19599  01-Jul-21  \n",
      "\n",
      "[19600 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "merged_df = pd.merge(df1, df2, on='index')\n",
    "\n",
    "print(merged_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.to_csv(f'D:/DATA SCIENCE/Internship/CrypoDataAnalysis/test/merged_data_till.from7451_to_27050.final.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "MergeError",
     "evalue": "Passing 'suffixes' which cause duplicate columns {'Date_x', 'pred_score_x', 'Headline_y'} is not allowed.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMergeError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 24\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Iterate over the remaining DataFrames and merge\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m df \u001b[38;5;129;01min\u001b[39;00m dataframes[\u001b[38;5;241m1\u001b[39m:]:\n\u001b[1;32m---> 24\u001b[0m     combined_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmerge\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcombined_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mindex\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mouter\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Drop duplicate rows based on the 'index' column\u001b[39;00m\n\u001b[0;32m     27\u001b[0m combined_df \u001b[38;5;241m=\u001b[39m combined_df\u001b[38;5;241m.\u001b[39mdrop_duplicates(subset\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Nahala\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py:184\u001b[0m, in \u001b[0;36mmerge\u001b[1;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    170\u001b[0m     op \u001b[38;5;241m=\u001b[39m _MergeOperation(\n\u001b[0;32m    171\u001b[0m         left_df,\n\u001b[0;32m    172\u001b[0m         right_df,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    182\u001b[0m         validate\u001b[38;5;241m=\u001b[39mvalidate,\n\u001b[0;32m    183\u001b[0m     )\n\u001b[1;32m--> 184\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Nahala\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py:888\u001b[0m, in \u001b[0;36m_MergeOperation.get_result\u001b[1;34m(self, copy)\u001b[0m\n\u001b[0;32m    884\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_indicator_pre_merge(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright)\n\u001b[0;32m    886\u001b[0m join_index, left_indexer, right_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_join_info()\n\u001b[1;32m--> 888\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reindex_and_concat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    889\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjoin_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mleft_indexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mright_indexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\n\u001b[0;32m    890\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    891\u001b[0m result \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge_type)\n\u001b[0;32m    893\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindicator:\n",
      "File \u001b[1;32mc:\\Users\\Nahala\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py:840\u001b[0m, in \u001b[0;36m_MergeOperation._reindex_and_concat\u001b[1;34m(self, join_index, left_indexer, right_indexer, copy)\u001b[0m\n\u001b[0;32m    837\u001b[0m left \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft[:]\n\u001b[0;32m    838\u001b[0m right \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright[:]\n\u001b[1;32m--> 840\u001b[0m llabels, rlabels \u001b[38;5;241m=\u001b[39m \u001b[43m_items_overlap_with_suffix\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    841\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mleft\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_info_axis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mright\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_info_axis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msuffixes\u001b[49m\n\u001b[0;32m    842\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    844\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m left_indexer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_range_indexer(left_indexer, \u001b[38;5;28mlen\u001b[39m(left)):\n\u001b[0;32m    845\u001b[0m     \u001b[38;5;66;03m# Pinning the index here (and in the right code just below) is not\u001b[39;00m\n\u001b[0;32m    846\u001b[0m     \u001b[38;5;66;03m#  necessary, but makes the `.take` more performant if we have e.g.\u001b[39;00m\n\u001b[0;32m    847\u001b[0m     \u001b[38;5;66;03m#  a MultiIndex for left.index.\u001b[39;00m\n\u001b[0;32m    848\u001b[0m     lmgr \u001b[38;5;241m=\u001b[39m left\u001b[38;5;241m.\u001b[39m_mgr\u001b[38;5;241m.\u001b[39mreindex_indexer(\n\u001b[0;32m    849\u001b[0m         join_index,\n\u001b[0;32m    850\u001b[0m         left_indexer,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    855\u001b[0m         use_na_proxy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    856\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Nahala\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py:2757\u001b[0m, in \u001b[0;36m_items_overlap_with_suffix\u001b[1;34m(left, right, suffixes)\u001b[0m\n\u001b[0;32m   2755\u001b[0m     dups\u001b[38;5;241m.\u001b[39mextend(rlabels[(rlabels\u001b[38;5;241m.\u001b[39mduplicated()) \u001b[38;5;241m&\u001b[39m (\u001b[38;5;241m~\u001b[39mright\u001b[38;5;241m.\u001b[39mduplicated())]\u001b[38;5;241m.\u001b[39mtolist())\n\u001b[0;32m   2756\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dups:\n\u001b[1;32m-> 2757\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MergeError(\n\u001b[0;32m   2758\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msuffixes\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m which cause duplicate columns \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mset\u001b[39m(dups)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2759\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnot allowed.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   2760\u001b[0m     )\n\u001b[0;32m   2762\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m llabels, rlabels\n",
      "\u001b[1;31mMergeError\u001b[0m: Passing 'suffixes' which cause duplicate columns {'Date_x', 'pred_score_x', 'Headline_y'} is not allowed."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Directory containing the CSV files\n",
    "folder_path = r'D:\\DATA SCIENCE\\Internship\\CrypoDataAnalysis\\SENT_ANAL'\n",
    "\n",
    "# List all CSV files in the folder\n",
    "csv_files = [file for file in os.listdir(folder_path) if file.endswith('.csv')]\n",
    "\n",
    "# Initialize an empty list to hold DataFrames\n",
    "dataframes = []\n",
    "\n",
    "# Read each CSV file and append to the list\n",
    "for file in csv_files:\n",
    "    df = pd.read_csv(os.path.join(folder_path, file))\n",
    "    dataframes.append(df)\n",
    "\n",
    "# Merge all DataFrames on the 'index' column\n",
    "# Start with the first DataFrame\n",
    "combined_df = dataframes[0]\n",
    "\n",
    "# Iterate over the remaining DataFrames and merge\n",
    "for df in dataframes[1:]:\n",
    "    combined_df = pd.merge(combined_df, df, on='index', how='outer')\n",
    "\n",
    "# Drop duplicate rows based on the 'index' column\n",
    "combined_df = combined_df.drop_duplicates(subset='index')\n",
    "\n",
    "# Reset the index if necessary\n",
    "combined_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Print the combined DataFrame\n",
    "print(combined_df)\n",
    "\n",
    "# Save the combined DataFrame to a new CSV file if needed\n",
    "combined_df.to_csv('combined_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "MergeError",
     "evalue": "Passing 'suffixes' which cause duplicate columns {'Date_x', 'pred_score_x', 'Headline_y'} is not allowed.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMergeError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 19\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Merge the current DataFrame with the combined DataFrame\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m combined_df\u001b[38;5;241m.\u001b[39mempty:\n\u001b[1;32m---> 19\u001b[0m     combined_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmerge\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcombined_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mindex\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mouter\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     21\u001b[0m     combined_df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mcopy()\n",
      "File \u001b[1;32mc:\\Users\\Nahala\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py:184\u001b[0m, in \u001b[0;36mmerge\u001b[1;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    170\u001b[0m     op \u001b[38;5;241m=\u001b[39m _MergeOperation(\n\u001b[0;32m    171\u001b[0m         left_df,\n\u001b[0;32m    172\u001b[0m         right_df,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    182\u001b[0m         validate\u001b[38;5;241m=\u001b[39mvalidate,\n\u001b[0;32m    183\u001b[0m     )\n\u001b[1;32m--> 184\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Nahala\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py:888\u001b[0m, in \u001b[0;36m_MergeOperation.get_result\u001b[1;34m(self, copy)\u001b[0m\n\u001b[0;32m    884\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_indicator_pre_merge(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright)\n\u001b[0;32m    886\u001b[0m join_index, left_indexer, right_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_join_info()\n\u001b[1;32m--> 888\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reindex_and_concat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    889\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjoin_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mleft_indexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mright_indexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\n\u001b[0;32m    890\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    891\u001b[0m result \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge_type)\n\u001b[0;32m    893\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindicator:\n",
      "File \u001b[1;32mc:\\Users\\Nahala\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py:840\u001b[0m, in \u001b[0;36m_MergeOperation._reindex_and_concat\u001b[1;34m(self, join_index, left_indexer, right_indexer, copy)\u001b[0m\n\u001b[0;32m    837\u001b[0m left \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft[:]\n\u001b[0;32m    838\u001b[0m right \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright[:]\n\u001b[1;32m--> 840\u001b[0m llabels, rlabels \u001b[38;5;241m=\u001b[39m \u001b[43m_items_overlap_with_suffix\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    841\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mleft\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_info_axis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mright\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_info_axis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msuffixes\u001b[49m\n\u001b[0;32m    842\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    844\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m left_indexer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_range_indexer(left_indexer, \u001b[38;5;28mlen\u001b[39m(left)):\n\u001b[0;32m    845\u001b[0m     \u001b[38;5;66;03m# Pinning the index here (and in the right code just below) is not\u001b[39;00m\n\u001b[0;32m    846\u001b[0m     \u001b[38;5;66;03m#  necessary, but makes the `.take` more performant if we have e.g.\u001b[39;00m\n\u001b[0;32m    847\u001b[0m     \u001b[38;5;66;03m#  a MultiIndex for left.index.\u001b[39;00m\n\u001b[0;32m    848\u001b[0m     lmgr \u001b[38;5;241m=\u001b[39m left\u001b[38;5;241m.\u001b[39m_mgr\u001b[38;5;241m.\u001b[39mreindex_indexer(\n\u001b[0;32m    849\u001b[0m         join_index,\n\u001b[0;32m    850\u001b[0m         left_indexer,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    855\u001b[0m         use_na_proxy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    856\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Nahala\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py:2757\u001b[0m, in \u001b[0;36m_items_overlap_with_suffix\u001b[1;34m(left, right, suffixes)\u001b[0m\n\u001b[0;32m   2755\u001b[0m     dups\u001b[38;5;241m.\u001b[39mextend(rlabels[(rlabels\u001b[38;5;241m.\u001b[39mduplicated()) \u001b[38;5;241m&\u001b[39m (\u001b[38;5;241m~\u001b[39mright\u001b[38;5;241m.\u001b[39mduplicated())]\u001b[38;5;241m.\u001b[39mtolist())\n\u001b[0;32m   2756\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dups:\n\u001b[1;32m-> 2757\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MergeError(\n\u001b[0;32m   2758\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msuffixes\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m which cause duplicate columns \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mset\u001b[39m(dups)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2759\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnot allowed.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   2760\u001b[0m     )\n\u001b[0;32m   2762\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m llabels, rlabels\n",
      "\u001b[1;31mMergeError\u001b[0m: Passing 'suffixes' which cause duplicate columns {'Date_x', 'pred_score_x', 'Headline_y'} is not allowed."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Path to the folder containing the CSV files\n",
    "folder_path = r'D:\\DATA SCIENCE\\Internship\\CrypoDataAnalysis\\SENT_ANAL'\n",
    "\n",
    "# Initialize an empty DataFrame to store the combined data\n",
    "combined_df = pd.DataFrame()\n",
    "\n",
    "# Loop through all files in the folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.csv'):\n",
    "        # Read the CSV file into a DataFrame\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Merge the current DataFrame with the combined DataFrame\n",
    "        if not combined_df.empty:\n",
    "            combined_df = pd.merge(combined_df, df, on='index', how='outer')\n",
    "        else:\n",
    "            combined_df = df.copy()\n",
    "\n",
    "# List of columns to exclude while combining\n",
    "exclude_columns = ['Date_x', 'pred_score_x', 'Headline_y']\n",
    "\n",
    "# Remove the excluded columns from the combined DataFrame\n",
    "combined_df = combined_df.drop(columns=[col for col in combined_df.columns if col.endswith(('_x', '_y')) or col in exclude_columns])\n",
    "\n",
    "# Remove duplicate rows based on the 'index' column\n",
    "combined_df = combined_df.drop_duplicates(subset=['index'])\n",
    "\n",
    "# Reset the index of the combined DataFrame\n",
    "combined_df = combined_df.reset_index(drop=True)\n",
    "\n",
    "# Print the combined DataFrame\n",
    "print(combined_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 50,\n",
       " 100,\n",
       " 150,\n",
       " 200,\n",
       " 250,\n",
       " 300,\n",
       " 350,\n",
       " 400,\n",
       " 450,\n",
       " 500,\n",
       " 550,\n",
       " 600,\n",
       " 650,\n",
       " 700,\n",
       " 750,\n",
       " 800,\n",
       " 850,\n",
       " 900,\n",
       " 950,\n",
       " 1000,\n",
       " 1050,\n",
       " 1100,\n",
       " 1150,\n",
       " 1200,\n",
       " 1250,\n",
       " 1300,\n",
       " 1350,\n",
       " 1400,\n",
       " 1450,\n",
       " 1500,\n",
       " 1550,\n",
       " 1600,\n",
       " 1650,\n",
       " 1700,\n",
       " 1750,\n",
       " 1800,\n",
       " 1850,\n",
       " 1900,\n",
       " 1950,\n",
       " 2000,\n",
       " 2050,\n",
       " 2100,\n",
       " 2150,\n",
       " 2200,\n",
       " 2250,\n",
       " 2300,\n",
       " 2350,\n",
       " 2400,\n",
       " 2450,\n",
       " 2500,\n",
       " 2550,\n",
       " 2600,\n",
       " 2650,\n",
       " 2700,\n",
       " 2750,\n",
       " 2800,\n",
       " 2850,\n",
       " 2900,\n",
       " 2950,\n",
       " 3000,\n",
       " 3050,\n",
       " 3100,\n",
       " 3150,\n",
       " 3200,\n",
       " 3250,\n",
       " 3300,\n",
       " 3350,\n",
       " 3400,\n",
       " 3450,\n",
       " 3500,\n",
       " 3550,\n",
       " 3600,\n",
       " 3650,\n",
       " 3700,\n",
       " 3750,\n",
       " 3800,\n",
       " 3850,\n",
       " 3900,\n",
       " 3950,\n",
       " 4000,\n",
       " 4050,\n",
       " 4100,\n",
       " 4150,\n",
       " 4200,\n",
       " 4250,\n",
       " 4300,\n",
       " 4350,\n",
       " 4400,\n",
       " 4450,\n",
       " 4500,\n",
       " 4550,\n",
       " 4600,\n",
       " 4650,\n",
       " 4700,\n",
       " 4750,\n",
       " 4800,\n",
       " 4850,\n",
       " 4900,\n",
       " 4950,\n",
       " 5000,\n",
       " 5050,\n",
       " 5100,\n",
       " 5150,\n",
       " 5200,\n",
       " 5250,\n",
       " 5300,\n",
       " 5350,\n",
       " 5450,\n",
       " 5500,\n",
       " 5550,\n",
       " 5600,\n",
       " 5650,\n",
       " 5700,\n",
       " 5750,\n",
       " 5800,\n",
       " 5850,\n",
       " 5900,\n",
       " 5950,\n",
       " 6000,\n",
       " 6050,\n",
       " 6100,\n",
       " 6150,\n",
       " 6200,\n",
       " 6250,\n",
       " 6300,\n",
       " 6350,\n",
       " 6450,\n",
       " 6500,\n",
       " 6550,\n",
       " 6600,\n",
       " 6650,\n",
       " 6700,\n",
       " 6750,\n",
       " 6800,\n",
       " 6850,\n",
       " 6900,\n",
       " 6950,\n",
       " 7000,\n",
       " 7050,\n",
       " 7100,\n",
       " 7150,\n",
       " 7200,\n",
       " 7250,\n",
       " 7300,\n",
       " 7350,\n",
       " 7400,\n",
       " 7450,\n",
       " 7500,\n",
       " 7550,\n",
       " 7600,\n",
       " 7650,\n",
       " 7700,\n",
       " 7750,\n",
       " 7800,\n",
       " 7850,\n",
       " 7900,\n",
       " 7950,\n",
       " 8000,\n",
       " 8050,\n",
       " 8100,\n",
       " 8150,\n",
       " 8200,\n",
       " 8250,\n",
       " 8300,\n",
       " 8350,\n",
       " 8400,\n",
       " 8450,\n",
       " 8500,\n",
       " 8550,\n",
       " 8600,\n",
       " 8650,\n",
       " 8700,\n",
       " 8750,\n",
       " 8800,\n",
       " 8850,\n",
       " 8900,\n",
       " 8950,\n",
       " 9000,\n",
       " 9050,\n",
       " 9100,\n",
       " 9150,\n",
       " 9200,\n",
       " 9250,\n",
       " 9300,\n",
       " 9350,\n",
       " 9400,\n",
       " 9450,\n",
       " 9500,\n",
       " 9550,\n",
       " 9600,\n",
       " 9650,\n",
       " 9700,\n",
       " 9750,\n",
       " 9800,\n",
       " 9850,\n",
       " 9900,\n",
       " 9950,\n",
       " 10000,\n",
       " 10050,\n",
       " 10100,\n",
       " 10150,\n",
       " 10200,\n",
       " 10250,\n",
       " 10300,\n",
       " 10350,\n",
       " 10400,\n",
       " 10450,\n",
       " 10500,\n",
       " 10550,\n",
       " 10600,\n",
       " 10650,\n",
       " 10700,\n",
       " 10750,\n",
       " 10800,\n",
       " 10850,\n",
       " 10900,\n",
       " 10950,\n",
       " 11000,\n",
       " 11050,\n",
       " 11100,\n",
       " 11150,\n",
       " 11200,\n",
       " 11250,\n",
       " 11300,\n",
       " 11350,\n",
       " 11400,\n",
       " 11450,\n",
       " 11500,\n",
       " 11550,\n",
       " 11600,\n",
       " 11650,\n",
       " 11700,\n",
       " 11750,\n",
       " 11800,\n",
       " 11850,\n",
       " 11900,\n",
       " 11950,\n",
       " 12000,\n",
       " 12050,\n",
       " 12100,\n",
       " 12150,\n",
       " 12200,\n",
       " 12250,\n",
       " 12300,\n",
       " 12350,\n",
       " 12400,\n",
       " 12450,\n",
       " 12500,\n",
       " 12550,\n",
       " 12600,\n",
       " 12650,\n",
       " 12700,\n",
       " 12750,\n",
       " 12800,\n",
       " 12850,\n",
       " 12900,\n",
       " 12950,\n",
       " 13000,\n",
       " 13050,\n",
       " 13100,\n",
       " 13150,\n",
       " 13200,\n",
       " 13250,\n",
       " 13300,\n",
       " 13350,\n",
       " 13400,\n",
       " 13450,\n",
       " 13500,\n",
       " 13550,\n",
       " 13600,\n",
       " 13650,\n",
       " 13700,\n",
       " 13750,\n",
       " 13800,\n",
       " 13850,\n",
       " 13900,\n",
       " 13950,\n",
       " 14000,\n",
       " 14050,\n",
       " 14100,\n",
       " 14150,\n",
       " 14200,\n",
       " 14250,\n",
       " 14300,\n",
       " 14350,\n",
       " 14400,\n",
       " 14500,\n",
       " 14550,\n",
       " 14600,\n",
       " 14650,\n",
       " 14700,\n",
       " 14750,\n",
       " 14800,\n",
       " 14850,\n",
       " 14900,\n",
       " 14950,\n",
       " 15000,\n",
       " 15050,\n",
       " 15100,\n",
       " 15150,\n",
       " 15200,\n",
       " 15250,\n",
       " 15300,\n",
       " 15350,\n",
       " 15400,\n",
       " 15450,\n",
       " 15500,\n",
       " 15600,\n",
       " 15650,\n",
       " 15700,\n",
       " 15750,\n",
       " 15800,\n",
       " 15850,\n",
       " 15900,\n",
       " 15950,\n",
       " 16000,\n",
       " 16050,\n",
       " 16100,\n",
       " 16150,\n",
       " 16200,\n",
       " 16250,\n",
       " 16300,\n",
       " 16350,\n",
       " 16400,\n",
       " 16450,\n",
       " 16500,\n",
       " 16550,\n",
       " 16600,\n",
       " 16650,\n",
       " 16700,\n",
       " 16750,\n",
       " 16800,\n",
       " 16850,\n",
       " 16900,\n",
       " 16950,\n",
       " 17000,\n",
       " 17050,\n",
       " 17100,\n",
       " 17150,\n",
       " 17200,\n",
       " 17250,\n",
       " 17300,\n",
       " 17350,\n",
       " 17400,\n",
       " 17450,\n",
       " 17500,\n",
       " 17550,\n",
       " 17600,\n",
       " 17650,\n",
       " 17700,\n",
       " 17750,\n",
       " 17800,\n",
       " 17850,\n",
       " 17900,\n",
       " 17950,\n",
       " 18000,\n",
       " 18050,\n",
       " 18100,\n",
       " 18150,\n",
       " 18200,\n",
       " 18250,\n",
       " 18300,\n",
       " 18350,\n",
       " 18400,\n",
       " 18450,\n",
       " 18500,\n",
       " 18550,\n",
       " 18600,\n",
       " 18650,\n",
       " 18700,\n",
       " 18750,\n",
       " 18800,\n",
       " 18850,\n",
       " 18900,\n",
       " 18950,\n",
       " 19000,\n",
       " 19050,\n",
       " 19100,\n",
       " 19150,\n",
       " 19200,\n",
       " 19250,\n",
       " 19300,\n",
       " 19350,\n",
       " 19400,\n",
       " 19450,\n",
       " 19500,\n",
       " 19550,\n",
       " 19600,\n",
       " 19650,\n",
       " 19750,\n",
       " 19800,\n",
       " 19850,\n",
       " 19900,\n",
       " 19950,\n",
       " 20000,\n",
       " 20050,\n",
       " 20100,\n",
       " 20150,\n",
       " 20200,\n",
       " 20250,\n",
       " 20300,\n",
       " 20350,\n",
       " 20400,\n",
       " 20450,\n",
       " 20500,\n",
       " 20550,\n",
       " 20600,\n",
       " 20650,\n",
       " 20700,\n",
       " 20750,\n",
       " 20800,\n",
       " 20850,\n",
       " 20900,\n",
       " 20950,\n",
       " 21000,\n",
       " 21050,\n",
       " 21100,\n",
       " 21150,\n",
       " 21200,\n",
       " 21250,\n",
       " 21300,\n",
       " 21350,\n",
       " 21400,\n",
       " 21450,\n",
       " 21500,\n",
       " 21550,\n",
       " 21600,\n",
       " 21650,\n",
       " 21700,\n",
       " 21750,\n",
       " 21800,\n",
       " 21900,\n",
       " 21950,\n",
       " 22000,\n",
       " 22050,\n",
       " 22100,\n",
       " 22150,\n",
       " 22200,\n",
       " 22250,\n",
       " 22300,\n",
       " 22350,\n",
       " 22400,\n",
       " 22450,\n",
       " 22500,\n",
       " 22550,\n",
       " 22600,\n",
       " 22650,\n",
       " 22700,\n",
       " 22750,\n",
       " 22800,\n",
       " 22850,\n",
       " 22900,\n",
       " 22950,\n",
       " 23000,\n",
       " 23050,\n",
       " 23100,\n",
       " 23150,\n",
       " 23200,\n",
       " 23250,\n",
       " 23300,\n",
       " 23350,\n",
       " 23400,\n",
       " 23450,\n",
       " 23500,\n",
       " 23550,\n",
       " 23600,\n",
       " 23650,\n",
       " 23700,\n",
       " 23750,\n",
       " 23800,\n",
       " 23850,\n",
       " 23900,\n",
       " 23950,\n",
       " 24000,\n",
       " 24050,\n",
       " 24100,\n",
       " 24150,\n",
       " 24200,\n",
       " 24250,\n",
       " 24300,\n",
       " 24350,\n",
       " 24400,\n",
       " 24450,\n",
       " 24500,\n",
       " 24550,\n",
       " 24600,\n",
       " 24650,\n",
       " 24700,\n",
       " 24750,\n",
       " 24800,\n",
       " 24850,\n",
       " 24950,\n",
       " 25000,\n",
       " 25050,\n",
       " 25100,\n",
       " 25150,\n",
       " 25200,\n",
       " 25250,\n",
       " 25300,\n",
       " 25350,\n",
       " 25400,\n",
       " 25450,\n",
       " 25500,\n",
       " 25550,\n",
       " 25600,\n",
       " 25650,\n",
       " 25700,\n",
       " 25750,\n",
       " 25800,\n",
       " 25850,\n",
       " 25900,\n",
       " 25950,\n",
       " 26000,\n",
       " 26050,\n",
       " 26100,\n",
       " 26150,\n",
       " 26250,\n",
       " 26300,\n",
       " 26350,\n",
       " 26400,\n",
       " 26450,\n",
       " 26500,\n",
       " 26550,\n",
       " 26600,\n",
       " 26650,\n",
       " 26700,\n",
       " 26750,\n",
       " 26800,\n",
       " 26850,\n",
       " 26900,\n",
       " 26950,\n",
       " 27000,\n",
       " 27050]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Path to the folder containing the CSV files\n",
    "folder_path = r'D:\\DATA SCIENCE\\Internship\\CrypoDataAnalysis\\SENT_ANAL'\n",
    "# Get a list of file names in the folder\n",
    "file_names = os.listdir(folder_path)\n",
    "\n",
    "# Sort the file names\n",
    "file_names.sort()\n",
    "\n",
    "# Initialize variables to track step and last numeric part\n",
    "step = None\n",
    "last_numeric_part = None\n",
    "ls=[]\n",
    "# Iterate through file names to analyze step\n",
    "for file_name in file_names:\n",
    "    numeric_part = file_name.split(\"merged_data_till.\")[1].split(\".\")[0]\n",
    "    numeric_part = int(numeric_part)\n",
    "    ls.append(numeric_part)\n",
    "sorted(ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged CSV file saved to D:\\DATA SCIENCE\\Internship\\CrypoDataAnalysis\\SENT_ANAL\\merged_output.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define the path to the folder containing the CSV files\n",
    "folder_path = r'D:\\DATA SCIENCE\\Internship\\CrypoDataAnalysis\\SENT_ANAL'\n",
    "\n",
    "# Get a list of all CSV files in the folder\n",
    "csv_files = [file for file in os.listdir(folder_path) if file.endswith('.csv')]\n",
    "\n",
    "# Initialize an empty list to store DataFrames\n",
    "dfs = []\n",
    "\n",
    "# Columns to keep in the final DataFrame\n",
    "columns_to_keep = ['index', 'Date', 'Headline', 'pred_score']\n",
    "\n",
    "# Read each CSV file, select the relevant columns, and append to the list\n",
    "for csv_file in csv_files:\n",
    "    file_path = os.path.join(folder_path, csv_file)\n",
    "    df = pd.read_csv(file_path)\n",
    "    # Select only the columns we need\n",
    "    df = df[columns_to_keep]\n",
    "    dfs.append(df)\n",
    "\n",
    "# Concatenate all DataFrames\n",
    "merged_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Drop duplicate rows based on 'index' if necessary\n",
    "# merged_df = merged_df.drop_duplicates(subset=['index'])\n",
    "\n",
    "# Save the merged DataFrame to a new CSV file\n",
    "merged_csv_path = os.path.join(folder_path, 'merged_output.csv')\n",
    "merged_df.to_csv(merged_csv_path, index=False)\n",
    "\n",
    "print(f\"Merged CSV file saved to {merged_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
